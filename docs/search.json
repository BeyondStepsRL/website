[
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Post",
    "section": "",
    "text": "Archive Study Materials\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow to read papers\n\n\n\n\n\n\netc\n\n\n\n논문 읽는 법에 관한 영상 요약\n\n\n\n\n\nMar 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLearning robust perceptive locomotion for quadrupedal robots in the wild\n\n\n\n\n\n\nreview\n\n\n\ntbai 2nd paper review\n\n\n\n\n\nMar 1, 2025\n\n\nJungyeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\n[6] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nCombining Legged and Manipulator/ Foundation Model / / /\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[5] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nImpedance Matching/ / / /\n\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[4] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\n / / Small Home Robot /MI-HGNN\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[3] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nHybrid Internal Model/Locomotion over Challenging Terrain/Extreme Parkour/Differentiable Simulation\n\n\n\n\n\nFeb 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[2] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nActor-Cross-Critic/ANYmal Parkour/Stiffness Tuning/Privileged Information\n\n\n\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nDreamFLEX/RL-augmented MPC/CAIMAN/Safe RL/MPC Testbed\n\n\n\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear MPC\n\n\n\n\n\n\nmpc\n\n\npaper\n\n\n\nBasics of Linear MPC Controller for Quadruped Walking Robots\n\n\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html",
    "href": "post/study/2025-03-01-how-to-read.html",
    "title": "How to read papers",
    "section": "",
    "text": "논문을 읽는법 영상은 논문 읽는 효율적인 방법에 대한 팁을 제공합니다. 논문 읽는 것에 정답이 없고 이미 각자의 방법이 있겠으나, 참고하기에 좋은 것 같아 AI 요약본으로 공유합니다.\n핵심은 처음부터 모든 내용을 꼼꼼히 읽는 것이 아니라, 자신의 이해도에 맞춰 전략적으로 접근하는 것입니다. 처음에는 랜드마크 논문 을 찾아 Introduction과 Related Works를 통해 분야의 큰 그림을 파악하고, 이후에는 Method와 Experiment 중심으로 읽어 나가는 것이 중요합니다. 또한, 논문 에서 감춰진 한계점 을 파악하고, 새로운 아이디어를 발견하는 데 집중해야 합니다. 이 영상은 연구 주제를 설정 하고 논문의 완성도 를 높이는 데 실질적인 도움을 줄 수 있습니다."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#논문-읽기의-시작",
    "href": "post/study/2025-03-01-how-to-read.html#논문-읽기의-시작",
    "title": "How to read papers",
    "section": "1. 🏁논문 읽기의 시작",
    "text": "1. 🏁논문 읽기의 시작\n\n일반적으로 논문 한 편을 쓰기 위해 약 100편의 관련 논문 을 읽어야 하지만, 이는 학술적 내용과 수식 때문에 쉽지 않은 과제다.\n기존의 논문 읽기 방법(초록, 결론 순으로 읽기)은 빠르게 읽을 수는 있지만, 내용 이해와 기억에는 효과적이지 않다는 것이 밝혀졌다.\n효율적인 논문 읽기의 핵심은 정해진 순서가 아닌, 읽는 사람의 이해 수준 에 따라 접근 방식을 조정하는 것이다.\n새로운 연구 분야에 진입할 때는 경험이 전혀 없는 상태에서 시작하므로, 이에 맞는 특별한 접근 방식이 필요하다."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#랜드마크-논문-찾기-연구의-시작점",
    "href": "post/study/2025-03-01-how-to-read.html#랜드마크-논문-찾기-연구의-시작점",
    "title": "How to read papers",
    "section": "2. 🔍랜드마크 논문 찾기: 연구의 시작점",
    "text": "2. 🔍랜드마크 논문 찾기: 연구의 시작점\n\n연구 분야의 랜드마크  논문 을 찾는 것이 논문 읽기의 첫 단계다.\n키워드로 검색한 논문의 Related Works 섹션을 통해 관련 연구들을 파악할 수 있다.\n여러 논문 에서 공통적으로 인용되는  논문 들을 모아두면 약 10개 정도의 핵심 논문 을 찾을 수 있다.\n구글 스칼라에서 인용 횟수 를 확인하여 논문의 중요성과 품질을 판단할 수 있다.\n랜드마크 논문을 찾았다고 해서 바로 방법론(Methodology)을 읽는 것은 적절하지 않다."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#연구-분야의-큰-그림-파악하기",
    "href": "post/study/2025-03-01-how-to-read.html#연구-분야의-큰-그림-파악하기",
    "title": "How to read papers",
    "section": "3. ⛰️연구 분야의 큰 그림 파악하기",
    "text": "3. ⛰️연구 분야의 큰 그림 파악하기\n\n초심자는 개별 논문 의 문제 해결 방식보다 연구 분야의 전체적인 맥락 을 먼저 이해해야 한다.\n랜드마크 논문 의 Introduction과 Related Works 를 자세히 읽어 연구 분야의 큰 방향성을 파악해야 한다.\n마치 이야기책을 읽듯이 연구 분야의 흐름을 파악하는 과정은 생각보다 흥미롭다.\n여러 논문 을 읽다 보면 반복되는 내용 이 나타나는데, 이는 해당 분야의 전반적인 이해를 얻었다는 신호다.\n이러한 과정을 통해 연구 분야에서 어떤 일이 일어나고 있는지 대략적으로 파악할 수 있다."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#논문-읽기의-효율적-접근-방법",
    "href": "post/study/2025-03-01-how-to-read.html#논문-읽기의-효율적-접근-방법",
    "title": "How to read papers",
    "section": "4. 🧐논문 읽기의 효율적 접근 방법",
    "text": "4. 🧐논문 읽기의 효율적 접근 방법\n\n연구 분야의 컨텍스트를 파악한 후에는 Introduction과 Related Works를 건너뛰고 Methodology와 Experiment 위주로 논문 을 읽는다.\n여러 논문 의 Introduction과 Related Works는 비슷한 내용 을 다루므로, 이미 알고 있는 내용은 생략하고 문제 해결 방식에 집중 한다.\n이러한 전략적 접근으로 읽어야 할 양이 절반 이하로 줄어들어 효율성이 크게 향상된다.\n처음부터 끝까지 모든 내용을 꼼꼼히 읽는 대신, 중요한 논문들을 선별하여 Methodology와 Experiment 위주 로 읽는다.\n이러한 방식으로 논문을 읽다 보면 연구 분야의 전체적인 흐름과 미해결 문제 들을 파악할 수 있게 된다."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#좋은-논문의-조건과-효과적인-논문-읽기-방법",
    "href": "post/study/2025-03-01-how-to-read.html#좋은-논문의-조건과-효과적인-논문-읽기-방법",
    "title": "How to read papers",
    "section": "5. 🚩좋은 논문의 조건과 효과적인 논문 읽기 방법",
    "text": "5. 🚩좋은 논문의 조건과 효과적인 논문 읽기 방법\n\n좋은 논문 은 노벨티(novelty) 와 완결성 이라는 두 가지 요소를 충족해야 한다.\n노벨티 는 다른 사람이 시도하지 않은 새롭고 가치 있는 아이디어를 의미하며, 이를 위해서는 기존 연구를 파악해야 한다.\n논문 100편을 읽는 진정한 목적은 연구 분야를 이해하고 좋은 연구 주제를 설정 하기 위함이다.\n논문 을 읽을 때는 방법론(Methodology)과 실험(Experiment) 위주로 읽되, 각 방법의 한계점 을 파악하는 것이 중요하다.\n논문은 밀도 높은 텍스트 이므로, 모든 논문 을 완전히 소화하기보다는 효율적으로 선별하고 이해 하는 능력이 중요하다."
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html",
    "href": "post/study/2025-02-17-mpc.html",
    "title": "Linear MPC",
    "section": "",
    "text": "This post is sourced from pympc-quadruped and translated into Korean."
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#dynamic-constraints",
    "href": "post/study/2025-02-17-mpc.html#dynamic-constraints",
    "title": "Linear MPC",
    "section": "1. Dynamic Constraints",
    "text": "1. Dynamic Constraints\n\na. Approximated Angular Velocity Dynamics\nThe robot’s orientation is expressed as a vector of Z-Y-X Euler angles \\(\\Theta = [\\phi, \\theta, \\psi]^\\text{T}\\), where\n\n\\(\\psi\\) is the yaw,\n\\(\\theta\\) is the pitch, and\n\\(\\phi\\) is the roll.\n\nThese angles correspond to a sequence of rotations such that the transform from body to world coordinates can be expressed as \\[\n\\mathbf{R} = \\mathbf{R}_z(\\psi)\\mathbf{R}_y(\\theta)\\mathbf{R}_x(\\phi)\n\\]\nwhere \\(\\mathbf{R}_n(\\alpha)\\) represents a positive rotation of \\(\\alpha\\) about the \\(n\\)-axis. In details, we can write\n\\[\n\\mathbf{R}_z(\\psi) = \\begin{bmatrix}\n    \\cos\\psi & -\\sin \\psi & 0 \\\\\n    \\sin\\psi & \\cos \\psi  & 0 \\\\\n    0        & 0          & 1\n\\end{bmatrix},\n\\mathbf{R}_y(\\theta) = \\begin{bmatrix}\n    \\cos\\theta  & 0 & \\sin\\theta \\\\\n    0           & 1 & 0          \\\\\n    -\\sin\\theta & 0 & \\cos\\theta\n\\end{bmatrix},\n\\mathbf{R}_z(\\psi) = \\begin{bmatrix}\n    \\cos\\psi & -\\sin \\psi & 0 \\\\\n    \\sin\\psi & \\cos \\psi  & 0 \\\\\n    0        & 0          & 1\n\\end{bmatrix}\n\\]\nFrom [2], we have known that \\(\\dot{\\mathbf{R}} = [\\mathbf{\\omega}]\\mathbf{R}\\), where \\(\\mathbf{\\omega} \\in \\mathbb{R}^3\\) is the robot’s angular velocity, \\([\\mathbf{\\omega}] \\in \\mathbb{R}^{3\\times3}\\) is defined as the skew-symmetric matrix with respect to \\(\\mathbf{\\omega}\\), and \\(\\mathbf{R}\\) is the rotation matrix which transforms from body to world coordinates. Then the angular velocity in world coordinates can be found with\n\\[\n\\begin{aligned}\n[\\mathbf{\\omega}] &= \\dot{\\mathbf{R}}\\mathbf{R}^{-1} = \\dot{\\mathbf{R}}\\mathbf{R}^\\text{T} \\\\\n&= \\left( \\frac{\\partial R}{\\partial \\psi} + \\frac{\\partial R}{\\partial \\theta} + \\frac{\\partial R}{\\partial \\phi} \\right)\\mathbf{R}^\\text{T} \\\\\n&= \\begin{bmatrix}\n    0 & \\dot{\\phi} \\sin\\theta - \\dot{\\psi} & \\dot{\\theta}\\cos\\psi + \\dot{\\phi}\\cos\\theta\\sin\\psi \\\\\n    -\\dot{\\phi} \\sin\\theta + \\dot{\\psi} & 0 & \\dot{\\theta}\\sin\\psi - \\dot{\\phi}\\cos\\psi\\cos\\theta \\\\\n    -\\dot{\\theta}\\cos\\psi - \\dot{\\phi}\\cos\\theta\\sin\\psi & -\\dot{\\theta}\\sin\\psi + \\dot{\\phi}\\cos\\psi\\cos\\theta & 0\n\\end{bmatrix}\n\\end{aligned}\n\\]\nThe above result is easy to get using the MATLAB script.\nsyms psi theta phi real\nsyms psidot thetadot phidot real\n\nRz = [cos(psi), -sin(psi), 0;\n      sin(psi), cos(psi),  0;\n      0,        0,         1];\n\nRy = [cos(theta),  0, sin(theta);\n      0,           1, 0;\n      -sin(theta), 0, cos(theta)];\n  \nRx = [1, 0,        0;\n      0, cos(phi), -sin(phi);\n      0, sin(phi), cos(phi)];\n  \nR = simplify(Rz*Ry*Rx);\n\nSomega = simplify((diff(R,phi)*phidot + diff(R,theta)*thetadot + diff(R,psi)*psidot) * R')\nNow we are ready to build connections between the angular velocity in world coordinates \\(\\mathbf{\\omega}\\) and the rate of change of Euler angles \\(\\dot{\\mathbf{\\Theta}} = [\\dot{\\phi}, \\dot{\\theta}, \\dot{\\psi}]^\\text{T}\\).\n\\[\n\\mathbf{\\omega} = \\begin{bmatrix}\n    -\\dot{\\theta}\\sin\\psi + \\dot{\\phi}\\cos\\psi\\cos\\theta \\\\\n    \\dot{\\theta}\\cos\\psi + \\dot{\\phi}\\cos\\theta\\sin\\psi \\\\\n    -\\dot{\\phi} \\sin\\theta + \\dot{\\psi}\n\\end{bmatrix}\n= \\begin{bmatrix}\n    \\cos\\theta\\cos\\psi & -\\sin\\psi & 0 \\\\\n    \\cos\\theta\\sin\\psi & \\cos\\psi  & 0 \\\\\n    -\\sin\\theta        & 0         & 1\n\\end{bmatrix}\\begin{bmatrix}\n    \\dot{\\phi} \\\\ \\dot{\\theta} \\\\ \\dot{\\psi}\n\\end{bmatrix} = \\mathbf{E}\\dot{\\mathbf{\\Theta}}\n\\]\nIf the robot is not pointed vertically, which means \\(\\cos\\theta \\neq 0\\), the matrix \\(\\mathbf{E}\\) is invertable. In such case, we can get\n\\[\n\\dot{\\mathbf{\\Theta}} = \\mathbf{E}^{-1}\\mathbf{\\omega} = \\begin{bmatrix}\n    \\frac{\\cos\\psi}{\\cos\\theta} & \\frac{\\sin\\psi}{\\cos\\theta} & 0 \\\\\n    -\\sin\\psi                   & \\cos\\psi                    & 0 \\\\\n    \\cos\\psi\\tan\\theta          & \\sin\\psi\\tan\\theta          & 1\n\\end{bmatrix}\\mathbf{\\omega}\n\\]\nFor small values of roll \\(\\phi\\) and pitch \\(\\theta\\), the above equation can be approximated as\n\\[\n\\dot{\\mathbf{\\Theta}} \\approx \\begin{bmatrix}\n    \\cos\\psi  & \\sin\\psi & 0 \\\\\n    -\\sin\\psi & \\cos\\psi & 0 \\\\\n    0         & 0        & 1\n\\end{bmatrix} \\mathbf{\\omega}\n\\]\nwhich is equivalent to\n\\[\n\\dot{\\mathbf{\\Theta}} \\approx \\mathbf{R}_z^\\text{T} \\mathbf{\\omega}\n\\tag{1}\n\\]\nNote that the order in which the Euler angle rotations are defined is important; with an alternate sequence of rotations, the approximation will be inaccurate for reasonable robot orientations.\n\n\nb. Simplified Single Rigid Body Model\nThe predictive controller models the robot as a single rigid body subject to forces at the contact patches. Although ignoring leg dynamics is a major simplification, the controller is still able to stabilize a high-DoF system and is robust to these multi-body effects.\nFor the Cheetah 3 robot, this simplification is reasonable: the mass of the legs is roughly 10% of the robot's total mass\nFor each ground reaction force \\(\\mathbf{f}_i \\in \\mathbb{R}^3\\), the vector from the CoM to the point where the force is applied is \\(\\mathbf{r}_i \\in \\mathbb{R}^3\\). The rigid body dynamics in world coordinates are given by\n\\[\n\\begin{aligned}\n\\ddot{\\mathbf{p}} &= \\frac{\\sum_{i=1}^n\\mathbf{f}_i}{m} - \\mathbf{g} \\\\\n\\frac{\\text{d}}{\\text{d}t}(\\mathbf{I\\omega}) &= \\sum_{i=1}^n \\mathbf{r}_i \\times \\mathbf{f}_i\\\\\n\\dot{\\mathbf{R}} &= [\\mathbf{\\omega}]\\mathbf{R}\n\\end{aligned} \\tag{2}\n\\]\nwhere \\(\\mathbf{p} \\in \\mathbb{R}^3\\) is the robot’s position in world frame, \\(m \\in \\mathbb{R}\\) is the robot’s mass, \\(\\mathbf{g} \\in \\mathbb{R}^3\\) is the acceleration of gravity, and \\(\\mathbf{I} \\in \\mathbb{R}^3\\) is the robot’s inertia tensor.\n\n\n변수 정리\n\n\n\\(\\mathbf{f}\\): 지면 반발력\n\\(\\mathbf{r}_i\\): 질량 중심(CoM)에서 힘이 작용하는 지점까지의 벡터\n\\(\\mathbf{p}\\): world frame에서 robot position\n\n\n\nThe nonlinear dynamics in the second and third equation of (2) motivate the approximations to avoid the nonconvex optimization that would otherwise be required for model predictive control.\n(2)의 두 번째와 세 번째 방정식에서 나타나는 비선형 동역학은, 모델 예측 제어에서 요구되는 nonconvex 최적화를 피하기 위해 근사화를 도입해야 할 필요성을 제시합니다.\n\n\n2번째 식\nThe second equation in (2) can be approximated with:\n\\[\n\\frac{\\text{d}}{\\text{d}t}(\\mathbf{I\\omega}) = \\mathbf{I\\dot{\\omega}} + \\omega \\times (\\mathbf{I\\omega}) \\approx \\mathbf{I\\dot{\\omega}} = \\sum_{i=1}^n \\mathbf{r}_i \\times \\mathbf{f}_i \\tag{3}\n\\]\nThis approximation has been made in other people’s work. The \\(\\omega \\times (\\mathbf{I\\omega})\\) term is small for bodies with small angular velocities and does not contribute significantly to the dynamics of the robot. The inertia tensor in the world coordinate system can be found with\n이 근사는 다른 연구에서도 사용되었습니다. \\(\\omega \\times (\\mathbf{I\\omega})\\) 항은 작은 각속도를 가지는 물체에서는 작으며, 로봇의 동역학에 크게 영향을 미치지 않습니다. 월드 좌표계에서의 inertia tensor는 다음과 같이 구할 수 있습니다.\n\\[\n\\mathbf{I} = \\mathbf{R}\\mathbf{I}_{\\mathcal{B}}\\mathbf{R}^\\text{T}\n\\]\nwhere \\(\\mathbf{I}_\\mathcal{B}\\) is the inertia tensor in body coordinates. For small roll and pitch angles, This can be approximated by\n\\[\n\\mathbf{\\hat{I}} = \\mathbf{R}_z(\\psi)\\mathbf{I}_{\\mathcal{B}}\\mathbf{R}_z(\\psi)^\\text{T}\n\\tag{4}\n\\]\nwhere \\(\\mathbf{\\hat{I}}\\) is the approximated robot’s inertia tensor in world frame. Combining equations (3)(4), we get\n\\[\n\\mathbf{\\dot{\\omega}} = \\mathbf{\\hat{I}}^{-1}\\sum_{i=1}^n \\mathbf{r}_i \\times \\mathbf{f}_i = \\mathbf{\\hat{I}}^{-1}\\sum_{i=1}^n [\\mathbf{r}_i] \\mathbf{f}_i\n\\tag{5}\n\\]\n\n\n3번째 식\nFor the third equation of (2), we have made the approximation in section 1.a, which gives us \\[\n\\dot{\\mathbf{\\Theta}} \\approx \\mathbf{R}_z^\\text{T} \\mathbf{\\omega}\n\\]\n\n\nc. Continuous-Time State Space Model\nFrom the discussion above, we can write the simplified single rigid body model using equations (1)(2)\n\\[\n\\begin{aligned}\n\\dot{\\mathbf{\\Theta}} &= \\mathbf{R}_z^\\text{T} \\mathbf{\\omega} \\\\\n\\dot{\\mathbf{p}} &= \\dot{\\mathbf{p}} \\\\\n\\mathbf{\\dot{\\omega}} &= \\mathbf{\\hat{I}}^{-1}\\sum_{i=1}^n [\\mathbf{r}_i] \\mathbf{f}_i \\\\\n\\ddot{\\mathbf{p}} &= \\frac{\\sum_{i=1}^n\\mathbf{f}_i}{m} - \\mathbf{g}\n\\end{aligned}\n\\tag{6}\n\\]\nIn matrix form:\n\\[\n\\begin{bmatrix}\n    \\mathbf{\\dot{\\Theta}} \\\\ \\mathbf{\\dot{p}} \\\\ \\mathbf{\\dot{\\omega}} \\\\ \\mathbf{\\ddot{p}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{R}_z^\\text{T} & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{I}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{\\Theta} \\\\ \\mathbf{p} \\\\ \\mathbf{\\omega} \\\\ \\mathbf{\\dot{p}}\n\\end{bmatrix} +\n\\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1]\\\\\n    \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{f}_1 \\\\ \\mathbf{f}_2 \\\\ \\mathbf{f}_3 \\\\ \\mathbf{f}_4\n\\end{bmatrix} +\n\\begin{bmatrix}\n    \\mathbf{0}_{31} \\\\ \\mathbf{0}_{31} \\\\ \\mathbf{0}_{31} \\\\ -\\mathbf{g}\n\\end{bmatrix}\n\\]\nThis equation can be rewritten with an additional gravity state \\(g\\) (note that here \\(g\\) is a scalar) to put the dynamics into the convenient state-space form: \\[\n\\dot{\\mathbf{x}}(t) = \\mathbf{A_c}(\\psi)\\mathbf{x}(t) + \\mathbf{B_c}(\\mathbf{r}_1, \\cdots, \\mathbf{r}_4, \\psi)\\mathbf{u}(t)\n\\tag{7}\n\\]\nwhere \\(\\mathbf{A_c} \\in \\mathbb{R}^{13\\times13}\\) and \\(\\mathbf{B_c} \\in \\mathbb{R}^{13\\times12}\\).\nIn details, we have \\[\n\\begin{bmatrix}\n    \\mathbf{\\dot{\\Theta}} \\\\ \\mathbf{\\dot{p}} \\\\ \\mathbf{\\dot{\\omega}} \\\\ \\mathbf{\\ddot{p}} \\\\ -\\dot{g}\n\\end{bmatrix} = \\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{R}_z^\\text{T} & \\mathbf{0}_3 & 0\\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{I}_3 & 0\\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3 & 0\\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3 & \\mathbf{e}_z\\\\\n    0            & 0            & 0                     & 0            & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{\\Theta} \\\\ \\mathbf{p} \\\\ \\mathbf{\\omega} \\\\ \\mathbf{\\dot{p}} \\\\ -g\n\\end{bmatrix} +\n\\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1]\\\\\n    \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} \\\\\n    0 & 0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{f}_1 \\\\ \\mathbf{f}_2 \\\\ \\mathbf{f}_3 \\\\ \\mathbf{f}_4\n\\end{bmatrix}\n\\tag{8}\n\\]\nThis form depends only on yaw and footstep locations. If these can be computed ahead of time, the dynamics become linear time-varying, which is suitable for convex model predictive control.\n이 형태는 오직 yaw와 발걸음 위치에만 의존합니다. 만약 이것들이 미리 계산될 수 있다면, 동역학은 선형 시간 변화 형태가 되어 convex model predictive control에 적합해집니다.\n\n\nd. Discretization\nSee https://en.wikipedia.org/wiki/Discretization for more details about the purposed method.\n\\[\n\\exp\\left(\\begin{bmatrix}\n          \\mathbf{A_c} & \\mathbf{B_c} \\\\\n          \\mathbf{0}   & \\mathbf{0}\n          \\end{bmatrix} \\text{dt}\\right) =\n          \\begin{bmatrix}\n          \\mathbf{A_d} & \\mathbf{B_d} \\\\\n          \\mathbf{0}   & \\mathbf{I}\n          \\end{bmatrix} \\tag{9}\n\\]\nThis allows us to express the dynamics in the discrete time form\n\\[\n\\mathbf{x}[k+1] = \\mathbf{A_d} \\mathbf{x}[k] + \\mathbf{B_d}[k]\\mathbf{u}[k]\n\\tag{10}\n\\]\nThe above approximation is only accurate if the robot is able to follow the reference trajectory. Large deviations from the reference trajectory, possibly caused by external or terrain disturbances, will result in \\(\\mathbf{B_d}[k]\\) being inaccurate. However, for the first time step, \\(\\mathbf{B_d}[k]\\) is calculated from the current robot state, and will always be correct. If, at any point, the robot is disturbed from following the reference trajectory, the next iteration of the MPC, which happens at most 40 ms after the disturbance, will recompute the reference trajectory based on the disturbed robot state, allowing it compensate for a disturbance.\n위 근사는 로봇이 reference trajectory을 따라갈 수 있을 때만 정확합니다. 외부 요인이나 지형의 방해로 인해 reference trajectory에서 크게 벗어나는 경우, \\(\\mathbf{B_d}[k]\\)가 부정확해질 수 있습니다.\n그러나 first timestep에서는 \\(\\mathbf{B_d}[k]\\)가 현재 로봇 상태에서 계산되므로 항상 정확합니다. 어떤 시점에서든 로봇이 참조 궤적에서 벗어나 방해를 받는다면, 방해가 발생한 후 최대 40ms 이내에 실행되는 MPC의 다음 반복에서, 방해를 받은 로봇 상태를 기반으로 reference trajectory을 재계산하여 방해를 보정할 수 있습니다."
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#force-constraints",
    "href": "post/study/2025-02-17-mpc.html#force-constraints",
    "title": "Linear MPC",
    "section": "2. Force Constraints",
    "text": "2. Force Constraints\n\na. Equality Constraints(등식조건)\nThe equality constraint\n\\[\n\\mathbf{D}_k \\mathbf{u}_k = \\mathbf{0} \\tag{11}\n\\]\nis used to set all forces from feet off the ground to zero, enforcing the desired gait, where \\(\\mathbf{D}_k\\) is a matrix which selects forces corresponding with feet not in contact with the ground at timestep \\(k\\).\n지면에서 떨어진 발들에 작용하는 모든 힘을 0으로 설정하여 원하는 gait을 강제하기 위해 사용됩니다. 여기서 \\(\\mathbf{D}_k\\)는 시간 단계 \\(k\\)에서 지면에 접촉하지 않은 발들에 해당하는 힘을 선택하는 행렬입니다.\n\n\nb. Inequality Constraints(부등식조건)\nThe inequality constraints limit the minimum and maximum \\(z\\)-force as well as a square pyramid approximation of the friction cone.\n부등식 제약 조건은 최소 및 최대 \\(z\\)-힘을 제한하며, friction cone의 사각뿔 근사를 포함합니다.\nFor each foot, we have the following 10 inequality constraints (\\(i = 1,2,3,4\\)).\n\\[\n\\begin{aligned}\nf_{\\min} \\leq &f_{i,z} \\leq f_{\\max} \\\\\n-\\mu f_{i,z} \\leq  &f_{i,x} \\leq \\mu f_{i,z} \\\\\n-\\mu f_{i,z} \\leq  &f_{i,y} \\leq \\mu f_{i,z}\n\\end{aligned}\n\\]\nWe want to write these constraints in matrix form. Thus we need to look these equations in detail.\nFor example, the constraints \\(-\\mu f_{i,z} \\leq \\pm f_{i,x} \\leq \\mu f_{i,z}\\) actually are \\[\n\\begin{aligned}\n-\\mu f_{i,z} &\\leq f_{i,x} \\\\\n-\\mu f_{i,z} &\\leq -f_{i,x} \\\\\nf_{i,x} &\\leq \\mu f_{i,z} \\\\\n-f_{i,x} &\\leq \\mu f_{i,z}\n\\end{aligned}\n\\]\nWe can rewrite these equations as\n\\[\n\\begin{aligned}\nf_{i,x} + \\mu f_{i,z} &\\geq 0 \\\\\n-f_{i,x} + \\mu f_{i,z} &\\geq 0 \\\\\nf_{i,x} - \\mu f_{i,z} &\\leq 0 \\\\\n-f_{i,x} - \\mu f_{i,z} &\\leq 0\n\\end{aligned}\n\\]\nWe can see that the first two equations and the last two equations are the same. Thus we can use the first two equations to replace these four equations. We can also note that \\(f_{\\min} = 0\\) as always. With the observation discussed above, we can write the force constraints for one foot on the ground as\n\\[\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n\\leq\n\\begin{bmatrix}\n1 & 0 & \\mu \\\\\n-1 & 0 & \\mu \\\\\n0 & 1 & \\mu \\\\\n0 & -1 & \\mu \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{i,x} \\\\ f_{i,y} \\\\ f_{i,z}\n\\end{bmatrix}\n\\leq\n\\begin{bmatrix}\n\\infty \\\\ \\infty \\\\ \\infty \\\\ \\infty \\\\ f_{\\max}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#reference-trajectory-generation",
    "href": "post/study/2025-02-17-mpc.html#reference-trajectory-generation",
    "title": "Linear MPC",
    "section": "3. Reference Trajectory Generation",
    "text": "3. Reference Trajectory Generation\nThe desired robot behavior is used to construct the reference trajectory. In the application, our reference trajectories are simple and only contain non-zero \\(xy\\)-velocity, \\(xy\\)-position, \\(z\\)-postion, yaw, and yaw rate. All parameters are commanded directly by the robot operator except for yaw and \\(xy\\)-position, which are determined by integrating the appropriate velocities. The other states (roll, pitch, roll rate, pitch rate and \\(z\\)-velocity) are always set to 0. The reference trajectory is also used to determine the dynamics constraints and future foot placement locations.\nIn practice, the reference trajectory is short (between 0.5 and 0.3 seconds) and recalculated often (every 0.05 to 0.03 seconds) to ensure the simplified dynamics remain accurate if the robot is disturbed."
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#qp-formulation",
    "href": "post/study/2025-02-17-mpc.html#qp-formulation",
    "title": "Linear MPC",
    "section": "4. QP Formulation",
    "text": "4. QP Formulation\n\na. Batch Formulation for Dynamic Constraints\nKey idea: For the state space model, express \\(x_0, x_1, \\cdots, x_k\\) as function of \\(u_0\\).\n\\[\n\\begin{aligned}\nx_1 &= Ax_0 + Bu_0 \\\\\nx_2 &= Ax_1 + Bu_1 = A^2x_0 + ABu_0 + Bu_1 \\\\\n&\\vdots \\\\\nx_k & = A^kx_0 + A^{k-1}Bu_0 + A^{k-2}Bu_1 + \\cdots + Bu_{k-1}\n\\end{aligned}\n\\]\nWe can write these equations in matrix form\n\\[\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k\n\\end{bmatrix} =\n\\begin{bmatrix}\nA \\\\ A^2 \\\\ \\vdots \\\\A^k\n\\end{bmatrix}\nx_0\n+\n\\begin{bmatrix}\nB & 0 & \\cdots & 0 \\\\\nAB & B & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA^{k-1}B & A^{k-2}B & \\cdots & B\n\\end{bmatrix}\n\\begin{bmatrix}\nu_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{k-1}\n\\end{bmatrix}\n\\]\nwhere \\(k\\) is the horizon length. Let \\(x_t\\) denotes the system state at time step \\(t\\), i.e. at current state, then we can write\n\\[\n\\begin{bmatrix}\nx_{t+1} \\\\ x_{t+2} \\\\ \\vdots \\\\ x_{t+k}\n\\end{bmatrix} =\n\\begin{bmatrix}\nA \\\\ A^2 \\\\ \\vdots \\\\A^k\n\\end{bmatrix}\nx_t\n+\n\\begin{bmatrix}\nB & 0 & \\cdots & 0 \\\\\nAB & B & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA^{k-1}B & A^{k-2}B & \\cdots & B\n\\end{bmatrix}\n\\begin{bmatrix}\nu_t \\\\ u_{t+1} \\\\ \\vdots \\\\ u_{t+k-1}\n\\end{bmatrix}\n\\]\nWe can denote this equation as\n\\[\nX = S^x x_t + S^u U\n\\]\nFor a 2-norm cost function, we can write\n\\[\n\\begin{aligned}\nJ_k(x_t,\\mathbf{U}) &= J_f(x_k) + \\sum_{i=0}^{k-1}l(x_i,u_i) \\\\\n&= x_k^\\text{T} Q_f x_k + \\sum_{i=0}^{k-1}\\left(x_i^\\text{T}Q_ix_i + u_i^\\text{T}R_iu_i\\right) \\\\\n&= X^\\text{T}\\bar{Q}X + U^\\text{T}\\bar{R}U\n\\end{aligned}\n\\]\nwhere\n\\[\n\\mathbf{U} =\n\\begin{bmatrix}\n    X \\\\ U\n\\end{bmatrix} \\quad\n\\bar{Q} = \\begin{bmatrix}\n    Q_1 & & & \\\\\n     & \\ddots & & \\\\\n     & & Q_{k-1} & \\\\\n     & & & Q_f\n\\end{bmatrix} \\quad\n\\bar{R} = \\begin{bmatrix}\n    R & & \\\\\n     & \\ddots & \\\\\n     & & R \\\\\n\\end{bmatrix}\n\\]\nSubstituting the expression of state space model into the cost, we have\n\\[\n\\begin{aligned}\nJ_k(x_t,\\mathbf{U}) &= X^\\text{T}\\bar{Q}X + U^\\text{T}\\bar{R}U \\\\\n&= \\left(S^x x_t + S^u U\\right)^\\text{T}\\bar{Q}\\left(S^x x_t + S^u U\\right) + U^\\text{T}\\bar{R}U \\\\\n&= U^\\text{T}\\underbrace{\\left((S^u)^\\text{T}\\bar{Q}S^u+\\bar{R}\\right)}_H U + 2x_t^\\text{T}\\underbrace{(S^x)^\\text{T}\\bar{Q}S^u}_F U + x_t^\\text{T}\\underbrace{(S^x)^\\text{T}\\bar{Q}S^x}_Y x_t\\\\\n&= U^\\text{T}HU + 2x_tFU + x_t^\\text{T}Yx_t\n\\end{aligned}\n\\] Compare with the standard form of cost in QP formulation \\(\\frac{1}{2}U^\\text{T}HU + U^\\text{T}g\\), we can easily get \\[\n\\begin{aligned}\nH &= 2\\left((S^u)^\\text{T}\\bar{Q}S^u + \\bar{R}\\right) \\\\\ng &= 2(S^u)^\\text{T}\\bar{Q}S^xx_t\n\\end{aligned}\n\\]\n\n\nb. Create QP Cost\nRecall that the form of our MPC problem is\n\\[\n\\begin{aligned}\n\\min_{x, u} \\quad& \\sum_{i=0}^{k-1}(x_{i+1}-x_{i+1,\\text{ref}})^{\\text{T}}Q_i(x_{i+1}-x_{i+1,\\text{ref}}) + u_i^\\text{T}R_iu_i \\\\\n\\text{s.t.} \\quad& x_{i+1} = A_ix_i + B_iu_i \\\\\n& \\underline{c}_i \\leq C_iu_i \\leq \\overline{c}_i \\\\\n& D_iu_i = 0\n\\end{aligned}\n\\]\nwhere \\(i = 0, \\cdots, k-1\\).\nIn this project, we choose \\(Q_1 = \\cdots = Q_{k-1} = Q_f\\), i.e.\n\\[\n\\bar{Q} = \\begin{bmatrix}\n    Q & & & \\\\\n     & \\ddots & & \\\\\n     & & Q & \\\\\n     & & & Q\n\\end{bmatrix} \\quad\n\\bar{R} = \\begin{bmatrix}\n    R & & \\\\\n     & \\ddots & \\\\\n     & & R \\\\\n\\end{bmatrix}\n\\]\n_Qi = np.diag(np.array(parameters['Q'], dtype=float))\nQbar = np.kron(np.identity(horizon), _Qi)\n\n_r = parameters['R']\n_Ri = _r * np.identity(num_input, dtype=float)\nRbar = np.kron(np.identity(horizon), _Ri)\nLet’s substitute the dynamic constraint into the cost function, then we can get\n\\[\n\\begin{aligned}\nJ &= \\sum_{i=0}^{k-1}(x_{i+1}-x_{i+1,\\text{ref}})^{\\text{T}}Q_i(x_{i+1}-x_{i+1,\\text{ref}}) + u_i^\\text{T}R_iu_i \\\\\n&= \\sum_{i=0}^{k-1}(A_ix_i+B_iu_i-x_{i+1,\\text{ref}})^{\\text{T}}Q_i(A_ix_i + B_iu_i-x_{i+1,\\text{ref}}) + u_i^\\text{T}R_iu_i \\\\\n&= (S^x x_t + S^u U - X_{\\text{ref}})^\\text{T}\\bar{Q}(S^x x_t + S^u U - X_\\text{ref}) + U^\\text{T}\\bar{R}U\n\\end{aligned}\n\\]\nwhere \\(X_\\text{ref} = [x_{1,\\text{ref}}, \\cdots, x_{k,\\text{ref}}]^\\text{T}\\). We can get the QP matrix with the same procedure.\n\\[\n\\begin{aligned}\nH &= 2\\left((S^u)^\\text{T}\\bar{Q}S^u + \\bar{R}\\right) \\\\\ng &= 2(S^u)^\\text{T}\\bar{Q}(S^xx_t-X_\\text{ref})\n\\end{aligned}\n\\]\n\n\nc. Create QP Constraint\n\n\nd. QP Solver\nqpsolvers: https://pypi.org/project/qpsolvers/\nAs mentioned in the project description of qpsolvers, we need to write our formulation in the form\n\\[\n\\begin{aligned}\n\\min_x &\\quad \\frac{1}{2}x^\\text{T}Px + q^\\text{T}x \\\\\n\\text{s.t.} & \\quad Gx \\leq h \\\\\n&\\quad Ax = b \\\\\n&\\quad \\text{lb} \\leq x \\leq \\text{ub}\n\\end{aligned}\n\\]\nThen we can get the solution using the code below\nfrom qpsolvers import solve_qp\nx = solve_qp(P, q, G, h, A, b, lb, ub)\nprint(\"QP solution: x = {}\".format(x))\nThe QP problem in the paper is written as\n\\[\n\\begin{aligned}\n\\min_\\mathbf{U} &\\quad \\frac{1}{2}\\mathbf{U}^\\text{T}\\mathbf{HU} + \\mathbf{U}^\\text{T}\\mathbf{g} \\\\\n\\text{s.t.} &\\quad \\underline{\\mathbf{c}} \\leq \\mathbf{CU} \\leq \\overline{\\mathbf{c}}\n\\end{aligned}\n\\]\n\n\nReference\n[1] Di Carlo J., Wensing P. M., Katz B., et al. Dynamic Locomotion in the MIT Cheetah 3 Through Convex Model-Predictive Control[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018: 1-9. [PDF]\n[2] Bledt G, Powell M J, Katz B, et al. Mit cheetah 3: Design and Control of a Robust, Dynamic Quadruped Robot[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018: 2245-2252. [PDF]"
  },
  {
    "objectID": "post/paper/2025-02-26-papers.html",
    "href": "post/paper/2025-02-26-papers.html",
    "title": "[5] 3mins papers",
    "section": "",
    "text": "강화학습(Reinforcement Learning, RL)을 활용하여 사족보행 로봇의 점프 동작을 구현하는 동시에, 시뮬레이션-현실 간의 성능 차이(sim-to-real gap)를 줄이는 새로운 기법을 제안함. 이를 위해서 주파수 영역에서의 임피던스 매칭 기법을 활용하였음\n기존의 연구는 Domain gap, 무작위의 Domain randomization, 하나의 정책(점프 등)만을 학습, model-based의 reference motion을 참조 등이 필요했으나 본 연구에서는 임피던스 매칭 기법을 통해 Domain gap을 줄이고 reference없이 다양한 모션들을 수행할 수 있는 학습 방법을 제안함\n지금까지의 연구에서는 점프 모션의 경우, model-based controller나 reference를 활용학습하는 것이 일반적이었으나, 본 논문은 순수 강화학습만으로 55cm의 거리를 점프하는 모습을 보임. 매우 흥미로워서 좀 더 깊이 파보고자 함.\nPaper Link"
  },
  {
    "objectID": "post/paper/2025-02-26-papers.html#section-1",
    "href": "post/paper/2025-02-26-papers.html#section-1",
    "title": "[5] 3mins papers",
    "section": ">",
    "text": "&gt;\n\n\n\nPaper Link\nCode"
  },
  {
    "objectID": "post/paper/2025-02-26-papers.html#section-3",
    "href": "post/paper/2025-02-26-papers.html#section-3",
    "title": "[5] 3mins papers",
    "section": ">",
    "text": "&gt;\n\n\n\nPaper Link\nCode"
  },
  {
    "objectID": "post/paper/2025-02-26-papers.html#section-5",
    "href": "post/paper/2025-02-26-papers.html#section-5",
    "title": "[5] 3mins papers",
    "section": ">",
    "text": "&gt;\n\n\n\nPaper Link\nCode"
  },
  {
    "objectID": "post/paper/2025-02-20-papers.html",
    "href": "post/paper/2025-02-20-papers.html",
    "title": "[2] 3mins papers",
    "section": "",
    "text": "Actor-Cross-Critic\n\nLeveraging Privileged Information for Partially Observable Reinforcement Learning\n\n\nVAE 등을 사용하는 방법들은 간접적인 정보 제공을 하기 때문에 Asymmetric Actor Critic 구조의 문제를 해결해야 함\n두개의 Critic 네트워크들을 학습하고 Advantage가 더 큰 값을 선택하여 Actor에게 제공하는 방법. Oracle Critic과 Executor Critic 2개의 네트워크를 가지고 Overestimation 문제를 방지\n이러한 ACC의 학습 과정이 기존의 Actor Critic 방법보다 더 안정적이고 빠르게 수렴함을 증명\nPaper Link\n\n\n\n\nANYmal Parkour\n\nLearning Agile Navigation forQuadrupedal Robots\n\n\n완전 학습 기반(fully learned) 네비게이션: 환경의 3D 재구성 정보를 활용하여 보행 기술을 동적으로 선택.\n하이브리드 정책 구조: PPO 기반으로 Gaussian 분포(저수준 명령) + 범주형 분포(기술 선택)를 결합.\n새로운 보행 기술 학습: 점프, 등반, 웅크리기 등 다양한 역동적 동작을 학습하여 높은 장애물도 극복 가능.\n신경망 기반 3D 환경 재구성: 다중 해상도 맵핑을 통해 로봇 주변은 고해상도로, 멀리 있는 환경은 저해상도로 인식하여 실시간 처리 성능을 확보.\n지금까지 연구해온 연구 내용들을 총망라하여 perception 부터 agile한 control까지 가능하게 만드는 시스템을 구축한 느낌을 받음\nPaper Link\n\n\n\n\nStiffness Tuning\n\nVariable Stiffness for Robust Locomotion through Reinforcement Learning\n\n\n강화학습 보행 제어에서, joint stiffness 튜닝 없이도 성능을 유지하면서 변수 stiffness p gain을 action space에 통합하는 새로운 제어 패러다임을 제안.\nstiffness을 관절별(PJS), 다리별(PLS), 하이브리드(HJLS)로 그룹화하여 제어하는 방식을 적용하여 PLS는 속도 추적 및 외력 대응에서 우수하며, HJLS는 에너지 효율성을 극대화하는 결과를 보여줌\nPd 튜닝 중 d 게인은 p 게인에 의존 변수로 놓았고 기존의 action space 디자인을 색다르게 했다는 점에서 참신한 것 같음. 강화학습 보행제어에서 관습적으로 하는 부분을 건드렸다는 측면에서 새로웠음.\nPaper link\n\n\n\n\nPrivileged Information\n\nProvable Partially Observable Reinforcement Learning with Privileged Information\n\n\n강화학습은 무조건 적으로 Partially Observable MDP (POMDP) 환경이며, 학습 단계에서 시뮬레이션에서만 얻을 수 있는 노이즈가 없는 깨끗한 데이터인 특권 정보 (Privileged Information) 이 존재함. 예를 들어 노이즈가 없는 완전한 상태 정보를 이용하면 정책을 학습하는데 도움이 되며, 실제 실행에서는 전문가 증류 (expert distillation) 과 비대칭 액터-크리틱 (훈련과 실행 단계에서 다른 관찰값을 사용하는 방식 - 크리틱은 특권 정보를 활용하여 평가, 액터는 제한된 관찰값만을 사용)을 이용하여 현실 데이터에 적용함.\n최근 증류 기법이 여러 학습에서 매우 뛰어난 성능을 보이고 있음. 그러나 최근 적용되고 있는 증류기법은 항상 최적 정책을 보장하지 않으며, 근사 최적 정책을 학습하는 과정에서 손실이 발생함. 저자들은 POMDP에서 관찰값을 통해 현재의 진짜 상태 (state)를 결정할 수 있는 조건을 만족하는지에 대한 여부를 결정론적 필터 조건 (Deterministic Filter Condition)으로 정의함. 이 조건, 즉 필터가 결정론적일 경우에 (관찰값이 주어지면 현재 상태를 정확하게 추론이 가능할 때), 전문가 증류의 손실을 방지할 수 있음. 예시로, 로봇 손을 이용한 물체 조작은 (조건: 로봇이 손에 있는 물체의 위치와 방향을 정확히 측정할 수 있다면, 결과: 관찰을 통해 물체의 실제 상태를 결정할 수 있음) 이 조건을 만족함. 자율 주행 차량에서는 (조건: 도로의 미끄러움 정도는 차량 센서로 알 수 없음. 결과: 센서값이 동일해도 도로의 미끄러움 정도는 달라질 수 있음) 이 조건을 만족하지 않음.\n이 논문은 NIPS 논문이며, 수식적 증명이 들어가는 논문으로 72 페이지의 분량을 가지고 있네요. 리뷰어들이 고생을 많이 했겠습니다.. 이 논문의 아이디어를 차용해 보자면, 증류 및 비대칭 액터-크리틱은 적용할 수 있겠습니다. 다만, 사족보행 로봇은 이 ’결정론적 필터 조건’이 충족되지 않을 조건을 너무 많이 가지고 있기에, 이 논문의 아이디어를 적용해 보려면 적당한 assumption을 통해서 lower bound를 찾아내어 적용해 보는 것도 재미있겠다는 생각이 드네요\nPaper Link"
  },
  {
    "objectID": "post/paper/2025-02-28-papers.html",
    "href": "post/paper/2025-02-28-papers.html",
    "title": "[6] 3mins papers",
    "section": "",
    "text": "Combining Learning-based Locomotion Policy with Model-based Manipulation for Legged Mobile Manipulators\n\n3줄요약\n\nMPC를 사용하는 매니퓰레이터와 RL을 사용하는 4족로봇의 통합\n매니퓰레이터가 움직이면, 4족로봇은 그 외란을 예측해서 로봇이 균형을 유지하고 안정적으로 이동할 수 있게 함.\nRL은 매니퓰레이터의 동역학 모델을 직접적으로 사용하지 않음. 시뮬레이션에서 외란을 가해준다음에 MSE를 통해 예측모델을 만드는것임.\n\n\n\nTraining in Simulation\n\n로봇은 시뮬레이션 환경에서 무작위 힘의 조합인 wrench sequence를 적용받음. 학습과정에서 이 렌치 시퀀스 예측을 수행함.\n학습중에는 별도의 매니퓰레이터 제어기가 필요하지 않음. legged gym에서 push robot과 같은 함수를 지속적으로 가해주는것과 동일함.\n\n\n\nResults\n\n매니퓰레이터의 다양한 동작과 무게 변화에 대한 적응성이 생김.\n제어 시스템을 4족로봇, 매니퓰레이터 컨트롤러 두개의 모듈로 분리하여 각 모듈의 독립적인 개발 및 최적화를 가능하게 함.\n분명 이전에 이거랑 같은 컨셉인 시뮬레이션 논문이 아카이브에 올라왔었는데 지금은 못찾겠음. 확실히 직관적인 아이디어여도 실제 하드웨어로 실험을 해야 논문이 되는것같다.\nPaper Link\n\n \n\n\n\nFoundation Models in Robotics: Applications, Challenges, and the Future\n\n강화학습 + MPC 주제쪽으로 보고 있지만 이번에는 관심을 가지고 있는 Foundation model에 대해서 가지고 와봤다. 본 논문은 Foundation model이 로보틱스에서 어떤식으로 활용되어 왔고, 어떻게 연구가 될지에 대해서 논의하는 논문이다. Foundation model을 통해 로봇의 인지, 의사결정, 제어의 능력을 향상시키는 방법을 분석한다.\n특히, 기초 모델을 활용한 로봇 정책 학습과 강화학습의 접점으로 Language-Assisted Reinforcement Learning (LLM 기반 보상 학습 및 탐색), Vision-Language Value Learning (VLM을 활용한 Value Function 학습), Robotics Transformers (Transformer 기반 행동 정책 학습) 등이 제시되었으며, 이는 강화학습 기반의 제어기 설계에 유용할 수 있다.\n다만, 실시간 응답 속도 문제, 데이터 부족, 불확실성 정량화 부족 등의 한계가 존재하며, RL과 MPC를 결합한 제어기 학습을 위한 foundation model의 직접적인 적용 사례는 부족하다. 향후 연구 방향으로 RL과 결합 가능한 경량화된 foundation model 개발 및 로봇 제어 특화된 데이터셋 구축이 필요할 것으로 보인다.\nPaper Link\nYoutube"
  },
  {
    "objectID": "goal.html",
    "href": "goal.html",
    "title": "Goal",
    "section": "",
    "text": "목표로 제출하고자 하는 학회나 저널 제출기간 확인\n\n\n\n\n\nCORL 2025\nhttps://www.corl.org/home\n\n\n\nICRA 2026\n\n\n\nIROS 2026"
  },
  {
    "objectID": "post/paper/2025-02-18-papers.html",
    "href": "post/paper/2025-02-18-papers.html",
    "title": "[1] 3mins papers",
    "section": "",
    "text": "DreamFLEX\n\nLearning Fault-Aware Quadrupedal Locomotion Controller for Anomaly Situation in Rough Terrains\n\n\n로봇이 실제 오래 구동했을 시에 마주칠 수 있는 하드웨어 결함에 대응하는 방법론, joint의 결함이 생기면 이를 감지하여 결함이 있는 상황에 맞춰서 보행 패턴을 재구성\nDreamWaQ의 컨셉을 유지하며 CENet을 FEMNet으로 바꾸어 조인트 결함을 감지하는 것과 연관있는 term \\(f_t\\) 추가\nJoint Fault의 상황은 locked joint와weakened motor` 2가지 경우에 대해 상황을 정의하여 결함이 있는 발을 제외하고서라도 보행하는 것이 목적\nPaper Link\n\n\n\n\nRL-augmented MPC\n\nLearning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC\n\n\nRL을 보완하는 MPC 프레임워크 개발 고속 이동, 모델 불확실성 적응, 장애물 회피를 포함하는 적응형 블라인드(blind) 보행을 실현\n스탠스 발 힘 제어(stance foot force control)와 스윙 발 반사(swing foot reflection)를 결합 기존 MPC가 직면한 스윙 발 궤적의 불확실성 문제를 해결 모델 불확실성(model uncertainty)에 적응하면서 로봇의 균형 유지 능력을 개선\n로봇-독립적(robot-agnostic) RL 모듈 도입 다양한 로봇 플랫폼(Unitree A1, Go1, AlienGo)에서 zero-shot transfer 가능 RL 정책이 특정 로봇에 최적화되지 않고 다양한 환경에서도 일반화 가능\nstance phase와 swing phase를 분리하는 것은 당연한 전제로 생각했었는데, 이를 혼합하려는 시도는 신선하게 느껴짐\nPaper Link\n\n\n\n\nCAIMAN\n\nCausal Action Influence Detection for Sample Efficient Loco-manipulation\n\n\n4족보행 로봇이 크고 무거운 물체를 조작하며 이동할 수 있도록 비-파지(non-prehensile) 방식으로 크고 무거운 물체를 조작하며 이동(loco-manipulation)하는 Task를 학습\ncausal action influence(인과적 행동 영향, CAI)을 활용하여 로봇이 환경을 제어하는 상태를 탐지하고, 이를 내재적 동기(intrinsically motivated objective)로 삼아 hierarchical control strategy을 통해 학습\n물체를 미는 물리적 결과에 대한 동역학 모델을 학습하여 CAI 값을 끌어낸다는 점이 특이해서 흥미로웠으며, 최근에 Loco-Manipulation이라고 하면 보통 매니퓰레이터를 간단한거라도 다는데 여기에서는 그냥 밀기였음\nPaper Link\n\n\n\n\nSafe RL/MPC Testbed\n\n로봇의 변화 가능한 환경에서의 MPC, RL의 안정성 평가 Testbad 정의\n\n\nSafeRL/MPC라고 하면, 어떤 환경에서 안정성을 평가하는 것이 합리적인가\n로봇에 요구되는 Task가 다양해짐에 따라 평가할 수 있는 적절한 Testbed 및 기준 필요\n환경변화로 “로봇 외부의 환경 변화”, “범용적 임무 수행을 위한 로봇의 동작으로 인해 발생된 환경 변화”, “로봇의 조정 가능한 HW, 부착물의 위치, 구성으로 인해 발생된 환경 변화”를 생각해볼 수 있음"
  },
  {
    "objectID": "post/paper/2025-02-24-papers.html",
    "href": "post/paper/2025-02-24-papers.html",
    "title": "[4] 3mins papers",
    "section": "",
    "text": "Paper Link\nCode\n\n\n\nRLOC: Terrain-Aware Legged Locomotion using Reinforcement Learning and Optimal Control\n\n2020년 논문이지만, 계속해서 Privileged Information 컨셉을 유지하면서, 관련 논문을 찾고 있습니다. - 이 논문은 강화학습(RL)과 모델 기반 제어를 통합하여 사족보행 로봇이 다양한 지형에서 동적 보행을 수행할 수 있도록 하는 프레임워크를 제안합니다. 이 접근법은 온보드 고유수용성(proprioceptive) 및 외부수용성(exteroceptive) 피드백을 활용하여 센서 정보와 원하는 속도 명령을 발걸음 계획으로 매핑하는 RL 정책을 학습합니다. 학습된 정책은 모델 기반 모션 컨트롤러와 결합되어 복잡한 지형에서도 안정적인 보행을 구현합니다. 또한, 신체 전체의 움직임 추적 및 회복 제어를 위한 보조 RL 정책을 도입하여 물리적 파라미터의 변화와 외부 교란에 대응합니다. 이 프레임워크는 ANYmal B 및 ANYmal C 로봇 플랫폼에서 재학습 없이도 성공적으로 적용되었습니다.\n\n\n이 논문은 “Provable Partially Observable Reinforcement Learning with Privileged Information” 연구와 연결 지어 해석할 수 있습니다. 특히, 시뮬레이션에서 특권 정보(privileged information)를 활용하여 RL 정책을 학습하고, 실제 환경에서는 제한된 센서 정보만으로도 안정적인 보행을 구현하는 접근법은 두 연구의 공통된 특징입니다. 이는 특권 정보의 활용이 결정론적 필터 조건(deterministic filter condition)을 만족하지 않더라도, 적절한 제약 조건을 통해 안전한 강화학습 기반 보행 컨트롤러를 설계할 수 있음을 시사합니다.\n이러한 통찰을 바탕으로, 특권 정보를 활용한 교사-학생 학습(teaching-student learning)과 비대칭 액터-크리틱(asymmetric actor-critic) 방법을 통해 사족보행 로봇의 복잡한 지형에서의 안전한 보행을 구현할 수 있습니다. 또한, 적절한 제약 조건을 도입하여 강화학습 기반 보행 컨트롤러의 안정성과 안전성을 향상시킬 수 있을 것으로 기대됩니다.\nPaper Link\nVideo\n\n\n\nSmall Home Robot\n\nLearning Quiet Walking for a Small Home Robot\n\n\n지금까지의 연구는 강화학습이 얼마나 제어기를 강건하게 만들었느냐, 혹은 얼마나 fancy한 움직임을 만들었느냐에 초점을 두어왔었음. 하지만, 본 논문은 실제 가정에 로봇이 투입이 되었을 때 불편함을 느낄 가능성이 높은 “소음”에 초점을 맞추어 연구를 진행했다. 이처럼, 기술적인 어려움보다도 실제 삶에 있어서 도움이 될만한 기술을 연구하는 것 또한 좋은 방향으로 보인다.\n기술적인 어려움은 크게 없으며 curriculum learning을 통해 1st phase에서는 소음을 신경쓰지 않는 모션을 학습하고, 2nd phase에서 조용한 모션을 학습하도록 만든다. 네트워크의 output은 “PD Gain의 Scale”과 “ Target Joints Position”을 내보내 stiff한 모션을 만들지 damping 있는 모션을 만들지 조정한다.\n본 실험(slope 극복실험)에 따르면 제어기의 강건도와 조용함이 반비례 관계임을 확인할 수 있었다. 미래에는 perceptive sensor를 통해 좀 더 고도화된 보행전략을 취하도록 만들 수 있을 것이다. 또한, 발자국 사운드와 관련이있는 발 접촉 속도를 최소화하는 간접 reward를 줌으로써 문제를 해결했는데 이는 토크를 줄이므로써 배터리 효율성을 증가시키는 방식 등으로 활용이 가능할 것이다.\nPaper Link\nYoutube\n\n\n\n\nMI-HGNN\n\nMorphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception\n\n\nMI-HGNN은 로봇의 관절과 링크를 각각 노드와 엣지로 모델링하여, 기존 모델 대비 8.4% 성능 향상과 탁월한 파라미터 효율성, 일반화 및 샘플 효율성을 달성함\n로봇의 형태학(morphology) 정보를 반영한 이종 그래프 신경망(Heterogeneous Graph Neural Network) 아키텍처에 모델 기반 제약을 통합하여, 실제와 시뮬레이션 데이터에서 접촉 인식 문제를 효과적으로 해결\n이전에 미니 프로젝트로 했었던 GNN 아이디어는 단순히 보행하는 rl policy에 간접적인 정보를 넣어주는 걸로 진행했었는데 이 논문에서 GNN은 보다 직접적으로 ground reaction forces (GRFs)를 예측하는 학습을 시켰다는 점이 새로웠으며 제어하는데 GRF 정보가 필요할 때 한 모듈로 이용해보면 좋겠다는 생각이 들었음\nPaper Link"
  },
  {
    "objectID": "post/paper/2025-02-22-papers.html",
    "href": "post/paper/2025-02-22-papers.html",
    "title": "[3] 3mins papers",
    "section": "",
    "text": "Hybrid Internal Model\n\nLearning Agile Legged Locomotion with Simulated Robot Response\n\n\nContrasive Learning(SwaV 기반)을 통해 Latent vector 생성. Positive Pair와 Negative Pair를 분류하여 비슷한 환경에서는 Latent vector들이 비슷한 값을 갖도록, 다른 환경에서는 다른 표현을 갖도록 하는 클러스터링 방법을 사용하여 Latent vector space 공간을 구성하도록 함.\n직접적으로 환경 정보를 예측하지 않고 간접적으로 proprioceptive data를 기반으로 지형에 대응하는 방법임\n코드가 공개되어 있고 성능이 좋아보이며, 지형에 따라 Latent가 구분되는 것을 확인할 수 있으며 DreamWaQ보다 뚜렷하게 구분되는 특징을 확인할 수 있음\nPaper Link\nCode\n\n\n\n\nLearning Quadrupedal Locomotion over Challenging Terrain\n\n정말 유명한 논문임. 극도로 강건한 보행 컨트롤러 설계를 위해 proprioceptive (고유수용성 - 내계 정보 = joint encoder, IMU, etc) 만을 사용해 사족보행 로봇의 진흙, deformable terrain, 자갈, 동적인 장판, 식생, 거센 물살에서도 동작하게 만들 수 있음. 더 단순한 도메인에서 학습을 수행해도, 실제 자연 환경에서의 강건함을 극대화 할 수 있음.\n구현을 위해 교사 정책을 학습하였음. 시뮬레이션에서 지면의 정확한 높이, 경사도, 접촉 상태와 같은 특권 (privileged) 정보를 활용하여 정책을 학습하고, 실제 로봇에서는 student policy를 사용하여 고유수용성 데이터만을 입력으로 사용하는 학생 정책을 학습함.\n지난 논문인 Provable Partially Observable Reinforcement Learning with Privileged Information와 연결해 보면, 결국 이렇게 학습을 해서 동작을 하는 이유는, ’특권 정보’의 활용이 결정론적 필터 조건 (Deterministic Filter Condition) 조건을 만족하지 않더라도, 일부 제약 조건을 만족할 수 있다는 뜻으로 해석될 수 있음. 적절한 constraint 를 걸어서 lower bound를 만든다면, safe condition (constraint)를 만족할 수 있는 강화학습 보행 컨트롤러를 만들수 있지 않을까 생각됩니다.\nPaper Link\n\n\n\n\nExtreme Parkour with Legged Robots\n\n저비용의 센서(depth 카메라 한개)와 저렴한 사족보행로봇(actuation이 부정확함)을 활용하여 매우 뛰어난 수준의 파쿠르 동작을 수행하는 모델을 만드는 방법에 대해서 소개하는 논문임.\n이는 기존의 파쿠르와 같은 동작을 수행하는 논문들보다 훨씬 간단하면서도 저비용으로 실현할 수 있음을 증명했다. ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robots의 경우 구현을 하기 위해서는 mapping module 따로, planning module따로, control module 따로 필요했었지만, 이 방식은 하나의 네트워트만 학습시켜주면 된다. 이때 학습 기법은 위 이미지와 같이 privileged information을 통해 완성도 있는 teacher 모델을 만들고, 이를 Dual Distillation 학습 기법을 통하여 student 모델을 만든다. (phase 1: 전문가 데이터를 활용한 지도학습 / Phase 2: 지도 데이터 없이 로봇이 스스로 방향을 예측하며 학습)\n영상을 확인한 후, 학습이 매우 잘 이루어졌으며 sim-to-real 전이도 성공적으로 수행되었음을 알 수 있었다. 그러나 이러한 방식으로 학습된 네트워크가 충분히 안전한지에 대해서는 다소 의문이 든다. 특히, 좌우 및 후방에 대한 정보 없이 높은 속도로 전진하는 네트워크는 예상치 못한 위험을 초래할 가능성이 있어 보인다. 또한, 이전 연구에서는 상단 장애물까지 고려하여 로봇이 몸을 숙이는 동작을 학습하는 등 더 다양한 정보를 활용하여 성능을 향상시켰다. 따라서 단순히 저비용 네트워크를 구축하는 것이 반드시 최선의 방법이라고는 생각되지 않는다. 비용 절감도 중요하지만, 보다 풍부한 센서 정보를 활용하여 안전성과 전반적인 성능을 극대화하는 접근이 더 바람직할 것으로 보인다.\nPaper Link\nVideo\n\n\n\n\nLearning Quadruped Locomotion Using Differentiable Simulation\n\n효율적인 역전파를 위해 단순한 surrogate dynamics model에서 얻은 smooth gradients와 정확한 forward simulation을 위해 non-differentiable 시뮬레이터의 high fidelity을 결합한 새로운 정책학습 방식 제안\nnon-differentiable인 IsaacGym 시뮬레이터는 복잡한 contact 동역학을 시뮬레이션할 수 있으며, 이를 활용하여 단순화된 rigid-body dynamics model의 상태(state)를 정렬(align)함으로써, training pipeline이 실제 동역학에 기반을 두도록 보장\nDifferential simulator를 따로 개발하는 움직임(brax)도 있는데 differential loss term을 추가해서 non-differential simulator를 보완하는 방식이 새로웠으며 PPO보다 적은 agent수로도 학습 reward가 빠르게 maximization된다는 점이 놀라웠음\nPaper Link"
  },
  {
    "objectID": "post/study/2025-03-01-review.html#height-sample-randomization",
    "href": "post/study/2025-03-01-review.html#height-sample-randomization",
    "title": "Learning robust perceptive locomotion for quadrupedal robots in the wild",
    "section": "Height sample randomization",
    "text": "Height sample randomization\n\nStudent Policy 방법론 더 자세히\n\n👉 핵심 아이디어\n- 실제 환경에서 발생할 수 있는 다양한 센서 오류 및 맵 품질 저하를 학습 시뮬레이션\n- Noise 적용 방식을 세분화하여 다양한 조건에서 정책이 강건하게 작동하도록 유도\n- 지형 특성 변화도 반영하여 현실적인 지형 적응 능력 강화\n\nNoise 모델 적용\n\n\nNoise 모델 \\(n(\\tilde{o}^e_t | o^e_t, z)\\), \\(z \\in \\mathbb{R}^{8 \\times 4}\\) 사용\n높이 샘플에 두 가지 유형의 측정 Noise 추가\n\nScan Point를 수평으로 이동(Shift)\n\n높이 값에 교란 추가(Height Disturbance)\n\n\nNoise 값은 가우시안 분포 에서 샘플링되며, Noise 매개변수 \\(z\\) 가 분산을 결정\n\nNoise 매개변수 벡터 \\(z\\) 는 커리큘럼 학습의 일부\n\n훈련 기간이 진행됨에 따라 Noise 강도를 선형적으로 증가\n\n\n\n\n\n\nNoise는 3가지 다른 범위 에서 적용\n\nPer-Scan-Point → 각 스캔 포인트마다 매 시간 단계마다 재샘플링\nPer-Foot → 각 다리(발)별로 매 시간 단계마다 재샘플링\nPer-Episode → 모든 스캔 포인트에 대해 일정한 Noise 유지\n\n\n매핑 오류 시뮬레이션\n\n맵 품질 변화 및 오류 원인 시뮬레이션\n3가지 매핑 조건 정의 (훈련 에피소드에서 60%, 30%, 10% 확률로 선택)\n\n정상적인 운영 상태 → 양호한 맵 품질, 표준적인 Noise 적용\n\n지도 오프셋 발생 → 자세 추정 드리프트 또는 변형 가능한 지형(예: 진흙, 눈)으로 인해 큰 오프셋 추가\n\n매핑 실패 시뮬레이션 → 폐색(occlusion) 또는 센서 오류로 인해 각 스캔 포인트에 큰 Noise 추가\n\n\n\n\n\n지형 변화 시뮬레이션\n\n훈련 지형을 셀(Cell) 단위로 나누고, 특정 셀의 height map에 추가 오프셋 적용\n서로 다른 지형 특성 간의 전환 시뮬레이션 (예: 식생, 깊은 눈 등)"
  },
  {
    "objectID": "post/study/2025-03-01-review.html#belief-state-encoder",
    "href": "post/study/2025-03-01-review.html#belief-state-encoder",
    "title": "Learning robust perceptive locomotion for quadrupedal robots in the wild",
    "section": "Belief state encoder",
    "text": "Belief state encoder\n\nStudent Policy 방법론 더 자세히\n\n👉 핵심 아이디어\n- 고유 감각(Proprioception)과 외부 감각(Exteroception) 정보를 효과적으로 통합\n    - 외부 감각의 신뢰도를 학습하여 동적으로 조절 \n    - 외부 감각이 부정확할 때는 고유 감각을 더 많이 활용하도록 학습\n- RNN(GRU) 기반으로 Belif state를 추정하여 POMDP 문제 해결 \n- 게이트 디코더를 활용하여 Belif state의 유효성을 보장\nGated Encoder\n\n게이트 RNN 모델과 Multi-Modal Information Fusion 기법에서 영감\n\nadaptive gating 메커니즘 사용 → 외부 감각 정보를 얼마나 반영할지 조절\n\n\n\n\nBelif state 계산 과정\n\n입력 데이터\n\n고유 감각: \\(o^p_t\\)\n\n노이즈가 포함된 외부 감각 특징: \\(l^e_t = g_e(\\tilde{o}^e_t)\\)\n\n이전 숨겨진 상태: \\(h_t\\)\n\nRNN을 통해 중간 Belif state \\(b^\\text{'}_t\\) 계산\n\\[\nb^\\text{'}_t, h_{t+1} = \\text{RNN}(o^p_t, l^e_t, h_t)\n\\]\n\nGRU(Gated Recurrent Unit) 기반 RNN 사용\n\n시간적 정보(Sequential Information) 유지\n\nattention 벡터 \\(\\alpha\\) 계산: 외부 감각 정보를 얼마나 반영할지 결정 \\[\n\\alpha = \\sigma(g_a(b^\\text{'}_t))\n\\]\n\n\\(g_a\\): 완전 연결 신경망(FCN)\n\\(\\sigma(\\cdot)\\): 시그모이드 활성화 함수\n\\(\\alpha\\) 값이 클수록 외부 감각 정보를 더 많이 반영\n\n최종 Belif state \\(b_t\\) 계산\n\\[\nb_t = g_b(b^\\text{'}_t) + l^e_t \\cdot \\alpha\n\\]\n\n\\(g_b\\): 완전 연결 신경망(FCN)\n외부 감각 정보 \\(l^e_t\\) 를 attention 벡터 \\(\\alpha\\) 에 의해 가중 조합\n\n\nGated Decoder\n\n같은 게이트 메커니즘을 디코더에서도 사용\nPrivileged Information 및 높이 샘플을 재구성\nReconstruction Loss 계산 → Belif state \\(b_t\\) 가 환경 정보를 올바르게 반영하도록 유도\n\n\n\n\nGRU(Gated Recurrent Unit) 사용 이유\n\nGRU는 LSTM보다 계산량이 적으면서도 장기 의존성(Long-term dependencies)을 효과적으로 유지\n게이트 구조를 통해 중요한 정보만 선택적으로 저장 및 업데이트 가능"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beyond Steps RL",
    "section": "",
    "text": "Beyond Steps RL symbolizes the ambition to go beyond conventional boundaries in reinforcement learning (RL). The name reflects the integration of cutting-edge RL techniques with robotic locomotion, especially focusing on quadrupedal robots. It emphasizes innovation, exploration, and the pursuit of advancements that push RL applications beyond mere movement—toward solving real-world challenges with precision and adaptability.\n\n\nStudy Records\n\n\n\nNo.\nPaper\nPresenter\nDate\nFile\n\n\n\n\n1\nNot Only Rewards but Also Constraints\nJihong Kim\n24.09.01\npaper_review\n\n\n2\nAgile But Safe: Learning Collision-Free High-Speed Legged Locomotion\nJungYeon Lee\n24.09.01\npaper_review\n\n\n3\nSpinning Up in Deep RL\nChanwoo Park\n24.09.22\npaper_review\n\n\n4\nNot Only Rewards but Also Constraints II\nJehee Lee\n24.10.06\npaper_review\n\n\n5\nConstrained Policy Optimization\nJinwon Kim\n24.10.06\npaper_review\n\n\n6\nIPO: Interior-point Policy Optimization under Constraints\nJungYeon Lee\n24.10.06\npaper_review\n\n\n7\nTRPO/PPO\nChanwoo Park\n24.10.20\npaper_review\n\n\n8\nConstrained Policy Optimization II\nJihong Kim\n24.10.20\npaper_review\n\n\n9\nLearning-based legged locomotion; state of the art and future perspectives\nJinwon Kim\n24.11.17\npaper_review\n\n\n10\npympc-quadruped\nJihong Kim\n25.01.05\ncode_review\n\n\n11\nModel Predictive Control\nJungYeon Lee\n25.01.05\npaper_review\n\n\n\n\n\nMembers\n\n\n\n\n\n\n\n\n\n\n\nJungYeon Lee\nJihong Kim\nJinwon Kim\nChanwoo Park"
  }
]