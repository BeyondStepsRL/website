[
  {
    "objectID": "goal.html",
    "href": "goal.html",
    "title": "Goal",
    "section": "",
    "text": "목표로 제출하고자 하는 학회나 저널 제출기간 확인\n\n\nCORL 2025\nhttps://www.corl.org/home\n\n\nICRA 2026\n\n\n\nIROS 2026"
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-structure.html",
    "href": "post/competition/icros2025/2025-05-06-icros2025-structure.html",
    "title": "[icros2025] 2. 프로젝트 개념도",
    "section": "",
    "text": "본 페이지는 2025년 6월 25일까지 진행되는 사족보행 로봇(Unitree Go1) 대회 준비 과정을 정리합니다. 시스템 아키텍처, 주요 다이어그램, 일정, 주별 작업 분배 등을 포함하여 팀원들이 한눈에 전체 진행 상황을 파악하도록 구성했습니다.\n\n\n\n\n\n\n\n\n\n\n\n역할\n인원\n주요 책임\n\n\n\n\n화면/뷰어 개발\n(이름)\nViewer UI, 버튼, 조이스틱 통합\n\n\n제어기 학습\n(이름)\n통합·Falldown 제어기 학습 (2개로 구성), 네트워크 export 및 코드 최적화\n\n\n하드웨어/네트워크\n(이름)\nORIN 장착, Router 구성(mesho 망), 전원/열 관리\n\n\n통합·PM\n(이름)\n전체 일정 관리, 시스템 통합, 테스트\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    subgraph REMOTE [**원격**]\n        Joy[Joystick]\n        Laptop[노트북&lt;br/&gt;Viewer + Buttons]\n        Router1((AX3000M&lt;br/&gt;Router 1))\n    end\n    subgraph FIELD [**현장**]\n        Router2((AX3000M&lt;br/&gt;Router 2))\n        Router3((AX3000M&lt;br/&gt;Router N))\n        ORIN[Jetson ORIN PC]\n        Go1[Unitree Go1]\n    end\n    subgraph CICD [**학습 프레임워크-IsaacLab**]\n        TrainServer(학습 프레임워크&lt;br/&gt;RL/딥러닝 서버)\n        IntNet(통합 네트워크)\n        FalNet(Falldown 네트워크)\n    end\n    Joy -- \"/joy\" --&gt; Laptop\n    Laptop -- ROS2 --&gt; ORIN\n    Go1 -- 센서 데이터 --&gt; ORIN\n    ORIN -- 제어값 --&gt; Go1\n    Go1 -- 모니터링 --&gt; Laptop\n    Laptop -- 제어기 변경 및 각종 버튼 --&gt; Go1\n    Laptop --Ethernet--&gt; Router1\n    Router1 &lt;--Wi‑Fi--&gt; Router2\n    Router2 &lt;--Wi‑Fi--&gt; ORIN\n    ORIN &lt;--Wi‑Fi--&gt; Router3\n    Router2 &lt;--Wi‑Fi--&gt; Router3\n    TrainServer -- 학습 --&gt; IntNet\n    TrainServer -- 학습 --&gt; FalNet\n    IntNet -- 배포 --&gt; ORIN\n    FalNet -- 배포 --&gt; ORIN\n\n    style REMOTE fill:#f9f9ff,stroke:#888,stroke-width:2px\n    style FIELD fill:#eaffea,stroke:#888,stroke-width:2px\n    style CICD fill:#fffbe6,stroke:#e6b800,stroke-width:2px\n\n\n\n\n\n\n\n\n\n\n\n\n\n구성요소\n설명\n\n\n\n\nJoystick\n원격 조이스틱 입력 장치. 사용자가 로봇을 직접 조종할 수 있음\n\n\n노트북 (Viewer)\n조이스틱 신호 수신, 실시간 모니터링, 제어 버튼 제공\n\n\nRouter\n현장/원격 네트워크 연결 및 무선 품질 보완을 위한 메쉬망 구축(여러 대 사용)\n\n\nJetson ORIN PC\n로봇 제어 연산, 센서 데이터 처리, ROS2 통신의 중심\n\n\nUnitree Go1\n실제 사족보행 로봇, 센서 데이터 송신 및 제어 명령 수신\n\n\n학습 프레임워크\nRL/딥러닝 등 제어기 학습 및 모델 생성 서버\n\n\n\n\n\n\n\n\n\n\n\nsequenceDiagram\n    participant Joy as Joystick\n    participant NB as 노트북\n    participant ORIN as ORIN PC\n    participant Go1 as Unitree Go1\n\n    Joy-&gt;&gt;NB: /joy 메시지\n    NB-&gt;&gt;ORIN: /joy 전송\n    Go1-&gt;&gt;ORIN: 센서 데이터\n    ORIN-&gt;&gt;ORIN: NN 추론\n    ORIN-&gt;&gt;Go1: 제어 명령\n    Go1--&gt;&gt;NB: 실시간 모니터(카메라 및 센서 데이터 등 /Read Only)\n    NB--&gt;&gt;Go1: 버튼 입력(제어기 변경 및 특정 기능 버튼 등 /Write Only)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclassDiagram\n    class JoyInputHandler {\n        +subscribeJoy()\n        +mapToInput()\n    }\n    class UnifiedController {\n        +forward()\n        +turn()\n        +updateState()\n    }\n    class FalldownController {\n        +detectFall()\n        +recover()\n    }\n    class NetworkInterface {\n        +runInference()\n        +sendCommands()\n    }\n    JoyInputHandler --&gt; NetworkInterface\n    NetworkInterface --&gt; UnifiedController\n    NetworkInterface --&gt; FalldownController\n\n\n\n\n\n\n위 표는 예시임. 실제 코드를 확인하여 클래스 다이어그램을 재구축할 예정.\n\n\n\n\n\n\n\n\ngantt\n    title 2025 사족보행 로봇 대회 준비 일정\n    dateFormat  YYYY-MM-DD\n    axisFormat  %m/%d\n    section 설계\n    시스템 아키텍처 확정 :done, arch, 2025-05-07,3d\n    section RL 제어기\n    통합 제어기 학습 :u_ctrl, 2025-05-10,14d\n    Falldown 제어기 학습 :f_ctrl, after u_ctrl,7d\n    section 하드웨어\n    ORIN 장착 및 세팅 :orin, 2025-05-10,10d\n    라우터 구성 :router, after orin,5d\n    section UI\n    Viewer UI 개발 :ui1, 2025-05-10,14d\n    조이스틱 통합 :joyint, after ui1,5d\n    section 테스트\n    1차 통합 테스트 :test1, 2025-06-05,5d\n    필드 테스트 :test2, 2025-06-12,5d\n    버퍼 & 폴리싱 :buffer, 2025-06-19,5d\n    최종 배포 :milestone, 2025-06-25,1d\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n주차\n기간\n화면/뷰어\n제어기 학습\n하드웨어\n통합·PM\n\n\n\n\n1\n5/7–5/11\nUI 와이어프레임 설계\nIsaacLab 환경 세팅\nORIN 장착 구조 설계\n전체 아키텍처 문서화\n\n\n2\n5/12–5/18\n초기 Viewer 구현\n통합 제어기 학습 시작\nORIN 케이블링 & 전원\nROS2 인터페이스 정의\n\n\n3\n5/19–5/25\n조이스틱 입력 연동\n학습 지속 & 로그 분석\n라우터 테스트\n통신 지연 측정\n\n\n4\n5/26–6/1\n센서 데이터 오버레이\nFalldown 학습 시작\nORIN‑Go1 통신 벤치마킹\nHIL 테스트\n\n\n5\n6/2–6/8\nUI 안정화 & 예외처리\n학습 완료 & 모델 추출\n라우터 메쉬 구성\n초기 통합 테스트\n\n\n6\n6/9–6/15\nUX 피드백 반영\n최적화\n로봇 하드웨어 구성\n시스템 리허설\n\n\n7\n6/16–6/22\nUI 기능 동결\n최적화\n예비 HW 패킹\n현장 리허설\n\n\n8\n6/23–6/25\n\n테스트 지원\n\n최종 체크 & 대회"
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-structure.html#개요",
    "href": "post/competition/icros2025/2025-05-06-icros2025-structure.html#개요",
    "title": "[icros2025] 2. 프로젝트 개념도",
    "section": "",
    "text": "본 페이지는 2025년 6월 25일까지 진행되는 사족보행 로봇(Unitree Go1) 대회 준비 과정을 정리합니다. 시스템 아키텍처, 주요 다이어그램, 일정, 주별 작업 분배 등을 포함하여 팀원들이 한눈에 전체 진행 상황을 파악하도록 구성했습니다."
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-structure.html#팀-구성",
    "href": "post/competition/icros2025/2025-05-06-icros2025-structure.html#팀-구성",
    "title": "[icros2025] 2. 프로젝트 개념도",
    "section": "",
    "text": "역할\n인원\n주요 책임\n\n\n\n\n화면/뷰어 개발\n(이름)\nViewer UI, 버튼, 조이스틱 통합\n\n\n제어기 학습\n(이름)\n통합·Falldown 제어기 학습 (2개로 구성), 네트워크 export 및 코드 최적화\n\n\n하드웨어/네트워크\n(이름)\nORIN 장착, Router 구성(mesho 망), 전원/열 관리\n\n\n통합·PM\n(이름)\n전체 일정 관리, 시스템 통합, 테스트"
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-structure.html#시스템-아키텍처",
    "href": "post/competition/icros2025/2025-05-06-icros2025-structure.html#시스템-아키텍처",
    "title": "[icros2025] 2. 프로젝트 개념도",
    "section": "",
    "text": "flowchart TD\n    subgraph REMOTE [**원격**]\n        Joy[Joystick]\n        Laptop[노트북&lt;br/&gt;Viewer + Buttons]\n        Router1((AX3000M&lt;br/&gt;Router 1))\n    end\n    subgraph FIELD [**현장**]\n        Router2((AX3000M&lt;br/&gt;Router 2))\n        Router3((AX3000M&lt;br/&gt;Router N))\n        ORIN[Jetson ORIN PC]\n        Go1[Unitree Go1]\n    end\n    subgraph CICD [**학습 프레임워크-IsaacLab**]\n        TrainServer(학습 프레임워크&lt;br/&gt;RL/딥러닝 서버)\n        IntNet(통합 네트워크)\n        FalNet(Falldown 네트워크)\n    end\n    Joy -- \"/joy\" --&gt; Laptop\n    Laptop -- ROS2 --&gt; ORIN\n    Go1 -- 센서 데이터 --&gt; ORIN\n    ORIN -- 제어값 --&gt; Go1\n    Go1 -- 모니터링 --&gt; Laptop\n    Laptop -- 제어기 변경 및 각종 버튼 --&gt; Go1\n    Laptop --Ethernet--&gt; Router1\n    Router1 &lt;--Wi‑Fi--&gt; Router2\n    Router2 &lt;--Wi‑Fi--&gt; ORIN\n    ORIN &lt;--Wi‑Fi--&gt; Router3\n    Router2 &lt;--Wi‑Fi--&gt; Router3\n    TrainServer -- 학습 --&gt; IntNet\n    TrainServer -- 학습 --&gt; FalNet\n    IntNet -- 배포 --&gt; ORIN\n    FalNet -- 배포 --&gt; ORIN\n\n    style REMOTE fill:#f9f9ff,stroke:#888,stroke-width:2px\n    style FIELD fill:#eaffea,stroke:#888,stroke-width:2px\n    style CICD fill:#fffbe6,stroke:#e6b800,stroke-width:2px\n\n\n\n\n\n\n\n\n\n\n\n\n\n구성요소\n설명\n\n\n\n\nJoystick\n원격 조이스틱 입력 장치. 사용자가 로봇을 직접 조종할 수 있음\n\n\n노트북 (Viewer)\n조이스틱 신호 수신, 실시간 모니터링, 제어 버튼 제공\n\n\nRouter\n현장/원격 네트워크 연결 및 무선 품질 보완을 위한 메쉬망 구축(여러 대 사용)\n\n\nJetson ORIN PC\n로봇 제어 연산, 센서 데이터 처리, ROS2 통신의 중심\n\n\nUnitree Go1\n실제 사족보행 로봇, 센서 데이터 송신 및 제어 명령 수신\n\n\n학습 프레임워크\nRL/딥러닝 등 제어기 학습 및 모델 생성 서버"
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-structure.html#시퀀스-다이어그램",
    "href": "post/competition/icros2025/2025-05-06-icros2025-structure.html#시퀀스-다이어그램",
    "title": "[icros2025] 2. 프로젝트 개념도",
    "section": "",
    "text": "sequenceDiagram\n    participant Joy as Joystick\n    participant NB as 노트북\n    participant ORIN as ORIN PC\n    participant Go1 as Unitree Go1\n\n    Joy-&gt;&gt;NB: /joy 메시지\n    NB-&gt;&gt;ORIN: /joy 전송\n    Go1-&gt;&gt;ORIN: 센서 데이터\n    ORIN-&gt;&gt;ORIN: NN 추론\n    ORIN-&gt;&gt;Go1: 제어 명령\n    Go1--&gt;&gt;NB: 실시간 모니터(카메라 및 센서 데이터 등 /Read Only)\n    NB--&gt;&gt;Go1: 버튼 입력(제어기 변경 및 특정 기능 버튼 등 /Write Only)"
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-structure.html#클래스-다이어그램-ros2-노드-기준",
    "href": "post/competition/icros2025/2025-05-06-icros2025-structure.html#클래스-다이어그램-ros2-노드-기준",
    "title": "[icros2025] 2. 프로젝트 개념도",
    "section": "",
    "text": "classDiagram\n    class JoyInputHandler {\n        +subscribeJoy()\n        +mapToInput()\n    }\n    class UnifiedController {\n        +forward()\n        +turn()\n        +updateState()\n    }\n    class FalldownController {\n        +detectFall()\n        +recover()\n    }\n    class NetworkInterface {\n        +runInference()\n        +sendCommands()\n    }\n    JoyInputHandler --&gt; NetworkInterface\n    NetworkInterface --&gt; UnifiedController\n    NetworkInterface --&gt; FalldownController\n\n\n\n\n\n\n위 표는 예시임. 실제 코드를 확인하여 클래스 다이어그램을 재구축할 예정."
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-structure.html#일정-gantt",
    "href": "post/competition/icros2025/2025-05-06-icros2025-structure.html#일정-gantt",
    "title": "[icros2025] 2. 프로젝트 개념도",
    "section": "",
    "text": "gantt\n    title 2025 사족보행 로봇 대회 준비 일정\n    dateFormat  YYYY-MM-DD\n    axisFormat  %m/%d\n    section 설계\n    시스템 아키텍처 확정 :done, arch, 2025-05-07,3d\n    section RL 제어기\n    통합 제어기 학습 :u_ctrl, 2025-05-10,14d\n    Falldown 제어기 학습 :f_ctrl, after u_ctrl,7d\n    section 하드웨어\n    ORIN 장착 및 세팅 :orin, 2025-05-10,10d\n    라우터 구성 :router, after orin,5d\n    section UI\n    Viewer UI 개발 :ui1, 2025-05-10,14d\n    조이스틱 통합 :joyint, after ui1,5d\n    section 테스트\n    1차 통합 테스트 :test1, 2025-06-05,5d\n    필드 테스트 :test2, 2025-06-12,5d\n    버퍼 & 폴리싱 :buffer, 2025-06-19,5d\n    최종 배포 :milestone, 2025-06-25,1d"
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-structure.html#주별-칸반-보드-책임",
    "href": "post/competition/icros2025/2025-05-06-icros2025-structure.html#주별-칸반-보드-책임",
    "title": "[icros2025] 2. 프로젝트 개념도",
    "section": "",
    "text": "주차\n기간\n화면/뷰어\n제어기 학습\n하드웨어\n통합·PM\n\n\n\n\n1\n5/7–5/11\nUI 와이어프레임 설계\nIsaacLab 환경 세팅\nORIN 장착 구조 설계\n전체 아키텍처 문서화\n\n\n2\n5/12–5/18\n초기 Viewer 구현\n통합 제어기 학습 시작\nORIN 케이블링 & 전원\nROS2 인터페이스 정의\n\n\n3\n5/19–5/25\n조이스틱 입력 연동\n학습 지속 & 로그 분석\n라우터 테스트\n통신 지연 측정\n\n\n4\n5/26–6/1\n센서 데이터 오버레이\nFalldown 학습 시작\nORIN‑Go1 통신 벤치마킹\nHIL 테스트\n\n\n5\n6/2–6/8\nUI 안정화 & 예외처리\n학습 완료 & 모델 추출\n라우터 메쉬 구성\n초기 통합 테스트\n\n\n6\n6/9–6/15\nUX 피드백 반영\n최적화\n로봇 하드웨어 구성\n시스템 리허설\n\n\n7\n6/16–6/22\nUI 기능 동결\n최적화\n예비 HW 패킹\n현장 리허설\n\n\n8\n6/23–6/25\n\n테스트 지원\n\n최종 체크 & 대회"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-example.html",
    "href": "post/study/2025-04-19-IsaacLab-example.html",
    "title": "[IsaacLab] 02. 개발 세팅 및 예제 실행",
    "section": "",
    "text": "이 문서에서는 vscode에서 개발을 진행하기 위한 환경 설정 방법과, 강화학습 예제를 실행하는 방법을 설명합니다."
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-example.html#사전-준비사항",
    "href": "post/study/2025-04-19-IsaacLab-example.html#사전-준비사항",
    "title": "[IsaacLab] 02. 개발 세팅 및 예제 실행",
    "section": "사전 준비사항",
    "text": "사전 준비사항\n\nDocker Container"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-example.html#vscode-환경-설정",
    "href": "post/study/2025-04-19-IsaacLab-example.html#vscode-환경-설정",
    "title": "[IsaacLab] 02. 개발 세팅 및 예제 실행",
    "section": "vscode 환경 설정",
    "text": "vscode 환경 설정\n\nContainer 내부 환경으로 접속하기\n\n\n\nvscode image\n\n\n이때, Root directory가 /workspace/isaaclab이 되어야합니다. (.vscode의 task를 수행해야하기 때문)\nTask 실행 ​  ​ ctrl+shift+P 단축키를 통하여 명령어 창을 켠 후, Tasks: Run Task를 수행합니다. ​  ​ setup_python_env를 실행하면, 파이썬 라이브러리에 대한 환경변수들을 자동으로 설정해줍니다. 이를 통해 개발의 편의성을 높일 수 있습니다.\n\n\n\n\nimage-20250419170907261\n\n\n예를 들어 위 이미지와 같이 AppLauncher class 등에 대해서 바로 접근이 가능하여, 어떠한 역할을 하는 것인지, 또한 그 내부로 바로 접근이 가능하다는 등의 장점등이 있습니다."
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-example.html#isaaclab-예제-실행",
    "href": "post/study/2025-04-19-IsaacLab-example.html#isaaclab-예제-실행",
    "title": "[IsaacLab] 02. 개발 세팅 및 예제 실행",
    "section": "IsaacLab 예제 실행",
    "text": "IsaacLab 예제 실행\n\nGo1 로봇 학습 예제\n학습 시:\npython scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Velocity-Rough-Unitree-Go1-v0 --video\n학습 중인, 내용들은 logs/{알고리즘}/{env}/{timestamp}/ 에 저장됩니다. 이때 video 인자등을 추가하면 중간중간 학습 진행사항을 확인하기 좋습니다.\n\n실행 시:\npython scripts/reinforcement_learning/rsl_rl/play.py --task=Isaac-Velocity-Rough-Unitree-Go1-Play-v0 --num_envs 20\n\n\n학습 중, webrtc로 붙어서 확인을 하는 것은 권장하지 않습니다. 한번 gui를 연결하고 나면, 학습 속도가 매우 느려질 뿐 아니라, 접속을 끊었을 때 학습에 오류가 생기며 server 프로그램이 종료됩니다.\n\n\n기본적인 인자 설명들\n\n–video: 훈련 중 비디오를 녹화할지 여부를 결정합니다. 이 인자를 사용하면 비디오 녹화가 활성화됩니다.\n–video_length: 녹화할 비디오의 길이를 스텝 단위로 지정합니다. 기본값은 200입니다.\n–video_interval: 비디오 녹화 간의 간격을 스텝 단위로 지정합니다. 기본값은 2000입니다.\n–num_envs: 시뮬레이션할 환경의 수를 지정합니다.\n–task: 수행할 작업의 이름을 지정합니다.\n–seed: 환경에 사용할 시드 값을 지정합니다.\n–max_iterations: 강화 학습 정책 훈련의 최대 반복 횟수를 지정합니다.\n\n\n. 사용 가능한 환경 리스트\n아래 명령어를 통하여, 현재 사용가능한 환경들에 대해서 확인할 수 있습니다. 좀더 친절한 가이드는 여기에 있습니다.\n./isaaclab.sh -p scripts/environments/list_envs.py"
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html",
    "href": "post/study/2025-02-17-mpc.html",
    "title": "Linear MPC",
    "section": "",
    "text": "This post is sourced from pympc-quadruped and translated into Korean."
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#dynamic-constraints",
    "href": "post/study/2025-02-17-mpc.html#dynamic-constraints",
    "title": "Linear MPC",
    "section": "1. Dynamic Constraints",
    "text": "1. Dynamic Constraints\n\na. Approximated Angular Velocity Dynamics\nThe robot’s orientation is expressed as a vector of Z-Y-X Euler angles \\(\\Theta = [\\phi, \\theta, \\psi]^\\text{T}\\), where\n\n\\(\\psi\\) is the yaw,\n\\(\\theta\\) is the pitch, and\n\\(\\phi\\) is the roll.\n\nThese angles correspond to a sequence of rotations such that the transform from body to world coordinates can be expressed as \\[\n\\mathbf{R} = \\mathbf{R}_z(\\psi)\\mathbf{R}_y(\\theta)\\mathbf{R}_x(\\phi)\n\\]\nwhere \\(\\mathbf{R}_n(\\alpha)\\) represents a positive rotation of \\(\\alpha\\) about the \\(n\\)-axis. In details, we can write\n\\[\n\\mathbf{R}_z(\\psi) = \\begin{bmatrix}\n    \\cos\\psi & -\\sin \\psi & 0 \\\\\n    \\sin\\psi & \\cos \\psi  & 0 \\\\\n    0        & 0          & 1\n\\end{bmatrix},\n\\mathbf{R}_y(\\theta) = \\begin{bmatrix}\n    \\cos\\theta  & 0 & \\sin\\theta \\\\\n    0           & 1 & 0          \\\\\n    -\\sin\\theta & 0 & \\cos\\theta\n\\end{bmatrix},\n\\mathbf{R}_z(\\psi) = \\begin{bmatrix}\n    \\cos\\psi & -\\sin \\psi & 0 \\\\\n    \\sin\\psi & \\cos \\psi  & 0 \\\\\n    0        & 0          & 1\n\\end{bmatrix}\n\\]\nFrom [2], we have known that \\(\\dot{\\mathbf{R}} = [\\mathbf{\\omega}]\\mathbf{R}\\), where \\(\\mathbf{\\omega} \\in \\mathbb{R}^3\\) is the robot’s angular velocity, \\([\\mathbf{\\omega}] \\in \\mathbb{R}^{3\\times3}\\) is defined as the skew-symmetric matrix with respect to \\(\\mathbf{\\omega}\\), and \\(\\mathbf{R}\\) is the rotation matrix which transforms from body to world coordinates. Then the angular velocity in world coordinates can be found with\n\\[\n\\begin{aligned}\n[\\mathbf{\\omega}] &= \\dot{\\mathbf{R}}\\mathbf{R}^{-1} = \\dot{\\mathbf{R}}\\mathbf{R}^\\text{T} \\\\\n&= \\left( \\frac{\\partial R}{\\partial \\psi} + \\frac{\\partial R}{\\partial \\theta} + \\frac{\\partial R}{\\partial \\phi} \\right)\\mathbf{R}^\\text{T} \\\\\n&= \\begin{bmatrix}\n    0 & \\dot{\\phi} \\sin\\theta - \\dot{\\psi} & \\dot{\\theta}\\cos\\psi + \\dot{\\phi}\\cos\\theta\\sin\\psi \\\\\n    -\\dot{\\phi} \\sin\\theta + \\dot{\\psi} & 0 & \\dot{\\theta}\\sin\\psi - \\dot{\\phi}\\cos\\psi\\cos\\theta \\\\\n    -\\dot{\\theta}\\cos\\psi - \\dot{\\phi}\\cos\\theta\\sin\\psi & -\\dot{\\theta}\\sin\\psi + \\dot{\\phi}\\cos\\psi\\cos\\theta & 0\n\\end{bmatrix}\n\\end{aligned}\n\\]\nThe above result is easy to get using the MATLAB script.\nsyms psi theta phi real\nsyms psidot thetadot phidot real\n\nRz = [cos(psi), -sin(psi), 0;\n      sin(psi), cos(psi),  0;\n      0,        0,         1];\n\nRy = [cos(theta),  0, sin(theta);\n      0,           1, 0;\n      -sin(theta), 0, cos(theta)];\n  \nRx = [1, 0,        0;\n      0, cos(phi), -sin(phi);\n      0, sin(phi), cos(phi)];\n  \nR = simplify(Rz*Ry*Rx);\n\nSomega = simplify((diff(R,phi)*phidot + diff(R,theta)*thetadot + diff(R,psi)*psidot) * R')\nNow we are ready to build connections between the angular velocity in world coordinates \\(\\mathbf{\\omega}\\) and the rate of change of Euler angles \\(\\dot{\\mathbf{\\Theta}} = [\\dot{\\phi}, \\dot{\\theta}, \\dot{\\psi}]^\\text{T}\\).\n\\[\n\\mathbf{\\omega} = \\begin{bmatrix}\n    -\\dot{\\theta}\\sin\\psi + \\dot{\\phi}\\cos\\psi\\cos\\theta \\\\\n    \\dot{\\theta}\\cos\\psi + \\dot{\\phi}\\cos\\theta\\sin\\psi \\\\\n    -\\dot{\\phi} \\sin\\theta + \\dot{\\psi}\n\\end{bmatrix}\n= \\begin{bmatrix}\n    \\cos\\theta\\cos\\psi & -\\sin\\psi & 0 \\\\\n    \\cos\\theta\\sin\\psi & \\cos\\psi  & 0 \\\\\n    -\\sin\\theta        & 0         & 1\n\\end{bmatrix}\\begin{bmatrix}\n    \\dot{\\phi} \\\\ \\dot{\\theta} \\\\ \\dot{\\psi}\n\\end{bmatrix} = \\mathbf{E}\\dot{\\mathbf{\\Theta}}\n\\]\nIf the robot is not pointed vertically, which means \\(\\cos\\theta \\neq 0\\), the matrix \\(\\mathbf{E}\\) is invertable. In such case, we can get\n\\[\n\\dot{\\mathbf{\\Theta}} = \\mathbf{E}^{-1}\\mathbf{\\omega} = \\begin{bmatrix}\n    \\frac{\\cos\\psi}{\\cos\\theta} & \\frac{\\sin\\psi}{\\cos\\theta} & 0 \\\\\n    -\\sin\\psi                   & \\cos\\psi                    & 0 \\\\\n    \\cos\\psi\\tan\\theta          & \\sin\\psi\\tan\\theta          & 1\n\\end{bmatrix}\\mathbf{\\omega}\n\\]\nFor small values of roll \\(\\phi\\) and pitch \\(\\theta\\), the above equation can be approximated as\n\\[\n\\dot{\\mathbf{\\Theta}} \\approx \\begin{bmatrix}\n    \\cos\\psi  & \\sin\\psi & 0 \\\\\n    -\\sin\\psi & \\cos\\psi & 0 \\\\\n    0         & 0        & 1\n\\end{bmatrix} \\mathbf{\\omega}\n\\]\nwhich is equivalent to\n\\[\n\\dot{\\mathbf{\\Theta}} \\approx \\mathbf{R}_z^\\text{T} \\mathbf{\\omega}\n\\tag{1}\n\\]\nNote that the order in which the Euler angle rotations are defined is important; with an alternate sequence of rotations, the approximation will be inaccurate for reasonable robot orientations.\n\n\nb. Simplified Single Rigid Body Model\nThe predictive controller models the robot as a single rigid body subject to forces at the contact patches. Although ignoring leg dynamics is a major simplification, the controller is still able to stabilize a high-DoF system and is robust to these multi-body effects.\nFor the Cheetah 3 robot, this simplification is reasonable: the mass of the legs is roughly 10% of the robot's total mass\nFor each ground reaction force \\(\\mathbf{f}_i \\in \\mathbb{R}^3\\), the vector from the CoM to the point where the force is applied is \\(\\mathbf{r}_i \\in \\mathbb{R}^3\\). The rigid body dynamics in world coordinates are given by\n\\[\n\\begin{aligned}\n\\ddot{\\mathbf{p}} &= \\frac{\\sum_{i=1}^n\\mathbf{f}_i}{m} - \\mathbf{g} \\\\\n\\frac{\\text{d}}{\\text{d}t}(\\mathbf{I\\omega}) &= \\sum_{i=1}^n \\mathbf{r}_i \\times \\mathbf{f}_i\\\\\n\\dot{\\mathbf{R}} &= [\\mathbf{\\omega}]\\mathbf{R}\n\\end{aligned} \\tag{2}\n\\]\nwhere \\(\\mathbf{p} \\in \\mathbb{R}^3\\) is the robot’s position in world frame, \\(m \\in \\mathbb{R}\\) is the robot’s mass, \\(\\mathbf{g} \\in \\mathbb{R}^3\\) is the acceleration of gravity, and \\(\\mathbf{I} \\in \\mathbb{R}^3\\) is the robot’s inertia tensor.\n\n\n변수 정리\n\n\n\\(\\mathbf{f}\\): 지면 반발력\n\\(\\mathbf{r}_i\\): 질량 중심(CoM)에서 힘이 작용하는 지점까지의 벡터\n\\(\\mathbf{p}\\): world frame에서 robot position\n\n\n\nThe nonlinear dynamics in the second and third equation of (2) motivate the approximations to avoid the nonconvex optimization that would otherwise be required for model predictive control.\n(2)의 두 번째와 세 번째 방정식에서 나타나는 비선형 동역학은, 모델 예측 제어에서 요구되는 nonconvex 최적화를 피하기 위해 근사화를 도입해야 할 필요성을 제시합니다.\n\n\n2번째 식\nThe second equation in (2) can be approximated with:\n\\[\n\\frac{\\text{d}}{\\text{d}t}(\\mathbf{I\\omega}) = \\mathbf{I\\dot{\\omega}} + \\omega \\times (\\mathbf{I\\omega}) \\approx \\mathbf{I\\dot{\\omega}} = \\sum_{i=1}^n \\mathbf{r}_i \\times \\mathbf{f}_i \\tag{3}\n\\]\nThis approximation has been made in other people’s work. The \\(\\omega \\times (\\mathbf{I\\omega})\\) term is small for bodies with small angular velocities and does not contribute significantly to the dynamics of the robot. The inertia tensor in the world coordinate system can be found with\n이 근사는 다른 연구에서도 사용되었습니다. \\(\\omega \\times (\\mathbf{I\\omega})\\) 항은 작은 각속도를 가지는 물체에서는 작으며, 로봇의 동역학에 크게 영향을 미치지 않습니다. 월드 좌표계에서의 inertia tensor는 다음과 같이 구할 수 있습니다.\n\\[\n\\mathbf{I} = \\mathbf{R}\\mathbf{I}_{\\mathcal{B}}\\mathbf{R}^\\text{T}\n\\]\nwhere \\(\\mathbf{I}_\\mathcal{B}\\) is the inertia tensor in body coordinates. For small roll and pitch angles, This can be approximated by\n\\[\n\\mathbf{\\hat{I}} = \\mathbf{R}_z(\\psi)\\mathbf{I}_{\\mathcal{B}}\\mathbf{R}_z(\\psi)^\\text{T}\n\\tag{4}\n\\]\nwhere \\(\\mathbf{\\hat{I}}\\) is the approximated robot’s inertia tensor in world frame. Combining equations (3)(4), we get\n\\[\n\\mathbf{\\dot{\\omega}} = \\mathbf{\\hat{I}}^{-1}\\sum_{i=1}^n \\mathbf{r}_i \\times \\mathbf{f}_i = \\mathbf{\\hat{I}}^{-1}\\sum_{i=1}^n [\\mathbf{r}_i] \\mathbf{f}_i\n\\tag{5}\n\\]\n\n\n3번째 식\nFor the third equation of (2), we have made the approximation in section 1.a, which gives us \\[\n\\dot{\\mathbf{\\Theta}} \\approx \\mathbf{R}_z^\\text{T} \\mathbf{\\omega}\n\\]\n\n\nc. Continuous-Time State Space Model\nFrom the discussion above, we can write the simplified single rigid body model using equations (1)(2)\n\\[\n\\begin{aligned}\n\\dot{\\mathbf{\\Theta}} &= \\mathbf{R}_z^\\text{T} \\mathbf{\\omega} \\\\\n\\dot{\\mathbf{p}} &= \\dot{\\mathbf{p}} \\\\\n\\mathbf{\\dot{\\omega}} &= \\mathbf{\\hat{I}}^{-1}\\sum_{i=1}^n [\\mathbf{r}_i] \\mathbf{f}_i \\\\\n\\ddot{\\mathbf{p}} &= \\frac{\\sum_{i=1}^n\\mathbf{f}_i}{m} - \\mathbf{g}\n\\end{aligned}\n\\tag{6}\n\\]\nIn matrix form:\n\\[\n\\begin{bmatrix}\n    \\mathbf{\\dot{\\Theta}} \\\\ \\mathbf{\\dot{p}} \\\\ \\mathbf{\\dot{\\omega}} \\\\ \\mathbf{\\ddot{p}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{R}_z^\\text{T} & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{I}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{\\Theta} \\\\ \\mathbf{p} \\\\ \\mathbf{\\omega} \\\\ \\mathbf{\\dot{p}}\n\\end{bmatrix} +\n\\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1]\\\\\n    \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{f}_1 \\\\ \\mathbf{f}_2 \\\\ \\mathbf{f}_3 \\\\ \\mathbf{f}_4\n\\end{bmatrix} +\n\\begin{bmatrix}\n    \\mathbf{0}_{31} \\\\ \\mathbf{0}_{31} \\\\ \\mathbf{0}_{31} \\\\ -\\mathbf{g}\n\\end{bmatrix}\n\\]\nThis equation can be rewritten with an additional gravity state \\(g\\) (note that here \\(g\\) is a scalar) to put the dynamics into the convenient state-space form: \\[\n\\dot{\\mathbf{x}}(t) = \\mathbf{A_c}(\\psi)\\mathbf{x}(t) + \\mathbf{B_c}(\\mathbf{r}_1, \\cdots, \\mathbf{r}_4, \\psi)\\mathbf{u}(t)\n\\tag{7}\n\\]\nwhere \\(\\mathbf{A_c} \\in \\mathbb{R}^{13\\times13}\\) and \\(\\mathbf{B_c} \\in \\mathbb{R}^{13\\times12}\\).\nIn details, we have \\[\n\\begin{bmatrix}\n    \\mathbf{\\dot{\\Theta}} \\\\ \\mathbf{\\dot{p}} \\\\ \\mathbf{\\dot{\\omega}} \\\\ \\mathbf{\\ddot{p}} \\\\ -\\dot{g}\n\\end{bmatrix} = \\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{R}_z^\\text{T} & \\mathbf{0}_3 & 0\\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{I}_3 & 0\\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3 & 0\\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3 & \\mathbf{e}_z\\\\\n    0            & 0            & 0                     & 0            & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{\\Theta} \\\\ \\mathbf{p} \\\\ \\mathbf{\\omega} \\\\ \\mathbf{\\dot{p}} \\\\ -g\n\\end{bmatrix} +\n\\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1]\\\\\n    \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} \\\\\n    0 & 0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{f}_1 \\\\ \\mathbf{f}_2 \\\\ \\mathbf{f}_3 \\\\ \\mathbf{f}_4\n\\end{bmatrix}\n\\tag{8}\n\\]\nThis form depends only on yaw and footstep locations. If these can be computed ahead of time, the dynamics become linear time-varying, which is suitable for convex model predictive control.\n이 형태는 오직 yaw와 발걸음 위치에만 의존합니다. 만약 이것들이 미리 계산될 수 있다면, 동역학은 선형 시간 변화 형태가 되어 convex model predictive control에 적합해집니다.\n\n\nd. Discretization\nSee https://en.wikipedia.org/wiki/Discretization for more details about the purposed method.\n\\[\n\\exp\\left(\\begin{bmatrix}\n          \\mathbf{A_c} & \\mathbf{B_c} \\\\\n          \\mathbf{0}   & \\mathbf{0}\n          \\end{bmatrix} \\text{dt}\\right) =\n          \\begin{bmatrix}\n          \\mathbf{A_d} & \\mathbf{B_d} \\\\\n          \\mathbf{0}   & \\mathbf{I}\n          \\end{bmatrix} \\tag{9}\n\\]\nThis allows us to express the dynamics in the discrete time form\n\\[\n\\mathbf{x}[k+1] = \\mathbf{A_d} \\mathbf{x}[k] + \\mathbf{B_d}[k]\\mathbf{u}[k]\n\\tag{10}\n\\]\nThe above approximation is only accurate if the robot is able to follow the reference trajectory. Large deviations from the reference trajectory, possibly caused by external or terrain disturbances, will result in \\(\\mathbf{B_d}[k]\\) being inaccurate. However, for the first time step, \\(\\mathbf{B_d}[k]\\) is calculated from the current robot state, and will always be correct. If, at any point, the robot is disturbed from following the reference trajectory, the next iteration of the MPC, which happens at most 40 ms after the disturbance, will recompute the reference trajectory based on the disturbed robot state, allowing it compensate for a disturbance.\n위 근사는 로봇이 reference trajectory을 따라갈 수 있을 때만 정확합니다. 외부 요인이나 지형의 방해로 인해 reference trajectory에서 크게 벗어나는 경우, \\(\\mathbf{B_d}[k]\\)가 부정확해질 수 있습니다.\n그러나 first timestep에서는 \\(\\mathbf{B_d}[k]\\)가 현재 로봇 상태에서 계산되므로 항상 정확합니다. 어떤 시점에서든 로봇이 참조 궤적에서 벗어나 방해를 받는다면, 방해가 발생한 후 최대 40ms 이내에 실행되는 MPC의 다음 반복에서, 방해를 받은 로봇 상태를 기반으로 reference trajectory을 재계산하여 방해를 보정할 수 있습니다."
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#force-constraints",
    "href": "post/study/2025-02-17-mpc.html#force-constraints",
    "title": "Linear MPC",
    "section": "2. Force Constraints",
    "text": "2. Force Constraints\n\na. Equality Constraints(등식조건)\nThe equality constraint\n\\[\n\\mathbf{D}_k \\mathbf{u}_k = \\mathbf{0} \\tag{11}\n\\]\nis used to set all forces from feet off the ground to zero, enforcing the desired gait, where \\(\\mathbf{D}_k\\) is a matrix which selects forces corresponding with feet not in contact with the ground at timestep \\(k\\).\n지면에서 떨어진 발들에 작용하는 모든 힘을 0으로 설정하여 원하는 gait을 강제하기 위해 사용됩니다. 여기서 \\(\\mathbf{D}_k\\)는 시간 단계 \\(k\\)에서 지면에 접촉하지 않은 발들에 해당하는 힘을 선택하는 행렬입니다.\n\n\nb. Inequality Constraints(부등식조건)\nThe inequality constraints limit the minimum and maximum \\(z\\)-force as well as a square pyramid approximation of the friction cone.\n부등식 제약 조건은 최소 및 최대 \\(z\\)-힘을 제한하며, friction cone의 사각뿔 근사를 포함합니다.\nFor each foot, we have the following 10 inequality constraints (\\(i = 1,2,3,4\\)).\n\\[\n\\begin{aligned}\nf_{\\min} \\leq &f_{i,z} \\leq f_{\\max} \\\\\n-\\mu f_{i,z} \\leq  &f_{i,x} \\leq \\mu f_{i,z} \\\\\n-\\mu f_{i,z} \\leq  &f_{i,y} \\leq \\mu f_{i,z}\n\\end{aligned}\n\\]\nWe want to write these constraints in matrix form. Thus we need to look these equations in detail.\nFor example, the constraints \\(-\\mu f_{i,z} \\leq \\pm f_{i,x} \\leq \\mu f_{i,z}\\) actually are \\[\n\\begin{aligned}\n-\\mu f_{i,z} &\\leq f_{i,x} \\\\\n-\\mu f_{i,z} &\\leq -f_{i,x} \\\\\nf_{i,x} &\\leq \\mu f_{i,z} \\\\\n-f_{i,x} &\\leq \\mu f_{i,z}\n\\end{aligned}\n\\]\nWe can rewrite these equations as\n\\[\n\\begin{aligned}\nf_{i,x} + \\mu f_{i,z} &\\geq 0 \\\\\n-f_{i,x} + \\mu f_{i,z} &\\geq 0 \\\\\nf_{i,x} - \\mu f_{i,z} &\\leq 0 \\\\\n-f_{i,x} - \\mu f_{i,z} &\\leq 0\n\\end{aligned}\n\\]\nWe can see that the first two equations and the last two equations are the same. Thus we can use the first two equations to replace these four equations. We can also note that \\(f_{\\min} = 0\\) as always. With the observation discussed above, we can write the force constraints for one foot on the ground as\n\\[\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n\\leq\n\\begin{bmatrix}\n1 & 0 & \\mu \\\\\n-1 & 0 & \\mu \\\\\n0 & 1 & \\mu \\\\\n0 & -1 & \\mu \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{i,x} \\\\ f_{i,y} \\\\ f_{i,z}\n\\end{bmatrix}\n\\leq\n\\begin{bmatrix}\n\\infty \\\\ \\infty \\\\ \\infty \\\\ \\infty \\\\ f_{\\max}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#reference-trajectory-generation",
    "href": "post/study/2025-02-17-mpc.html#reference-trajectory-generation",
    "title": "Linear MPC",
    "section": "3. Reference Trajectory Generation",
    "text": "3. Reference Trajectory Generation\nThe desired robot behavior is used to construct the reference trajectory. In the application, our reference trajectories are simple and only contain non-zero \\(xy\\)-velocity, \\(xy\\)-position, \\(z\\)-postion, yaw, and yaw rate. All parameters are commanded directly by the robot operator except for yaw and \\(xy\\)-position, which are determined by integrating the appropriate velocities. The other states (roll, pitch, roll rate, pitch rate and \\(z\\)-velocity) are always set to 0. The reference trajectory is also used to determine the dynamics constraints and future foot placement locations.\nIn practice, the reference trajectory is short (between 0.5 and 0.3 seconds) and recalculated often (every 0.05 to 0.03 seconds) to ensure the simplified dynamics remain accurate if the robot is disturbed."
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#qp-formulation",
    "href": "post/study/2025-02-17-mpc.html#qp-formulation",
    "title": "Linear MPC",
    "section": "4. QP Formulation",
    "text": "4. QP Formulation\n\na. Batch Formulation for Dynamic Constraints\nKey idea: For the state space model, express \\(x_0, x_1, \\cdots, x_k\\) as function of \\(u_0\\).\n\\[\n\\begin{aligned}\nx_1 &= Ax_0 + Bu_0 \\\\\nx_2 &= Ax_1 + Bu_1 = A^2x_0 + ABu_0 + Bu_1 \\\\\n&\\vdots \\\\\nx_k & = A^kx_0 + A^{k-1}Bu_0 + A^{k-2}Bu_1 + \\cdots + Bu_{k-1}\n\\end{aligned}\n\\]\nWe can write these equations in matrix form\n\\[\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k\n\\end{bmatrix} =\n\\begin{bmatrix}\nA \\\\ A^2 \\\\ \\vdots \\\\A^k\n\\end{bmatrix}\nx_0\n+\n\\begin{bmatrix}\nB & 0 & \\cdots & 0 \\\\\nAB & B & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA^{k-1}B & A^{k-2}B & \\cdots & B\n\\end{bmatrix}\n\\begin{bmatrix}\nu_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{k-1}\n\\end{bmatrix}\n\\]\nwhere \\(k\\) is the horizon length. Let \\(x_t\\) denotes the system state at time step \\(t\\), i.e. at current state, then we can write\n\\[\n\\begin{bmatrix}\nx_{t+1} \\\\ x_{t+2} \\\\ \\vdots \\\\ x_{t+k}\n\\end{bmatrix} =\n\\begin{bmatrix}\nA \\\\ A^2 \\\\ \\vdots \\\\A^k\n\\end{bmatrix}\nx_t\n+\n\\begin{bmatrix}\nB & 0 & \\cdots & 0 \\\\\nAB & B & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA^{k-1}B & A^{k-2}B & \\cdots & B\n\\end{bmatrix}\n\\begin{bmatrix}\nu_t \\\\ u_{t+1} \\\\ \\vdots \\\\ u_{t+k-1}\n\\end{bmatrix}\n\\]\nWe can denote this equation as\n\\[\nX = S^x x_t + S^u U\n\\]\nFor a 2-norm cost function, we can write\n\\[\n\\begin{aligned}\nJ_k(x_t,\\mathbf{U}) &= J_f(x_k) + \\sum_{i=0}^{k-1}l(x_i,u_i) \\\\\n&= x_k^\\text{T} Q_f x_k + \\sum_{i=0}^{k-1}\\left(x_i^\\text{T}Q_ix_i + u_i^\\text{T}R_iu_i\\right) \\\\\n&= X^\\text{T}\\bar{Q}X + U^\\text{T}\\bar{R}U\n\\end{aligned}\n\\]\nwhere\n\\[\n\\mathbf{U} =\n\\begin{bmatrix}\n    X \\\\ U\n\\end{bmatrix} \\quad\n\\bar{Q} = \\begin{bmatrix}\n    Q_1 & & & \\\\\n     & \\ddots & & \\\\\n     & & Q_{k-1} & \\\\\n     & & & Q_f\n\\end{bmatrix} \\quad\n\\bar{R} = \\begin{bmatrix}\n    R & & \\\\\n     & \\ddots & \\\\\n     & & R \\\\\n\\end{bmatrix}\n\\]\nSubstituting the expression of state space model into the cost, we have\n\\[\n\\begin{aligned}\nJ_k(x_t,\\mathbf{U}) &= X^\\text{T}\\bar{Q}X + U^\\text{T}\\bar{R}U \\\\\n&= \\left(S^x x_t + S^u U\\right)^\\text{T}\\bar{Q}\\left(S^x x_t + S^u U\\right) + U^\\text{T}\\bar{R}U \\\\\n&= U^\\text{T}\\underbrace{\\left((S^u)^\\text{T}\\bar{Q}S^u+\\bar{R}\\right)}_H U + 2x_t^\\text{T}\\underbrace{(S^x)^\\text{T}\\bar{Q}S^u}_F U + x_t^\\text{T}\\underbrace{(S^x)^\\text{T}\\bar{Q}S^x}_Y x_t\\\\\n&= U^\\text{T}HU + 2x_tFU + x_t^\\text{T}Yx_t\n\\end{aligned}\n\\] Compare with the standard form of cost in QP formulation \\(\\frac{1}{2}U^\\text{T}HU + U^\\text{T}g\\), we can easily get \\[\n\\begin{aligned}\nH &= 2\\left((S^u)^\\text{T}\\bar{Q}S^u + \\bar{R}\\right) \\\\\ng &= 2(S^u)^\\text{T}\\bar{Q}S^xx_t\n\\end{aligned}\n\\]\n\n\nb. Create QP Cost\nRecall that the form of our MPC problem is\n\\[\n\\begin{aligned}\n\\min_{x, u} \\quad& \\sum_{i=0}^{k-1}(x_{i+1}-x_{i+1,\\text{ref}})^{\\text{T}}Q_i(x_{i+1}-x_{i+1,\\text{ref}}) + u_i^\\text{T}R_iu_i \\\\\n\\text{s.t.} \\quad& x_{i+1} = A_ix_i + B_iu_i \\\\\n& \\underline{c}_i \\leq C_iu_i \\leq \\overline{c}_i \\\\\n& D_iu_i = 0\n\\end{aligned}\n\\]\nwhere \\(i = 0, \\cdots, k-1\\).\nIn this project, we choose \\(Q_1 = \\cdots = Q_{k-1} = Q_f\\), i.e.\n\\[\n\\bar{Q} = \\begin{bmatrix}\n    Q & & & \\\\\n     & \\ddots & & \\\\\n     & & Q & \\\\\n     & & & Q\n\\end{bmatrix} \\quad\n\\bar{R} = \\begin{bmatrix}\n    R & & \\\\\n     & \\ddots & \\\\\n     & & R \\\\\n\\end{bmatrix}\n\\]\n_Qi = np.diag(np.array(parameters['Q'], dtype=float))\nQbar = np.kron(np.identity(horizon), _Qi)\n\n_r = parameters['R']\n_Ri = _r * np.identity(num_input, dtype=float)\nRbar = np.kron(np.identity(horizon), _Ri)\nLet’s substitute the dynamic constraint into the cost function, then we can get\n\\[\n\\begin{aligned}\nJ &= \\sum_{i=0}^{k-1}(x_{i+1}-x_{i+1,\\text{ref}})^{\\text{T}}Q_i(x_{i+1}-x_{i+1,\\text{ref}}) + u_i^\\text{T}R_iu_i \\\\\n&= \\sum_{i=0}^{k-1}(A_ix_i+B_iu_i-x_{i+1,\\text{ref}})^{\\text{T}}Q_i(A_ix_i + B_iu_i-x_{i+1,\\text{ref}}) + u_i^\\text{T}R_iu_i \\\\\n&= (S^x x_t + S^u U - X_{\\text{ref}})^\\text{T}\\bar{Q}(S^x x_t + S^u U - X_\\text{ref}) + U^\\text{T}\\bar{R}U\n\\end{aligned}\n\\]\nwhere \\(X_\\text{ref} = [x_{1,\\text{ref}}, \\cdots, x_{k,\\text{ref}}]^\\text{T}\\). We can get the QP matrix with the same procedure.\n\\[\n\\begin{aligned}\nH &= 2\\left((S^u)^\\text{T}\\bar{Q}S^u + \\bar{R}\\right) \\\\\ng &= 2(S^u)^\\text{T}\\bar{Q}(S^xx_t-X_\\text{ref})\n\\end{aligned}\n\\]\n\n\nc. Create QP Constraint\n\n\nd. QP Solver\nqpsolvers: https://pypi.org/project/qpsolvers/\nAs mentioned in the project description of qpsolvers, we need to write our formulation in the form\n\\[\n\\begin{aligned}\n\\min_x &\\quad \\frac{1}{2}x^\\text{T}Px + q^\\text{T}x \\\\\n\\text{s.t.} & \\quad Gx \\leq h \\\\\n&\\quad Ax = b \\\\\n&\\quad \\text{lb} \\leq x \\leq \\text{ub}\n\\end{aligned}\n\\]\nThen we can get the solution using the code below\nfrom qpsolvers import solve_qp\nx = solve_qp(P, q, G, h, A, b, lb, ub)\nprint(\"QP solution: x = {}\".format(x))\nThe QP problem in the paper is written as\n\\[\n\\begin{aligned}\n\\min_\\mathbf{U} &\\quad \\frac{1}{2}\\mathbf{U}^\\text{T}\\mathbf{HU} + \\mathbf{U}^\\text{T}\\mathbf{g} \\\\\n\\text{s.t.} &\\quad \\underline{\\mathbf{c}} \\leq \\mathbf{CU} \\leq \\overline{\\mathbf{c}}\n\\end{aligned}\n\\]\n\n\nReference\n[1] Di Carlo J., Wensing P. M., Katz B., et al. Dynamic Locomotion in the MIT Cheetah 3 Through Convex Model-Predictive Control[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018: 1-9. [PDF]\n[2] Bledt G, Powell M J, Katz B, et al. Mit cheetah 3: Design and Control of a Robust, Dynamic Quadruped Robot[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018: 2245-2252. [PDF]"
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html",
    "href": "post/study/2025-03-01-how-to-read.html",
    "title": "How to read papers",
    "section": "",
    "text": "논문을 읽는법 영상은 논문 읽는 효율적인 방법에 대한 팁을 제공합니다. 논문 읽는 것에 정답이 없고 이미 각자의 방법이 있겠으나, 참고하기에 좋은 것 같아 AI 요약본으로 공유합니다.\n핵심은 처음부터 모든 내용을 꼼꼼히 읽는 것이 아니라, 자신의 이해도에 맞춰 전략적으로 접근하는 것입니다. 처음에는 랜드마크 논문 을 찾아 Introduction과 Related Works를 통해 분야의 큰 그림을 파악하고, 이후에는 Method와 Experiment 중심으로 읽어 나가는 것이 중요합니다. 또한, 논문 에서 감춰진 한계점 을 파악하고, 새로운 아이디어를 발견하는 데 집중해야 합니다. 이 영상은 연구 주제를 설정 하고 논문의 완성도 를 높이는 데 실질적인 도움을 줄 수 있습니다."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#논문-읽기의-시작",
    "href": "post/study/2025-03-01-how-to-read.html#논문-읽기의-시작",
    "title": "How to read papers",
    "section": "1. 🏁논문 읽기의 시작",
    "text": "1. 🏁논문 읽기의 시작\n\n일반적으로 논문 한 편을 쓰기 위해 약 100편의 관련 논문 을 읽어야 하지만, 이는 학술적 내용과 수식 때문에 쉽지 않은 과제다.\n기존의 논문 읽기 방법(초록, 결론 순으로 읽기)은 빠르게 읽을 수는 있지만, 내용 이해와 기억에는 효과적이지 않다는 것이 밝혀졌다.\n효율적인 논문 읽기의 핵심은 정해진 순서가 아닌, 읽는 사람의 이해 수준 에 따라 접근 방식을 조정하는 것이다.\n새로운 연구 분야에 진입할 때는 경험이 전혀 없는 상태에서 시작하므로, 이에 맞는 특별한 접근 방식이 필요하다."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#랜드마크-논문-찾기-연구의-시작점",
    "href": "post/study/2025-03-01-how-to-read.html#랜드마크-논문-찾기-연구의-시작점",
    "title": "How to read papers",
    "section": "2. 🔍랜드마크 논문 찾기: 연구의 시작점",
    "text": "2. 🔍랜드마크 논문 찾기: 연구의 시작점\n\n연구 분야의 랜드마크  논문 을 찾는 것이 논문 읽기의 첫 단계다.\n키워드로 검색한 논문의 Related Works 섹션을 통해 관련 연구들을 파악할 수 있다.\n여러 논문 에서 공통적으로 인용되는  논문 들을 모아두면 약 10개 정도의 핵심 논문 을 찾을 수 있다.\n구글 스칼라에서 인용 횟수 를 확인하여 논문의 중요성과 품질을 판단할 수 있다.\n랜드마크 논문을 찾았다고 해서 바로 방법론(Methodology)을 읽는 것은 적절하지 않다."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#연구-분야의-큰-그림-파악하기",
    "href": "post/study/2025-03-01-how-to-read.html#연구-분야의-큰-그림-파악하기",
    "title": "How to read papers",
    "section": "3. ⛰️연구 분야의 큰 그림 파악하기",
    "text": "3. ⛰️연구 분야의 큰 그림 파악하기\n\n초심자는 개별 논문 의 문제 해결 방식보다 연구 분야의 전체적인 맥락 을 먼저 이해해야 한다.\n랜드마크 논문 의 Introduction과 Related Works 를 자세히 읽어 연구 분야의 큰 방향성을 파악해야 한다.\n마치 이야기책을 읽듯이 연구 분야의 흐름을 파악하는 과정은 생각보다 흥미롭다.\n여러 논문 을 읽다 보면 반복되는 내용 이 나타나는데, 이는 해당 분야의 전반적인 이해를 얻었다는 신호다.\n이러한 과정을 통해 연구 분야에서 어떤 일이 일어나고 있는지 대략적으로 파악할 수 있다."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#논문-읽기의-효율적-접근-방법",
    "href": "post/study/2025-03-01-how-to-read.html#논문-읽기의-효율적-접근-방법",
    "title": "How to read papers",
    "section": "4. 🧐논문 읽기의 효율적 접근 방법",
    "text": "4. 🧐논문 읽기의 효율적 접근 방법\n\n연구 분야의 컨텍스트를 파악한 후에는 Introduction과 Related Works를 건너뛰고 Methodology와 Experiment 위주로 논문 을 읽는다.\n여러 논문 의 Introduction과 Related Works는 비슷한 내용 을 다루므로, 이미 알고 있는 내용은 생략하고 문제 해결 방식에 집중 한다.\n이러한 전략적 접근으로 읽어야 할 양이 절반 이하로 줄어들어 효율성이 크게 향상된다.\n처음부터 끝까지 모든 내용을 꼼꼼히 읽는 대신, 중요한 논문들을 선별하여 Methodology와 Experiment 위주 로 읽는다.\n이러한 방식으로 논문을 읽다 보면 연구 분야의 전체적인 흐름과 미해결 문제 들을 파악할 수 있게 된다."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#좋은-논문의-조건과-효과적인-논문-읽기-방법",
    "href": "post/study/2025-03-01-how-to-read.html#좋은-논문의-조건과-효과적인-논문-읽기-방법",
    "title": "How to read papers",
    "section": "5. 🚩좋은 논문의 조건과 효과적인 논문 읽기 방법",
    "text": "5. 🚩좋은 논문의 조건과 효과적인 논문 읽기 방법\n\n좋은 논문 은 노벨티(novelty) 와 완결성 이라는 두 가지 요소를 충족해야 한다.\n노벨티 는 다른 사람이 시도하지 않은 새롭고 가치 있는 아이디어를 의미하며, 이를 위해서는 기존 연구를 파악해야 한다.\n논문 100편을 읽는 진정한 목적은 연구 분야를 이해하고 좋은 연구 주제를 설정 하기 위함이다.\n논문 을 읽을 때는 방법론(Methodology)과 실험(Experiment) 위주로 읽되, 각 방법의 한계점 을 파악하는 것이 중요하다.\n논문은 밀도 높은 텍스트 이므로, 모든 논문 을 완전히 소화하기보다는 효율적으로 선별하고 이해 하는 능력이 중요하다."
  },
  {
    "objectID": "post/paper/2025-03-02-quad-wild.html#height-sample-randomization",
    "href": "post/paper/2025-03-02-quad-wild.html#height-sample-randomization",
    "title": "Learning robust perceptive locomotion for quadrupedal robots in the wild",
    "section": "Height sample randomization",
    "text": "Height sample randomization\n\nStudent Policy 방법론 더 자세히\n\n👉 핵심 아이디어\n- 실제 환경에서 발생할 수 있는 다양한 센서 오류 및 맵 품질 저하를 학습 시뮬레이션\n- Noise 적용 방식을 세분화하여 다양한 조건에서 정책이 강건하게 작동하도록 유도\n- 지형 특성 변화도 반영하여 현실적인 지형 적응 능력 강화\n\nNoise 모델 적용\n\n\nNoise 모델 \\(n(\\tilde{o}^e_t | o^e_t, z)\\), \\(z \\in \\mathbb{R}^{8 \\times 4}\\) 사용\n높이 샘플에 두 가지 유형의 측정 Noise 추가\n\nScan Point를 수평으로 이동(Shift)\n\n높이 값에 교란 추가(Height Disturbance)\n\n\nNoise 값은 가우시안 분포 에서 샘플링되며, Noise 매개변수 \\(z\\) 가 분산을 결정\n\nNoise 매개변수 벡터 \\(z\\) 는 커리큘럼 학습의 일부\n\n훈련 기간이 진행됨에 따라 Noise 강도를 선형적으로 증가\n\n\n\n\n\n\nNoise는 3가지 다른 범위 에서 적용\n\nPer-Scan-Point → 각 스캔 포인트마다 매 시간 단계마다 재샘플링\nPer-Foot → 각 다리(발)별로 매 시간 단계마다 재샘플링\nPer-Episode → 모든 스캔 포인트에 대해 일정한 Noise 유지\n\n\n매핑 오류 시뮬레이션\n\n맵 품질 변화 및 오류 원인 시뮬레이션\n3가지 매핑 조건 정의 (훈련 에피소드에서 60%, 30%, 10% 확률로 선택)\n\n정상적인 운영 상태 → 양호한 맵 품질, 표준적인 Noise 적용\n\n지도 오프셋 발생 → 자세 추정 드리프트 또는 변형 가능한 지형(예: 진흙, 눈)으로 인해 큰 오프셋 추가\n\n매핑 실패 시뮬레이션 → 폐색(occlusion) 또는 센서 오류로 인해 각 스캔 포인트에 큰 Noise 추가\n\n\n\n\n\n지형 변화 시뮬레이션\n\n훈련 지형을 셀(Cell) 단위로 나누고, 특정 셀의 height map에 추가 오프셋 적용\n서로 다른 지형 특성 간의 전환 시뮬레이션 (예: 식생, 깊은 눈 등)"
  },
  {
    "objectID": "post/paper/2025-03-02-quad-wild.html#belief-state-encoder",
    "href": "post/paper/2025-03-02-quad-wild.html#belief-state-encoder",
    "title": "Learning robust perceptive locomotion for quadrupedal robots in the wild",
    "section": "Belief state encoder",
    "text": "Belief state encoder\n\nStudent Policy 방법론 더 자세히\n\n👉 핵심 아이디어\n- 고유 감각(Proprioception)과 외부 감각(Exteroception) 정보를 효과적으로 통합\n    - 외부 감각의 신뢰도를 학습하여 동적으로 조절 \n    - 외부 감각이 부정확할 때는 고유 감각을 더 많이 활용하도록 학습\n- RNN(GRU) 기반으로 Belif state를 추정하여 POMDP 문제 해결 \n- 게이트 디코더를 활용하여 Belif state의 유효성을 보장\nGated Encoder\n\n게이트 RNN 모델과 Multi-Modal Information Fusion 기법에서 영감\n\nadaptive gating 메커니즘 사용 → 외부 감각 정보를 얼마나 반영할지 조절\n\n\n\n\nBelif state 계산 과정\n\n입력 데이터\n\n고유 감각: \\(o^p_t\\)\n\n노이즈가 포함된 외부 감각 특징: \\(l^e_t = g_e(\\tilde{o}^e_t)\\)\n\n이전 숨겨진 상태: \\(h_t\\)\n\nRNN을 통해 중간 Belif state \\(b^\\text{'}_t\\) 계산\n\\[\nb^\\text{'}_t, h_{t+1} = \\text{RNN}(o^p_t, l^e_t, h_t)\n\\]\n\nGRU(Gated Recurrent Unit) 기반 RNN 사용\n\n시간적 정보(Sequential Information) 유지\n\nattention 벡터 \\(\\alpha\\) 계산: 외부 감각 정보를 얼마나 반영할지 결정 \\[\n\\alpha = \\sigma(g_a(b^\\text{'}_t))\n\\]\n\n\\(g_a\\): 완전 연결 신경망(FCN)\n\\(\\sigma(\\cdot)\\): 시그모이드 활성화 함수\n\\(\\alpha\\) 값이 클수록 외부 감각 정보를 더 많이 반영\n\n최종 Belif state \\(b_t\\) 계산\n\\[\nb_t = g_b(b^\\text{'}_t) + l^e_t \\cdot \\alpha\n\\]\n\n\\(g_b\\): 완전 연결 신경망(FCN)\n외부 감각 정보 \\(l^e_t\\) 를 attention 벡터 \\(\\alpha\\) 에 의해 가중 조합\n\n\nGated Decoder\n\n같은 게이트 메커니즘을 디코더에서도 사용\nPrivileged Information 및 높이 샘플을 재구성\nReconstruction Loss 계산 → Belif state \\(b_t\\) 가 환경 정보를 올바르게 반영하도록 유도\n\n\n\n\nGRU(Gated Recurrent Unit) 사용 이유\n\nGRU는 LSTM보다 계산량이 적으면서도 장기 의존성(Long-term dependencies)을 효과적으로 유지\n게이트 구조를 통해 중요한 정보만 선택적으로 저장 및 업데이트 가능"
  },
  {
    "objectID": "post/paper/2025-02-22-papers.html",
    "href": "post/paper/2025-02-22-papers.html",
    "title": "[3] 3mins papers",
    "section": "",
    "text": "Hybrid Internal Model\n\nLearning Agile Legged Locomotion with Simulated Robot Response\n\n\nContrasive Learning(SwaV 기반)을 통해 Latent vector 생성. Positive Pair와 Negative Pair를 분류하여 비슷한 환경에서는 Latent vector들이 비슷한 값을 갖도록, 다른 환경에서는 다른 표현을 갖도록 하는 클러스터링 방법을 사용하여 Latent vector space 공간을 구성하도록 함.\n직접적으로 환경 정보를 예측하지 않고 간접적으로 proprioceptive data를 기반으로 지형에 대응하는 방법임\n코드가 공개되어 있고 성능이 좋아보이며, 지형에 따라 Latent가 구분되는 것을 확인할 수 있으며 DreamWaQ보다 뚜렷하게 구분되는 특징을 확인할 수 있음\nPaper Link\nCode\n\n\n\n\nLearning Quadrupedal Locomotion over Challenging Terrain\n\n정말 유명한 논문임. 극도로 강건한 보행 컨트롤러 설계를 위해 proprioceptive (고유수용성 - 내계 정보 = joint encoder, IMU, etc) 만을 사용해 사족보행 로봇의 진흙, deformable terrain, 자갈, 동적인 장판, 식생, 거센 물살에서도 동작하게 만들 수 있음. 더 단순한 도메인에서 학습을 수행해도, 실제 자연 환경에서의 강건함을 극대화 할 수 있음.\n구현을 위해 교사 정책을 학습하였음. 시뮬레이션에서 지면의 정확한 높이, 경사도, 접촉 상태와 같은 특권 (privileged) 정보를 활용하여 정책을 학습하고, 실제 로봇에서는 student policy를 사용하여 고유수용성 데이터만을 입력으로 사용하는 학생 정책을 학습함.\n지난 논문인 Provable Partially Observable Reinforcement Learning with Privileged Information와 연결해 보면, 결국 이렇게 학습을 해서 동작을 하는 이유는, ’특권 정보’의 활용이 결정론적 필터 조건 (Deterministic Filter Condition) 조건을 만족하지 않더라도, 일부 제약 조건을 만족할 수 있다는 뜻으로 해석될 수 있음. 적절한 constraint 를 걸어서 lower bound를 만든다면, safe condition (constraint)를 만족할 수 있는 강화학습 보행 컨트롤러를 만들수 있지 않을까 생각됩니다.\nPaper Link\n\n\n\n\nExtreme Parkour with Legged Robots\n\n저비용의 센서(depth 카메라 한개)와 저렴한 사족보행로봇(actuation이 부정확함)을 활용하여 매우 뛰어난 수준의 파쿠르 동작을 수행하는 모델을 만드는 방법에 대해서 소개하는 논문임.\n이는 기존의 파쿠르와 같은 동작을 수행하는 논문들보다 훨씬 간단하면서도 저비용으로 실현할 수 있음을 증명했다. ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robots의 경우 구현을 하기 위해서는 mapping module 따로, planning module따로, control module 따로 필요했었지만, 이 방식은 하나의 네트워트만 학습시켜주면 된다. 이때 학습 기법은 위 이미지와 같이 privileged information을 통해 완성도 있는 teacher 모델을 만들고, 이를 Dual Distillation 학습 기법을 통하여 student 모델을 만든다. (phase 1: 전문가 데이터를 활용한 지도학습 / Phase 2: 지도 데이터 없이 로봇이 스스로 방향을 예측하며 학습)\n영상을 확인한 후, 학습이 매우 잘 이루어졌으며 sim-to-real 전이도 성공적으로 수행되었음을 알 수 있었다. 그러나 이러한 방식으로 학습된 네트워크가 충분히 안전한지에 대해서는 다소 의문이 든다. 특히, 좌우 및 후방에 대한 정보 없이 높은 속도로 전진하는 네트워크는 예상치 못한 위험을 초래할 가능성이 있어 보인다. 또한, 이전 연구에서는 상단 장애물까지 고려하여 로봇이 몸을 숙이는 동작을 학습하는 등 더 다양한 정보를 활용하여 성능을 향상시켰다. 따라서 단순히 저비용 네트워크를 구축하는 것이 반드시 최선의 방법이라고는 생각되지 않는다. 비용 절감도 중요하지만, 보다 풍부한 센서 정보를 활용하여 안전성과 전반적인 성능을 극대화하는 접근이 더 바람직할 것으로 보인다.\nPaper Link\nVideo\n\n\n\n\nLearning Quadruped Locomotion Using Differentiable Simulation\n\n효율적인 역전파를 위해 단순한 surrogate dynamics model에서 얻은 smooth gradients와 정확한 forward simulation을 위해 non-differentiable 시뮬레이터의 high fidelity을 결합한 새로운 정책학습 방식 제안\nnon-differentiable인 IsaacGym 시뮬레이터는 복잡한 contact 동역학을 시뮬레이션할 수 있으며, 이를 활용하여 단순화된 rigid-body dynamics model의 상태(state)를 정렬(align)함으로써, training pipeline이 실제 동역학에 기반을 두도록 보장\nDifferential simulator를 따로 개발하는 움직임(brax)도 있는데 differential loss term을 추가해서 non-differential simulator를 보완하는 방식이 새로웠으며 PPO보다 적은 agent수로도 학습 reward가 빠르게 maximization된다는 점이 놀라웠음\nPaper Link"
  },
  {
    "objectID": "post/paper/2025-02-20-papers.html",
    "href": "post/paper/2025-02-20-papers.html",
    "title": "[2] 3mins papers",
    "section": "",
    "text": "Actor-Cross-Critic\n\nLeveraging Privileged Information for Partially Observable Reinforcement Learning\n\n\nVAE 등을 사용하는 방법들은 간접적인 정보 제공을 하기 때문에 Asymmetric Actor Critic 구조의 문제를 해결해야 함\n두개의 Critic 네트워크들을 학습하고 Advantage가 더 큰 값을 선택하여 Actor에게 제공하는 방법. Oracle Critic과 Executor Critic 2개의 네트워크를 가지고 Overestimation 문제를 방지\n이러한 ACC의 학습 과정이 기존의 Actor Critic 방법보다 더 안정적이고 빠르게 수렴함을 증명\nPaper Link\n\n\n\n\nANYmal Parkour\n\nLearning Agile Navigation forQuadrupedal Robots\n\n\n완전 학습 기반(fully learned) 네비게이션: 환경의 3D 재구성 정보를 활용하여 보행 기술을 동적으로 선택.\n하이브리드 정책 구조: PPO 기반으로 Gaussian 분포(저수준 명령) + 범주형 분포(기술 선택)를 결합.\n새로운 보행 기술 학습: 점프, 등반, 웅크리기 등 다양한 역동적 동작을 학습하여 높은 장애물도 극복 가능.\n신경망 기반 3D 환경 재구성: 다중 해상도 맵핑을 통해 로봇 주변은 고해상도로, 멀리 있는 환경은 저해상도로 인식하여 실시간 처리 성능을 확보.\n지금까지 연구해온 연구 내용들을 총망라하여 perception 부터 agile한 control까지 가능하게 만드는 시스템을 구축한 느낌을 받음\nPaper Link\n\n\n\n\nStiffness Tuning\n\nVariable Stiffness for Robust Locomotion through Reinforcement Learning\n\n\n강화학습 보행 제어에서, joint stiffness 튜닝 없이도 성능을 유지하면서 변수 stiffness p gain을 action space에 통합하는 새로운 제어 패러다임을 제안.\nstiffness을 관절별(PJS), 다리별(PLS), 하이브리드(HJLS)로 그룹화하여 제어하는 방식을 적용하여 PLS는 속도 추적 및 외력 대응에서 우수하며, HJLS는 에너지 효율성을 극대화하는 결과를 보여줌\nPd 튜닝 중 d 게인은 p 게인에 의존 변수로 놓았고 기존의 action space 디자인을 색다르게 했다는 점에서 참신한 것 같음. 강화학습 보행제어에서 관습적으로 하는 부분을 건드렸다는 측면에서 새로웠음.\nPaper link\n\n\n\n\nPrivileged Information\n\nProvable Partially Observable Reinforcement Learning with Privileged Information\n\n\n강화학습은 무조건 적으로 Partially Observable MDP (POMDP) 환경이며, 학습 단계에서 시뮬레이션에서만 얻을 수 있는 노이즈가 없는 깨끗한 데이터인 특권 정보 (Privileged Information) 이 존재함. 예를 들어 노이즈가 없는 완전한 상태 정보를 이용하면 정책을 학습하는데 도움이 되며, 실제 실행에서는 전문가 증류 (expert distillation) 과 비대칭 액터-크리틱 (훈련과 실행 단계에서 다른 관찰값을 사용하는 방식 - 크리틱은 특권 정보를 활용하여 평가, 액터는 제한된 관찰값만을 사용)을 이용하여 현실 데이터에 적용함.\n최근 증류 기법이 여러 학습에서 매우 뛰어난 성능을 보이고 있음. 그러나 최근 적용되고 있는 증류기법은 항상 최적 정책을 보장하지 않으며, 근사 최적 정책을 학습하는 과정에서 손실이 발생함. 저자들은 POMDP에서 관찰값을 통해 현재의 진짜 상태 (state)를 결정할 수 있는 조건을 만족하는지에 대한 여부를 결정론적 필터 조건 (Deterministic Filter Condition)으로 정의함. 이 조건, 즉 필터가 결정론적일 경우에 (관찰값이 주어지면 현재 상태를 정확하게 추론이 가능할 때), 전문가 증류의 손실을 방지할 수 있음. 예시로, 로봇 손을 이용한 물체 조작은 (조건: 로봇이 손에 있는 물체의 위치와 방향을 정확히 측정할 수 있다면, 결과: 관찰을 통해 물체의 실제 상태를 결정할 수 있음) 이 조건을 만족함. 자율 주행 차량에서는 (조건: 도로의 미끄러움 정도는 차량 센서로 알 수 없음. 결과: 센서값이 동일해도 도로의 미끄러움 정도는 달라질 수 있음) 이 조건을 만족하지 않음.\n이 논문은 NIPS 논문이며, 수식적 증명이 들어가는 논문으로 72 페이지의 분량을 가지고 있네요. 리뷰어들이 고생을 많이 했겠습니다.. 이 논문의 아이디어를 차용해 보자면, 증류 및 비대칭 액터-크리틱은 적용할 수 있겠습니다. 다만, 사족보행 로봇은 이 ’결정론적 필터 조건’이 충족되지 않을 조건을 너무 많이 가지고 있기에, 이 논문의 아이디어를 적용해 보려면 적당한 assumption을 통해서 lower bound를 찾아내어 적용해 보는 것도 재미있겠다는 생각이 드네요\nPaper Link"
  },
  {
    "objectID": "post/paper/2025-04-13-diffusion-map.html",
    "href": "post/paper/2025-04-13-diffusion-map.html",
    "title": "Diffusion 아이디어",
    "section": "",
    "text": "논문 원문\n코드 Repository : ROS 코드는 공개 안함\n프로젝트 페이지"
  },
  {
    "objectID": "post/paper/2025-04-13-diffusion-map.html#diffusion-model-간단-소개",
    "href": "post/paper/2025-04-13-diffusion-map.html#diffusion-model-간단-소개",
    "title": "Diffusion 아이디어",
    "section": "Diffusion Model 간단 소개",
    "text": "Diffusion Model 간단 소개\nDDPM(Denoising Diffusion Probabilistic Model)은 최근 이미지 생성 등 분야에서 각광받는 생성 모델의 한 종류입니다. 기본 아이디어는 매우 간단합니다:\n모델은 먼저 이미지나 신호에 점차적으로 노이즈를 추가해 데이터를 파괴(destroy)하는 과정을 정의하고, 역으로 그렇게 섞인 노이즈를 단계적으로 제거하며 새로운 샘플을 생성하는 과정을 학습합니다. 다시 말해, 앞방향 과정에서는 깨끗한 데이터에 조금씩 가우시안 노이즈를 덧붙여 최종적으로 거의 무작위 노이즈 상태까지 만드는 반면, 역방향 과정에서는 완전한 노이즈에서 시작해 한 단계씩 노이즈를 제거(denoise)하면서 데이터를 복원하듯이 샘플을 만들어내는 것입니다. 모델은 이러한 노이즈 추가/제거 과정 전체를 확률적으로 학습하여, 최종적으로 아무것도 없는 상태(백색 노이즈)에서 출발해도 훈련 데이터와 유사한 새로운 데이터를 생성할 수 있게 됩니다.\n\n\n\nDiffusion Mechanism\n\n\nDDPM은 새로운 데이터를 만들어낼 때 다양하고 섬세한 제어가 가능하다는 것입니다. 예를 들어 조건부 샘플링이나 guidance 기법을 통해 원하는 스타일이나 특성을 유도할 수 있습니다. ADTG에서는 이러한 디퓨전 모델의 표현력과 제어 가능성을 지형 생성 문제에 적용합니다."
  },
  {
    "objectID": "post/paper/2025-04-13-diffusion-map.html#지형-인식-정책-최적화를-위한-적응형-커리큘럼-학습",
    "href": "post/paper/2025-04-13-diffusion-map.html#지형-인식-정책-최적화를-위한-적응형-커리큘럼-학습",
    "title": "Diffusion 아이디어",
    "section": "지형 인식 정책 최적화를 위한 적응형 커리큘럼 학습",
    "text": "지형 인식 정책 최적화를 위한 적응형 커리큘럼 학습\n사실상 현실적으로 모든 heightmap에 대해서 학습할 수는 없습니다. 비현실적인 해법은 모든 가능한 지형 \\(\\Lambda = (e_1, ..., e_N)\\)에서 훈련하는 것입니다. 이때 \\(p(e)\\)는 \\(\\Lambda\\)에 대한 균일 분포로 설정됩니다.\n효과적인 환경 생성기를 설계하는 것이 중요합니다.\n\n현실 세계의 분포와 일치하는 현실적인 환경을 생성하고\n현재 정책에 적절한 도전을 제공해야 합니다.\n\n조정 가능한 미리 정의된 지형 유형은 제어가 가능하다는 장점이 있지만 다양성이 부족할 수 있으며, 생성 모델은 현실성 면에서는 뛰어나지만 정책에 정확히 맞춘 도전 과제 설정에는 한계가 있을 수 있습니다."
  },
  {
    "objectID": "post/paper/2025-04-13-diffusion-map.html#디퓨전으로-지형을-생성하는-방법",
    "href": "post/paper/2025-04-13-diffusion-map.html#디퓨전으로-지형을-생성하는-방법",
    "title": "Diffusion 아이디어",
    "section": "디퓨전으로 지형을 생성하는 방법",
    "text": "디퓨전으로 지형을 생성하는 방법\n\n4장 Adaptive Diffusion Terrain Generator\n\n\n\n\n3\n\n\n\nDDPM 잠재 공간에서 “쉬운” 지형과 “어려운” 지형 간의 보간을 통해 정책 훈련을 최적화하는 지형을 생성\n훈련 데이터셋의 분산에 따라 초기 노이즈 입력을 조정하여 지형의 다양성을 풍부하게 하여 더 넓은 경험을 조성하고 보이지 않는 지형에서 정책의 일반화를 개선\n\nADTG의 핵심 아이디어는 이미 학습된 디퓨전 모델을 활용해 현재 정책에 가장 도움이 될 새로운 지형을 생성하되, 그 생성 과정을 현재 정책의 성능 지표로 안내(guidance)한다는 것입니다. 이를 위해 ADTG는 두 가지 주요 단계로 진행:\n(1) 정책 성능 기반 지형 생성 (Performance-Guided Generation): DDPM의 latent variable (noise)을 합성하여 정책 훈련을 최적화하는 지형을 생성합니다.\n(2) 초기 노이즈 조정을 통한 훈련 데이터셋 다양화 (Diversifying Training Dataset): 훈련 데이터셋의 variance에 따라 초기 노이즈 수준을 조절하여 지형 다양성을 증가시키고 정책의 일반화 능력을 향상시킵니다.\n\n\n1. 초기 노이즈 최적화와 latent 보간\n일반적으로 디퓨전 모델로 새로운 샘플을 생성하려면 우선 초기 노이즈(\\(e_K\\))를 무작위로 뽑아서 역확산(reverse diffusion)을 진행합니다. 하지만 ADTG에서는 이 초기 노이즈를 무작위로 선택하지 않고 최적화합니다.\n\n\\(e\\), \\(e_0\\), \\(e_k\\)를 훈련 데이터셋의 환경, DDPM을 통해 생성된 지형, 그리고 시간 단계 k의 DDPM 잠재 변수로 각각 나타냅니다. 이 세 변수는 모두 동일한 크기 \\(R^{W × H}\\)\n\n구체적으로, 현재 훈련 데이터셋에 포함된 지형들 중에서 일부를 선택하고, 각 지형 \\(e_i\\)에 대해 디퓨전 앞방향을 \\(k\\) 단계 진행하여 latent 표현 \\(e_i^k\\)를 얻습니다. 여기서 \\(k\\)는 일종의 중간 노이즈 수준을 나타내는 하이퍼파라미터로, \\(k=0\\)이면 원본 지형(노이즈 없음), \\(k=K\\)이면 완전한 가우시안 노이즈에 해당합니다. 이렇게 얻은 여러 개의 부분 노이즈 지형(latent)들을 합성(combine)하여 새로운 초기 노이즈 \\(e'_k\\)를 만들고자 하는데, 단순 평균이 아니라 정책의 성능에 기반한 가중 합으로 결합하는 것이 포인트입니다.\n\\[e'_k = \\frac{\\sum_i w(e_i, \\pi)\\; e_i^k}{\\sum_i w(e_i, \\pi)}\\]\n위 식에서 \\(w(e_i, \\pi)\\)는 지형 \\(e_i\\)에 대한 현재 정책 \\(\\pi\\)의 성능에 따라 달라지는 가중치입니다. 예를 들어, 어떤 지형에서 정책이 너무 쉽게 100% 성공한다면 그 지형은 학습에 덜 유용할 수 있고(너무 쉬움), 반대로 성공률이 0%에 가깝다면 현재 정책 수준에서는 너무 어려운 환경일 것입니다. ADTG는 가장 학습 효과가 좋은 중간 난이도를 목표로 가중치를 부여합니다.\n논문에서는 정책의 성공률을 난이도 척도 \\(s(e,\\pi)\\)로 사용하고, 목표 성공률을 \\(\\bar{s}\\)로 정한 뒤 각 지형의 가중치를 \\[w(e,\\pi) = \\exp\\!\\Big(-\\frac{(s(e,\\pi)-\\bar{s})^2}{\\sigma^2}\\Big)\\] 와 같이 설정합니다. 즉, 성공률 \\(s\\)가 목표 \\(\\bar{s}\\)와 가까운 지형일수록 가중치가 높아지고, 너무 쉽거나 너무 어려워서 성공률이 극단적인 지형은 가중치가 낮아지도록 가우시안 형태로 penalize한 것입니다. 이렇게 하면 현재 정책에게 적절한 난이도의 지형들이 초기 노이즈 합성에 더 큰 기여를 하게 됩니다.\n\n결과적으로, \\(e'_k\\)는 여러 기존 지형들의 특징을 성능 기반 가중치로 블렌딩한 중간 노이즈 상태\n\n이 \\(e'_k\\)를 디퓨전 모델의 역방향 과정에 투입하여 \\(k\\)부터 \\(0\\)까지 거꾸로 진행하면 최종적으로 새로운 지형 \\(e'_0\\)가 생성됩니다. 이 지형은 기여한 지형들로부터 전반적인 기복 패턴과 같은 고수준의 특징을 물려받되, 세부적으로는 디퓨전 모델의 확률적 생성 특성 덕분에 완전히 동일하지는 않은 변형된 지형이 됩니다. 쉽게 말하면, 여러 환경의 DNA를 섞어서 새로운 난이도의 환경을 만들어내는 것과 비슷합니다. 가중치 \\(w_i\\)를 어떻게 주느냐에 따라 난이도를 조절할 수 있는데요, 하나의 환경에 무게를 많이 두면 그 환경과 비슷한 지형이 나오고, 여러 환경을 고르게 섞으면 좀 더 새로운 느낌의 지형이 만들어집니다.\n\n\n2. diffusion 노이즈 단계 조절로 다양성 확보\n위 과정만으로도 정책에 맞춘 난이도의 지형을 만들어낼 수 있지만, 학습이 진행됨에 따라 훈련 데이터셋의 다양성이 점점 부족해지는 문제가 발생할 수 있습니다. 정책이 성장하면서 기존 지형들을 모두 정복하게 되면, 아무리 그들을 섞는다 해도 더 이상 새로운 자극을 주기가 어렵기 때문입니다. 이때 필요한 것이 지형의 완전히 새로운 형태, 즉 다양성(diversity)입니다.\nADTG는 이를 위해 디퓨전 과정의 시작 단계 \\(k\\)를 동적으로 조절하여 출력 지형의 참신함(novelty)을 제어합니다.\n\n디퓨전의 노이즈 단계 \\(k\\)를 크게 하면 할수록 최종 생성되는 지형이 본래 지형들과 동떨어진 새로운 모습을 띠게 되고,\n반대로 \\(k\\)를 작게 (즉 기존 지형에 적은 노이즈만 추가) 하면 생성물이 원래 환경들과 유사한 모습을 유지한다는 점입니다.\n무작위 노이즈에서 시작하면 디퓨전 모델이 다양한 결과를 만들 수 있지만, 원본에 가까운 상태에서 시작하면 변화의 폭이 적기 때문입니다.\n\n\n\n\nFigure 1-(a)\n\n\n실제로 논문 Figure 1-(a)에서도 forward step \\(K\\)(노이즈 단계)가 증가할수록 생성 지형의 분산(다양성)이 커지는 모습을 확인할 수 있습니다 TG는 현재 훈련 데이터셋의 다양성 지표를 측정하여, 다양성이 낮다고 판단되면 더 높은 \\(k\\)를 사용하고, 다양성이 충분하면 낮은 \\(k\\)를 사용하도록 합니다. 구체적으로, 지형 데이터셋의 표본들에 대해 주성분 분석(PCA)으로 몇 가지 주요 성분의 분산 \\(\\Lambda_{\\text{var}}\\)을 계산하고, 이를 0~1로 정규화하여 현재 데이터셋 다양성 척도로 삼습니다.\n\n\\(k = K (1 - \\Lambda_{\\text{var}})\\)로 설정하는 선형 스케줄러를 사용\n\n여기서 \\(K\\)는 디퓨전 모델의 최대 시간 단계(예: 모델이 학습된 총 노이즈 단계)입니다. 이 식의 의미는\n\n데이터셋 다양성 \\(\\Lambda_{\\text{var}}\\)가 낮을수록 \\(k\\)를 크게 잡아 (즉 더 앞쪽 노이즈 단계에서 시작하여) 새롭고 다양한 지형을 만들고,\n다양성이 높을수록 \\(k\\)를 작게 하여 기존과 비슷하지만 난이도 있는 지형을 만드는 쪽으로 조절한다는 것입니다.\n\n이렇게 함으로써 학습이 진행되는 동안 지형 다양성의 정체로 인한 정책 성능 정체를 방지하고, 계속해서 훈련 환경 풀(pool)을 확장시킬 수 있습니다.\n\n요약하면, ADTG는 난이도는 정책에 맞게 유지하되, 필요에 따라 노이즈를 더 섞어서 완전히 새로운 지형을 투입함으로써 커리큘럼의 폭을 넓히는 역할을 합니다.\n\n\n\n3. Control-as-Inference로 본 노이즈 보간\n흥미롭게도, ADTG의 이러한 노이즈 최적화 및 보간 전략은 제어를 추론 문제로 보는 관점(Control-as-Inference)으로 해석할 수도 있습니다. 원래 강화학습에서의 최적 제어 문제를 확률적 추론으로 나타낼 수 있다는 아이디어인데, 여기서는 반대로 환경을 생성하는 문제를 정책 향상이라는 목표 하에 하나의 추론과정으로 본 것입니다.\n논문 부록(A)에 따르면, 앞서 “문제 정의”에서 소개한 식의 최적화는 KL 제어 이론을 통해 유도될 수 있다고 합니다. 새로운 지형을 생성하는 것은 곧 어떤 초기 노이즈 \\(e_k\\)를 선택하는 문제이며, 그 노이즈를 시작으로 디퓨전 모델을 통해 만들어진 최종 지형 \\(e_0\\)에서 정책의 성능 향상이 최대화되길 바랍니다. 이 목표를 직접 달성하려면 \\(e_k\\)에 대한 목표 함수(J)를 최대화하는 최적화를 해야겠지만, 정책 성능 향상은 시뮬레이션을 돌려봐야 알 수 있기 때문에 미분 가능하지 않고 비싼 계산입니다.\n대신 이를 확률적 샘플링 문제로 변환하여, 성능이 좋은 환경이 나올 확률을 높이는 방식으로 접근합니다. 간단히 말해, “좋은 지형일수록 더 자주 뽑히도록 하자”는 것입니다.\n수식적으로는 참조 분포 \\(q(e_k)\\) (예: 기존 데이터셋 지형들을 일정 수준 노이즈 넣은 분포)로부터 샘플링된 후보들 중 return(정책 향상)이 높은 쪽에 가중치를 실어주는 형태로 해석할 수 있고, 그 결과 중요도 샘플링(importance sampling)을 통해 최적 노이즈의 기대값을 구하면 바로 앞서 사용한 가중치 평균 식으로 수렴합니다. 제어-추론 관점에서 보면, 높은 보상을 줄 환경을 샘플링하는 확률분포의 기대값을 계산한 것과 같으며, 이것이 이론적으로 정당화된다는 것이죠.\n정리하면, ADTG의 기법은 처음부터 끝까지 정책의 성능 신장이라는 목적을 갖고 설계되었고, 이를 확률적 생성 모델(Diffusion)의 제어 변수(초기 노이즈)를 학습자(정책)의 상태에 맞춰 조절함으로써 구현한 것이라 볼 수 있습니다. 이제 이러한 ADTG가 강화학습 커리큘럼과 어떻게 맞물려 돌아가는지 알아보겠습니다."
  },
  {
    "objectID": "post/paper/2025-04-13-diffusion-map.html#커리큘럼-강화학습과-adtg의-연계",
    "href": "post/paper/2025-04-13-diffusion-map.html#커리큘럼-강화학습과-adtg의-연계",
    "title": "Diffusion 아이디어",
    "section": "커리큘럼 강화학습과 ADTG의 연계",
    "text": "커리큘럼 강화학습과 ADTG의 연계\nADTG는 본질적으로 자동 커리큘럼을 구성하기 위한 환경 생성기로 설계되었습니다. 알고리즘의 작동 형태는 다음과 같습니다: 우선 초기에는 일부 기본 환경(elevation map)을 가지고 정책을 훈련하다가, 일정 주기마다 (혹은 병렬 학습 과정에서 지속적으로) 현재 정책에 맞는 새 지형을 생성하여 데이터셋에 추가합니다. 특히 매 반복(iteration)마다 현재 정책에게 가장 효과적인 학습 신호를 줄 환경을 선택하여 그 환경에서 정책을 조금 더 학습시킨 후, ADTG를 통해 새로운 환경을 만들어내는 식으로 진행됩니다. 이때 환경 선택은 앞서 정의한 가중치 함수 \\(w(e,\\pi)\\)를 활용해 현재 가장 적절한 난이도의 지형을 뽑습니다. 반드시 최고 가중치 하나만 고르는 대신, 확률적으로 샘플링하여 편중되지 않게 다양한 환경을 경험하도록 할 수도 있습니다.\n\n\n\n7\n\n\n\n이러한 절차를 통해 정책과 환경이 함께 공진화(co-evolve)하게 됩니다.\n\n정책이 발전하면 새로운 환경이 추가되고, 그 환경에서 다시 정책을 훈련하며, 시간이 지날수록 데이터셋은 점점 다양하고 어려운 지형들을 포함하게 됩니다. 이는 사람의 학습 과정에서 난이도를 조금씩 높여가며 실력을 향상시키는 교육 커리큘럼과 유사합니다. ADTG는 수동으로 커리큘럼을 설계하지 않아도 정책의 성공률 등 성능 측정치에 따라 난이도를 자동 조절해주므로, 일종의 자율 교사 역할을 수행한다고 볼 수 있습니다. 특히 강화학습에서 중요한 exploration-exploitation balance를 유지하면서도, 너무 쉬운 환경에 안주하거나 너무 어려운 환경에서 학습이 실패하는 상황을 방지하는 장점이 있습니다."
  },
  {
    "objectID": "post/paper/2025-04-13-diffusion-map.html#네트워크-구조",
    "href": "post/paper/2025-04-13-diffusion-map.html#네트워크-구조",
    "title": "Diffusion 아이디어",
    "section": "네트워크 구조",
    "text": "네트워크 구조\n\n\n\n5\n\n\n특권(교사) 정책과 배포(학생) 정책 모두 인코더-디코더 아키텍처를 기반으로 합니다. 인코더는 지형의 고도 또는 깊이 이미지를 입력으로 받아 특징 표현을 추출하며, 이를 위해 ResNet-18의 초기 16개 계층을 활용합니다.\n배포 정책은 선형 및 각도 명령 생성을 위해 신경 회로 정책(NCP), 그중에서도 폐쇄형 연속 시간(CfC) 네트워크를 사용합니다. Vanilla RNN, LSTM, GRU, NCP 모두 충분한 학습 단계를 거치면 유사한 정확도를 달성할 수 있지만, NCP는 훨씬 적은 수의 매개변수로도 우수한 성능을 보여 최종적으로 선택되었습니다."
  },
  {
    "objectID": "post/paper/2025-04-13-diffusion-map.html#실험-셋팅",
    "href": "post/paper/2025-04-13-diffusion-map.html#실험-셋팅",
    "title": "Diffusion 아이디어",
    "section": "실험 셋팅",
    "text": "실험 셋팅\nVelodyne-16 LiDAR, RealSense D435i 카메라 및 3DM-GX5-25 IMU가 장착되어 있습니다. 풀, 숲, 건조, 녹, 진흙, 자갈, 사막 하드, 사막 터프 및 사막 전문가 등 9개의 다양한 대표 환경에서 테스트합니다."
  },
  {
    "objectID": "post/paper/2025-04-13-diffusion-map.html#실험-결과",
    "href": "post/paper/2025-04-13-diffusion-map.html#실험-결과",
    "title": "Diffusion 아이디어",
    "section": "실험 결과",
    "text": "실험 결과\nADTG의 저자들은 이 방법을 wheeled robot인 Jackal 및 Husky 로봇의 주행 정책 학습에 적용하여, 시뮬레이션과 실제 야외 환경 실험을 모두 진행했습니다. 30개의 다양하고 거친 지형을 준비해 여러 최신 강화학습 및 플래너 기반 방법들과 비교한 결과, ADTG로 학습한 정책이 성공률, 주행 안정성 등 여러 지표에서 가장 우수한 성능을 보였습니다. 특히 ADTG는 학습 속도도 빨라서, 복잡한 지형에 대한 정책 수렴 속도가 기존 커리큘럼 방식들보다 향상되었음을 확인했습니다.\n더 놀라운 것은 sim-to-real 실험 결과로, 시뮬레이션에서 한 번도 보지 못한 새로운 실제 환경들(예: 숲, 진흙, 자갈, 모래 언덕 등 총 9종의 지형)에서 무학습 전이(Zero-shot)로 평가를 진행했는데도, ADTG로 훈련된 정책이 대부분의 환경에서 안정적으로 로봇을 주행시켰습니다 rics)). 반면, 고정된 절차적 생성이나 한정된 자연 지형 데이터로만 학습한 정책은 일부 현실 환경에서 거의 움직이지 못하거나 전복되는 등 일반화 성능이 떨어지는 모습을 보였습니다. 이 실험을 통해 ADTG의 적응형 생성이 시뮬레이션과 현실 사이의 갭을 줄여주고, 견고한 정책을 만들어내는 데 효과적임이 입증되었습니다.\n\n\n\n예시: 비교적 완만한 지형 (정책에게 쉬운 환경에 해당)\n\n\n\n\n\n예시: 기복이 심한 거친 지형 (정책에게 어려운 환경에 해당)\n\n\n위 이미지는 ADTG 저자들이 제시한 예시로, 좌측은 쉬운 환경의 지형 예시이고 우측은 어려운 환경 지형입니다. ADTG는 이러한 쉬운/어려운 환경 정보를 반영하여 중간 정도 난이도의 새로운 지형을 만들어냅니다. 또한 노이즈 수준 \\(k\\)를 조절함으로써, 필요할 경우 오른쪽 그림처럼 더 복잡하고 새로운 패턴의 지형도 생성해낼 수 있습니다. 이는 정책이 학습 초기에 단조로운 지형만 경험하는 것을 피하게 해주고, 후반에는 그동안 보지 못한 형태의 지형도 만나게 함으로써 예기치 않은 상황에 대한 대처 능력을 높여줍니다."
  },
  {
    "objectID": "post/paper/2025-04-13-diffusion-map.html#족-보행-로봇-실험",
    "href": "post/paper/2025-04-13-diffusion-map.html#족-보행-로봇-실험",
    "title": "Diffusion 아이디어",
    "section": "4족 보행 로봇 실험",
    "text": "4족 보행 로봇 실험\n실제 4족 보행 로봇 실험 환경에서는 지형의 크기가 [128 × 128], 수평 해상도는 0.1m입니다. 미끄러운 표면, 경사진 지형, 예측 불가능한 협곡 등 다양한 도전 과제를 포함한 총 9개의 환경이 사용되었습니다.\nADTG의 일반화 능력이 다양한 지형 조건에서도 유지되는지를 검증하기 위해, Jackal과 동일한 환경에서 Unitree Go1 4족 보행 로봇을 이용해 PGC 및 내장 MPC 컨트롤러와의 벤치마크 실험을 수행했습니다. ADTG와 PGC 정책은 모두 Parkour [66]의 “걷기” 정책을 기반으로 훈련되었습니다.\n하이킹 실험 [5, 13]과 유사하게, 공정성을 위해 로봇은 MPC가 생성한 발자국 경로를 따라 총 1.2km에 달하는 루프를 연속 주행했으며, 경로 내 어려운 구간을 우회할 수 없는 경우 전복을 실패로 간주했습니다.\n실험 결과, MPC는 총 6회 전복되었고, 이 중 3회는 고속 명령 수행 중, 나머지 3회는 자갈 및 모래 지형에서 발생했습니다. ADTG 정책은 고속 명령에 의한 2회의 실패가 있었으나 전반적으로 자연스럽고 안정적인 보행 자세를 유지했습니다. 반면, PGC 정책은 강한 수평 자세를 보였음에도 불구하고, 전 지형에서 발생한 갑작스러운 점프 동작으로 인해 총 11회의 전복이 발생했습니다.\n이 같은 결과는 ADTG 정책이 다양한 환경에서 보다 안정적이고 일반화된 보행 행동을 성공적으로 학습했음을 보여줍니다. PGC 정책의 불안정성은 과도하게 도전적인, 비현실적인 지형 조건에서의 훈련으로 인해 유도되었을 가능성이 있습니다.\nADTG와 내장된 Go1의 MPC 보행 제어는 자연스러운 동작을 보였으며, 프로시저 생성 커리큘럼(PGC) 기반 정책에 비해 더 안정적인 자세를 유지했습니다. 반면, PGC 기반 정책은 전반적으로 양호하게 작동했지만, 때때로 갑작스러운 점프 동작으로 인해 로봇이 뒤집히는 문제가 발생했습니다. 이러한 문제는 PGC가 지나치게 도전적인, 비현실적인 지형에서 훈련되었기 때문일 수 있습니다.\n\n\n\n6"
  },
  {
    "objectID": "post/paper/2025-04-13-diffusion-map.html#디퓨전-기반-지형-생성의-장점과-향후-전망",
    "href": "post/paper/2025-04-13-diffusion-map.html#디퓨전-기반-지형-생성의-장점과-향후-전망",
    "title": "Diffusion 아이디어",
    "section": "디퓨전 기반 지형 생성의 장점과 향후 전망",
    "text": "디퓨전 기반 지형 생성의 장점과 향후 전망\nADTG를 통해 확인된 가장 큰 성과는, 딥러닝 기반 생성모델(디퓨전)을 활용하여 강화학습 환경을 더 똑똑하게 만들 수 있다는 점입니다. 전통적인 절차적 생성이나 무작위 환경 샘플링과 달리, 디퓨전 모델은 실제 지형 데이터를 학습하여 얻은 복잡한 분포에서 샘플을 뽑아내기 때문에 현실성과 다양성 측면에서 월등합니다. 예를 들어, 바위가 흩어진 경사면이나 물이 고인 진흙탕처럼 사람이 일일이 매개변수로 규정하기 어려운 지형도 디퓨전 모델은 데이터만 주어지면 생성해낼 수 있습니다. ADTG는 여기에 한 걸음 더 나아가 현재 정책에 유용한 방향으로 생성과정을 제어했기 때문에, 필요하지 않은 극단적으로 복잡한 지형을 억제하고 학습 효율을 극대화할 수 있었습니다 ted)) 향후 강화학습과 생성 AI의 접목이 얼마나 큰 잠재력을 갖는지 보여주는 예시라고 할 수 있습니다.\n앞으로 이 분야에는 몇 가지 흥미로운 확장 가능성이 있습니다. 첫째, 본 논문에서는 지형의 높이맵(heightmap)에 대해서만 디퓨전 모델을 사용했지만, 이를 넘어 3차원 구조물이나 도시 환경 생성에도 비슷한 기법을 적용할 수 있을 것입니다. 예를 들어, 자율주행 자동차의 학습을 위해 도로 환경을 디퓨전 모델로 생성하고 난이도를 조절하는 식입니다. 둘째, 디퓨전 모델의 조건부 제어 능력을 활용하면 특정한 시나리오(예: “돌이 많은 지형” 혹은 “물이 있는 지형”)를 생성하도록 유도하는 것도 가능할 것입니다. 이는 단순히 난이도뿐 아니라 환경의 종류나 속성까지 커리큘럼에 넣을 수 있게 해줄 것입니다.\n요약하면, Adaptive Diffusion Terrain Generator (ADTG)는 강화학습 에이전트를 위한 환경을 디퓨전 생성모델로 자동 생성하고, 이를 정책의 학습 상태에 맞춰 조절함으로써, 일종의 자동 커리큘럼을 구현한 기법입니다. 기존의 제한적인 환경 생성 방식의 한계를 극복하고, 학습된 정책이 더 넓은 분포의 지형에 견고하게 일반화할 수 있도록 돕는 점이 인상적입니다. ADTG는 강화학습 연구자들에게 환경 설계에 대한 부담을 줄이는 동시에, 학습 효율과 성능을 모두 높일 수 있는 새로운 방향을 제시합니다."
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "Competition",
    "section": "",
    "text": "Archive Competition Materials\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n[icros2025] 2. 프로젝트 개념도\n\n\n\n\n\n\nCompetition\n\n\nlegged-robot\n\n\n\nicros2025 프로젝트 개념도\n\n\n\n\n\nMay 6, 2025\n\n\nJinwon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n[icros2025] 1. 4족보행 대회 소개\n\n\n\n\n\n\nCompetition\n\n\nlegged-robot\n\n\n\nicros2025 4족보행 대회 소개\n\n\n\n\n\nMay 6, 2025\n\n\nJinwon Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Post",
    "section": "",
    "text": "Archive Study Materials\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n[IsaacLab] 03. Task 설계 구조의 이해 part2\n\n\n\n\n\n\nIsaacLab\n\n\nRL\n\n\nTask Design\n\n\n\nIsaacLab 학습 프레임워크 구조 이해\n\n\n\n\n\nApr 19, 2025\n\n\nJinwon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n[IsaacLab] 03. Task 설계 구조의 이해 part1\n\n\n\n\n\n\nIsaacLab\n\n\nRL\n\n\nTask Design\n\n\n\n예제 코드를 통한 구현 방법의 차이 이해\n\n\n\n\n\nApr 19, 2025\n\n\nJinwon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n[IsaacLab] 02. 개발 세팅 및 예제 실행\n\n\n\n\n\n\nIsaacLab\n\n\nDocker\n\n\nSetting\n\n\nRL\n\n\nvscode\n\n\n\nvscode 환경 설정 및 학습 예제 실행\n\n\n\n\n\nApr 19, 2025\n\n\nJinwon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n[IsaacLab] 01. 환경 설정\n\n\n\n\n\n\nIsaacLab\n\n\nDocker\n\n\nSetting\n\n\nRL\n\n\nWebRTC\n\n\n\nDocker를 활용한 IsaacLab 환경 설정\n\n\n\n\n\nApr 19, 2025\n\n\nJinwon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nDiffusion 아이디어\n\n\n\n\n\n\ndiffusion\n\n\n\nAdaptive Diffusion Terrain Generator for Autonomous Uneven Terrain Navigation\n\n\n\n\n\nApr 13, 2025\n\n\nJungyeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\n250329 회의록\n\n\n\n\n\n\nmeeting\n\n\n\n25.03.29일자 Discussion 기록\n\n\n\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLearning robust perceptive locomotion for quadrupedal robots in the wild\n\n\n\n\n\n\nreview\n\n\n\ntbai 2nd paper review\n\n\n\n\n\nMar 2, 2025\n\n\nJungyeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\nHow to read papers\n\n\n\n\n\n\netc\n\n\n\n논문 읽는 법에 관한 영상 요약\n\n\n\n\n\nMar 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[5] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nImpedance Matching/Combining Legged and Manipulator/Foundation Models\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[4] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nSmall Home Robot/MI-HGNN\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[3] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nHybrid Internal Model/Locomotion over Challenging Terrain/Extreme Parkour/Differentiable Simulation\n\n\n\n\n\nFeb 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[2] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nActor-Cross-Critic/ANYmal Parkour/Stiffness Tuning/Privileged Information\n\n\n\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nDreamFLEX/RL-augmented MPC/CAIMAN/Safe RL/MPC Testbed\n\n\n\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear MPC\n\n\n\n\n\n\nmpc\n\n\npaper\n\n\n\nBasics of Linear MPC Controller for Quadruped Walking Robots\n\n\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beyond Steps RL",
    "section": "",
    "text": "Beyond Steps RL symbolizes the ambition to go beyond conventional boundaries in reinforcement learning (RL). The name reflects the integration of cutting-edge RL techniques with robotic locomotion, especially focusing on quadrupedal robots. It emphasizes innovation, exploration, and the pursuit of advancements that push RL applications beyond mere movement—toward solving real-world challenges with precision and adaptability.\n\n\nStudy Records\n\n\n\nNo.\nPaper\nPresenter\nDate\nFile\n\n\n\n\n1\nNot Only Rewards but Also Constraints\nJihong Kim\n24.09.01\npaper_review\n\n\n2\nAgile But Safe: Learning Collision-Free High-Speed Legged Locomotion\nJungYeon Lee\n24.09.01\npaper_review\n\n\n3\nSpinning Up in Deep RL\nChanwoo Park\n24.09.22\npaper_review\n\n\n4\nNot Only Rewards but Also Constraints II\nJehee Lee\n24.10.06\npaper_review\n\n\n5\nConstrained Policy Optimization\nJinwon Kim\n24.10.06\npaper_review\n\n\n6\nIPO: Interior-point Policy Optimization under Constraints\nJungYeon Lee\n24.10.06\npaper_review\n\n\n7\nTRPO/PPO\nChanwoo Park\n24.10.20\npaper_review\n\n\n8\nConstrained Policy Optimization II\nJihong Kim\n24.10.20\npaper_review\n\n\n9\nLearning-based legged locomotion; state of the art and future perspectives\nJinwon Kim\n24.11.17\npaper_review\n\n\n10\npympc-quadruped\nJihong Kim\n25.01.05\ncode_review\n\n\n11\nModel Predictive Control\nJungYeon Lee\n25.01.05\npaper_review\n\n\n\n\n\nMembers\n\n\n\n\n\n\n\n\n\n\n\nJungYeon Lee\nJihong Kim\nJinwon Kim\nChanwoo Park"
  },
  {
    "objectID": "post/paper/2025-02-18-papers.html",
    "href": "post/paper/2025-02-18-papers.html",
    "title": "[1] 3mins papers",
    "section": "",
    "text": "DreamFLEX\n\nLearning Fault-Aware Quadrupedal Locomotion Controller for Anomaly Situation in Rough Terrains\n\n\n로봇이 실제 오래 구동했을 시에 마주칠 수 있는 하드웨어 결함에 대응하는 방법론, joint의 결함이 생기면 이를 감지하여 결함이 있는 상황에 맞춰서 보행 패턴을 재구성\nDreamWaQ의 컨셉을 유지하며 CENet을 FEMNet으로 바꾸어 조인트 결함을 감지하는 것과 연관있는 term \\(f_t\\) 추가\nJoint Fault의 상황은 locked joint와weakened motor` 2가지 경우에 대해 상황을 정의하여 결함이 있는 발을 제외하고서라도 보행하는 것이 목적\nPaper Link\n\n\n\n\nRL-augmented MPC\n\nLearning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC\n\n\nRL을 보완하는 MPC 프레임워크 개발 고속 이동, 모델 불확실성 적응, 장애물 회피를 포함하는 적응형 블라인드(blind) 보행을 실현\n스탠스 발 힘 제어(stance foot force control)와 스윙 발 반사(swing foot reflection)를 결합 기존 MPC가 직면한 스윙 발 궤적의 불확실성 문제를 해결 모델 불확실성(model uncertainty)에 적응하면서 로봇의 균형 유지 능력을 개선\n로봇-독립적(robot-agnostic) RL 모듈 도입 다양한 로봇 플랫폼(Unitree A1, Go1, AlienGo)에서 zero-shot transfer 가능 RL 정책이 특정 로봇에 최적화되지 않고 다양한 환경에서도 일반화 가능\nstance phase와 swing phase를 분리하는 것은 당연한 전제로 생각했었는데, 이를 혼합하려는 시도는 신선하게 느껴짐\nPaper Link\n\n\n\n\nCAIMAN\n\nCausal Action Influence Detection for Sample Efficient Loco-manipulation\n\n\n4족보행 로봇이 크고 무거운 물체를 조작하며 이동할 수 있도록 비-파지(non-prehensile) 방식으로 크고 무거운 물체를 조작하며 이동(loco-manipulation)하는 Task를 학습\ncausal action influence(인과적 행동 영향, CAI)을 활용하여 로봇이 환경을 제어하는 상태를 탐지하고, 이를 내재적 동기(intrinsically motivated objective)로 삼아 hierarchical control strategy을 통해 학습\n물체를 미는 물리적 결과에 대한 동역학 모델을 학습하여 CAI 값을 끌어낸다는 점이 특이해서 흥미로웠으며, 최근에 Loco-Manipulation이라고 하면 보통 매니퓰레이터를 간단한거라도 다는데 여기에서는 그냥 밀기였음\nPaper Link\n\n\n\n\nSafe RL/MPC Testbed\n\n로봇의 변화 가능한 환경에서의 MPC, RL의 안정성 평가 Testbad 정의\n\n\nSafeRL/MPC라고 하면, 어떤 환경에서 안정성을 평가하는 것이 합리적인가\n로봇에 요구되는 Task가 다양해짐에 따라 평가할 수 있는 적절한 Testbed 및 기준 필요\n환경변화로 “로봇 외부의 환경 변화”, “범용적 임무 수행을 위한 로봇의 동작으로 인해 발생된 환경 변화”, “로봇의 조정 가능한 HW, 부착물의 위치, 구성으로 인해 발생된 환경 변화”를 생각해볼 수 있음"
  },
  {
    "objectID": "post/paper/2025-02-28-papers.html",
    "href": "post/paper/2025-02-28-papers.html",
    "title": "[5] 3mins papers",
    "section": "",
    "text": "Impedance Matching\n\nEnabling an RL-Based Running Jump in a Quadruped Robot\n\n\n강화학습(Reinforcement Learning, RL)을 활용하여 사족보행 로봇의 점프 동작을 구현하는 동시에, 시뮬레이션-현실 간의 성능 차이(sim-to-real gap)를 줄이는 새로운 기법을 제안함. 이를 위해서 주파수 영역에서의 임피던스 매칭 기법을 활용하였음\n기존의 연구는 Domain gap, 무작위의 Domain randomization, 하나의 정책(점프 등)만을 학습, model-based의 reference motion을 참조 등이 필요했으나 본 연구에서는 임피던스 매칭 기법을 통해 Domain gap을 줄이고 reference없이 다양한 모션들을 수행할 수 있는 학습 방법을 제안함\n지금까지의 연구에서는 점프 모션의 경우, model-based controller나 reference를 활용학습하는 것이 일반적이었으나, 본 논문은 순수 강화학습만으로 55cm의 거리를 점프하는 모습을 보임. 매우 흥미로워서 좀 더 깊이 파보고자 함.\nPaper Link\n\n\n\n\nCombining Learning-based Locomotion Policy with Model-based Manipulation for Legged Mobile Manipulators\n3줄요약\n\nMPC를 사용하는 매니퓰레이터와 RL을 사용하는 4족로봇의 통합\n매니퓰레이터가 움직이면, 4족로봇은 그 외란을 예측해서 로봇이 균형을 유지하고 안정적으로 이동할 수 있게 함.\nRL은 매니퓰레이터의 동역학 모델을 직접적으로 사용하지 않음. 시뮬레이션에서 외란을 가해준다음에 MSE를 통해 예측모델을 만드는것임.\n\nTraining in Simulation - 로봇은 시뮬레이션 환경에서 무작위 힘의 조합인 wrench sequence를 적용받음. 학습과정에서 이 렌치 시퀀스 예측을 수행함. - 학습중에는 별도의 매니퓰레이터 제어기가 필요하지 않음. legged gym에서 push robot과 같은 함수를 지속적으로 가해주는것과 동일함.\nResults - 매니퓰레이터의 다양한 동작과 무게 변화에 대한 적응성이 생김. - 제어 시스템을 4족로봇, 매니퓰레이터 컨트롤러 두개의 모듈로 분리하여 각 모듈의 독립적인 개발 및 최적화를 가능하게 함. - 분명 이전에 이거랑 같은 컨셉인 시뮬레이션 논문이 아카이브에 올라왔었는데 지금은 못찾겠음. 확실히 직관적인 아이디어여도 실제 하드웨어로 실험을 해야 논문이 되는것같다. - Paper Link\n \n\n\nFoundation Models in Robotics\n\nApplications, Challenges, and the Future\n\n\n강화학습 + MPC 주제쪽으로 보고 있지만 이번에는 관심을 가지고 있는 Foundation model에 대해서 가지고 와봤다. 본 논문은 Foundation model이 로보틱스에서 어떤식으로 활용되어 왔고, 어떻게 연구가 될지에 대해서 논의하는 논문이다. Foundation model을 통해 로봇의 인지, 의사결정, 제어의 능력을 향상시키는 방법을 분석한다.\n특히, 기초 모델을 활용한 로봇 정책 학습과 강화학습의 접점으로 Language-Assisted Reinforcement Learning (LLM 기반 보상 학습 및 탐색), Vision-Language Value Learning (VLM을 활용한 Value Function 학습), Robotics Transformers (Transformer 기반 행동 정책 학습) 등이 제시되었으며, 이는 강화학습 기반의 제어기 설계에 유용할 수 있다.\n다만, 실시간 응답 속도 문제, 데이터 부족, 불확실성 정량화 부족 등의 한계가 존재하며, RL과 MPC를 결합한 제어기 학습을 위한 foundation model의 직접적인 적용 사례는 부족하다. 향후 연구 방향으로 RL과 결합 가능한 경량화된 foundation model 개발 및 로봇 제어 특화된 데이터셋 구축이 필요할 것으로 보인다.\nPaper Link\nYoutube"
  },
  {
    "objectID": "post/paper/2025-02-24-papers.html",
    "href": "post/paper/2025-02-24-papers.html",
    "title": "[4] 3mins papers",
    "section": "",
    "text": "RLOC: Terrain-Aware Legged Locomotion using Reinforcement Learning and Optimal Control\n\n2020년 논문이지만, 계속해서 Privileged Information 컨셉을 유지하면서, 관련 논문을 찾고 있습니다. - 이 논문은 강화학습(RL)과 모델 기반 제어를 통합하여 사족보행 로봇이 다양한 지형에서 동적 보행을 수행할 수 있도록 하는 프레임워크를 제안합니다. 이 접근법은 온보드 고유수용성(proprioceptive) 및 외부수용성(exteroceptive) 피드백을 활용하여 센서 정보와 원하는 속도 명령을 발걸음 계획으로 매핑하는 RL 정책을 학습합니다. 학습된 정책은 모델 기반 모션 컨트롤러와 결합되어 복잡한 지형에서도 안정적인 보행을 구현합니다. 또한, 신체 전체의 움직임 추적 및 회복 제어를 위한 보조 RL 정책을 도입하여 물리적 파라미터의 변화와 외부 교란에 대응합니다. 이 프레임워크는 ANYmal B 및 ANYmal C 로봇 플랫폼에서 재학습 없이도 성공적으로 적용되었습니다.\n\n\n이 논문은 “Provable Partially Observable Reinforcement Learning with Privileged Information” 연구와 연결 지어 해석할 수 있습니다. 특히, 시뮬레이션에서 특권 정보(privileged information)를 활용하여 RL 정책을 학습하고, 실제 환경에서는 제한된 센서 정보만으로도 안정적인 보행을 구현하는 접근법은 두 연구의 공통된 특징입니다. 이는 특권 정보의 활용이 결정론적 필터 조건(deterministic filter condition)을 만족하지 않더라도, 적절한 제약 조건을 통해 안전한 강화학습 기반 보행 컨트롤러를 설계할 수 있음을 시사합니다.\n이러한 통찰을 바탕으로, 특권 정보를 활용한 교사-학생 학습(teaching-student learning)과 비대칭 액터-크리틱(asymmetric actor-critic) 방법을 통해 사족보행 로봇의 복잡한 지형에서의 안전한 보행을 구현할 수 있습니다. 또한, 적절한 제약 조건을 도입하여 강화학습 기반 보행 컨트롤러의 안정성과 안전성을 향상시킬 수 있을 것으로 기대됩니다.\nPaper Link\nVideo\n\n\n\nSmall Home Robot\n\nLearning Quiet Walking for a Small Home Robot\n\n\n지금까지의 연구는 강화학습이 얼마나 제어기를 강건하게 만들었느냐, 혹은 얼마나 fancy한 움직임을 만들었느냐에 초점을 두어왔었음. 하지만, 본 논문은 실제 가정에 로봇이 투입이 되었을 때 불편함을 느낄 가능성이 높은 “소음”에 초점을 맞추어 연구를 진행했다. 이처럼, 기술적인 어려움보다도 실제 삶에 있어서 도움이 될만한 기술을 연구하는 것 또한 좋은 방향으로 보인다.\n기술적인 어려움은 크게 없으며 curriculum learning을 통해 1st phase에서는 소음을 신경쓰지 않는 모션을 학습하고, 2nd phase에서 조용한 모션을 학습하도록 만든다. 네트워크의 output은 “PD Gain의 Scale”과 “ Target Joints Position”을 내보내 stiff한 모션을 만들지 damping 있는 모션을 만들지 조정한다.\n본 실험(slope 극복실험)에 따르면 제어기의 강건도와 조용함이 반비례 관계임을 확인할 수 있었다. 미래에는 perceptive sensor를 통해 좀 더 고도화된 보행전략을 취하도록 만들 수 있을 것이다. 또한, 발자국 사운드와 관련이있는 발 접촉 속도를 최소화하는 간접 reward를 줌으로써 문제를 해결했는데 이는 토크를 줄이므로써 배터리 효율성을 증가시키는 방식 등으로 활용이 가능할 것이다.\nPaper Link\nYoutube\n\n\n\n\nMI-HGNN\n\nMorphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception\n\n\nMI-HGNN은 로봇의 관절과 링크를 각각 노드와 엣지로 모델링하여, 기존 모델 대비 8.4% 성능 향상과 탁월한 파라미터 효율성, 일반화 및 샘플 효율성을 달성함\n로봇의 형태학(morphology) 정보를 반영한 이종 그래프 신경망(Heterogeneous Graph Neural Network) 아키텍처에 모델 기반 제약을 통합하여, 실제와 시뮬레이션 데이터에서 접촉 인식 문제를 효과적으로 해결\n이전에 미니 프로젝트로 했었던 GNN 아이디어는 단순히 보행하는 rl policy에 간접적인 정보를 넣어주는 걸로 진행했었는데 이 논문에서 GNN은 보다 직접적으로 ground reaction forces (GRFs)를 예측하는 학습을 시켰다는 점이 새로웠으며 제어하는데 GRF 정보가 필요할 때 한 모듈로 이용해보면 좋겠다는 생각이 들었음\nPaper Link"
  },
  {
    "objectID": "post/meeting/2025-03-29-meeting-note.html",
    "href": "post/meeting/2025-03-29-meeting-note.html",
    "title": "250329 회의록",
    "section": "",
    "text": "Attendee: JungYeon Lee, Jihong Kim\n\nMain\n\n저번주 아이디어 디벨롭 부분 공유\n\ns_t로 얻어내는 policy를 활용할 수 있는 방안 찾기\n\n\nPPO 에서 importance sampling 하는 부분을 간단하게 policy 1, 2로 대체하는 건 어떨까?\n\n\\[ r_t = \\frac{\\pi(a_t \\mid s_t)}{\\pi_\\text{old}(a_t \\mid s_t)} \\]\n\\[ r_t = \\frac{\\pi_2(a_t \\mid s_t)}{\\pi_1(a_t \\mid s_t)} \\]\n\n\n\n\nNext\n\n도커로 IsaacSim/Lab 설치 진행(@ Jinwon Kim)\nquadruped_sim2sim(https://github.com/evronix/quadruped_sim2sim) 코드 공유 (@Jihong Kim)\nDiffusion paper 내용 공유\nOff-poilcy RL VS. Offline RL 비교 내용 공유 (@Chanwoo Park)\n\n\n\nAgenda\n\n모임 일정 조정 필요"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-setting.html",
    "href": "post/study/2025-04-19-IsaacLab-setting.html",
    "title": "[IsaacLab] 01. 환경 설정",
    "section": "",
    "text": "Main image"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-setting.html#사전-준비사항",
    "href": "post/study/2025-04-19-IsaacLab-setting.html#사전-준비사항",
    "title": "[IsaacLab] 01. 환경 설정",
    "section": "사전 준비사항",
    "text": "사전 준비사항\n\nDocker 및 NVIDIA Container Toolkit 설치\n\nDocker 설치 가이드\nNVIDIA Container Toolkit 설치 가이드\n\nIsaacSim WebRTC 설치\n\nIsaacSim WebRTC Streaming Client"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-setting.html#isaaclab-설치-방법",
    "href": "post/study/2025-04-19-IsaacLab-setting.html#isaaclab-설치-방법",
    "title": "[IsaacLab] 01. 환경 설정",
    "section": "IsaacLab 설치 방법",
    "text": "IsaacLab 설치 방법\nIsaacLab 설치는 Docker를 통해 세 가지 방법으로 가능합니다:\n\nIsaacSim Docker에서 직접 설치\nIsaacLab의 Docker-compose 파일 활용 (추천)\n사전 빌드된 IsaacLab Docker 이미지 활용\n\n각 방법을 자세히 설명하겠습니다.\n\n1. IsaacSim Docker에서 직접 설치\n\nIsaac-sim 컨테이너 생성\n메모리 누수 문제를 방지하기 위해 메모리와 CPU를 제한합니다. 컴퓨터 사양에 맞게 조절하세요.\ndocker run --name isaac-sim-4.5.0 --entrypoint bash -it --runtime=nvidia --gpus all \\\n    --memory=20g --cpus=12 \\\n    -e \"ACCEPT_EULA=Y\" --network=host --ipc=host --pid=host --privileged -e DISPLAY=$DISPLAY  \\\n    -e \"PRIVACY_CONSENT=Y\" \\\n    -v ~/docker/isaac-sim/cache/kit:/isaac-sim/kit/cache:rw \\\n    -v ~/docker/isaac-sim/cache/ov:/root/.cache/ov:rw \\\n    -v ~/docker/isaac-sim/cache/pip:/root/.cache/pip:rw \\\n    -v ~/docker/isaac-sim/cache/glcache:/root/.cache/nvidia/GLCache:rw \\\n    -v ~/docker/isaac-sim/cache/computecache:/root/.nv/ComputeCache:rw \\\n    -v ~/docker/isaac-sim/logs:/root/.nvidia-omniverse/logs:rw \\\n    -v ~/docker/isaac-sim/data:/root/.local/share/ov/data:rw \\\n    -v ~/docker/isaac-sim/documents:/root/Documents:rw \\\n    nvcr.io/nvidia/isaac-sim:4.5.0\n컨테이너 내부에서 IsaacSim 실행\n\n아래 환경변수 설정하면 알아서 webrtc 모드로 사용하게 만들어줍니다. export LIVESTREAM=2\n\n./runheadless.sh -v\nROS 2 활용\ndocker cotainer에서 ros2를 또 설치하는 것은 꽤나 귀찮은 일입니다. 이 때문에 isaac에서 기본적으로 지원해주는 ROS 2 라이브러리를 사용하고자 합니다. 이를 위해서는 아래의 환경변수를 추가해줘야합니다.\nexport ISAACSIM_PATH=/isaac-sim\n\nexport RMW_IMPLEMENTATION=rmw_fastrtps_cpp\n\n# Can only be set once per terminal.\n# Setting this command multiple times will append the internal library path again potentially leading to conflicts\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ISAACSIM_PATH/exts/isaacsim.ros2.bridge/humble/lib\nIsaacLab 설치\nexport ISAACSIM_PYTHON_EXE=\"${ISAACSIM_PATH}/python.sh\"\ncd /\napt-get update && apt-get install git cmake build-essential\ngit clone https://github.com/isaac-sim/IsaacLab.git -b v2.0.2\ncd IsaacLab\nln -s ${ISAACSIM_PATH} _isaac_sim\n/IsaacLab/_isaac_sim/kit/python/bin/python3 -m pip install --upgrade pip\n./isaaclab.sh --install\n요약\n\n위 과정을 자동화한 스크립트를 setup.bash로 저장하여 실행할 수 있습니다.\n#!/bin/bash\nBASHRC=\"/root/.bashrc\"\necho 'export RMW_IMPLEMENTATION=rmw_fastrtps_cpp' &gt;&gt; $BASHRC\necho 'export ROS_DOMAIN_ID=26' &gt;&gt; $BASHRC\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ISAACSIM_PATH/exts/isaacsim.ros2.bridge/humble/lib' &gt;&gt; $BASHRC\necho 'export ISAACSIM_PYTHON_EXE=\"${ISAACSIM_PATH}/python.sh\"' &gt;&gt; $BASHRC\necho 'export LIVESTREAM=2' &gt;&gt; $BASHRC\nsource $BASHRC\napt-get update && apt-get install -y git cmake build-essential\ngit clone https://github.com/isaac-sim/IsaacLab.git -b v2.0.2\ncd /IsaacLab && ln -s ${ISAACSIM_PATH} _isaac_sim\n${ISAACSIM_PATH}/kit/python/bin/python3 -m pip install --upgrade pip\ncd /IsaacLab && ./isaaclab.sh --install\n\n\n\n\n2. IsaacLab의 Docker-compose 파일 활용\n\nIsaacLab repo 설치 (version 2.0.2 사용)\ngit clone &lt;https://github.com/isaac-sim/IsaacLab.git&gt; -b v2.0.2\nWebRTC 환경변수 설정\ndocker/.env.base에 IP와 livestream 설정을 추가합니다.\n# webrtc streaming variable\nLIVESTREAM=2\n\n# public ip address\nPUBLIC_IP=${송출하고 싶은 IP 주소}\n원리 설명…\n아래 코드를 보면, 초기에 환경변수를 가져오고, 이를 LIVESTREAM=2일 때 활용해서, 송출을 시작한다.\n코드 참조 링크 1\n코드 참조 링크 2\n참고: 포트는 아래와 같이 사용한다. 즉 아래의 포트를 열어놔야 외부에서 접근이 가능하다.\n\nThe following ports need to be opened: TCP/UDP 47995:48012, TCP/UDP 49000:49007, and TCP 49100.\n\n자원관리를 위한 설정\n자원관리를 위하여, 아래의 코드를 docker/docker-compose.yaml 에 추가해준다.\nservices:\n  isaac-lab-base:    \n    ...\n\n    mem_limit: 20g\n    cpus: 12\n\n    ...\n\n # This service adds a ROS2 Humble\n  # installation on top of the base image\n  isaac-lab-ros2:\n   ...\n\n    mem_limit: 20g\n    cpus: 12\n\n    ...\n도커 빌드 방법\n./docker/container.py start\n\n\n\n3. 사전 빌드된 IsaacLab Docker 이미지 활용\n사전 빌드된 컨테이너를 사용하여 로컬 빌드 없이 NGC에서 직접 가져올 수 있습니다. 이 컨테이너는 헤드리스 실행 전용입니다.\n\ndocker pull 명령어\ndocker pull nvcr.io/nvidia/isaac-lab:2.0.2\ndocker container 실행 방법\ndocker run --name isaac-lab --entrypoint bash -it --gpus all -e \"ACCEPT_EULA=Y\" --rm --network=host \\\n-e \"PRIVACY_CONSENT=Y\" \\\n-v ~/docker/isaac-sim/cache/kit:/isaac-sim/kit/cache:rw \\\n-v ~/docker/isaac-sim/cache/ov:/root/.cache/ov:rw \\\n-v ~/docker/isaac-sim/cache/pip:/root/.cache/pip:rw \\\n-v ~/docker/isaac-sim/cache/glcache:/root/.cache/nvidia/GLCache:rw \\\n-v ~/docker/isaac-sim/cache/computecache:/root/.nv/ComputeCache:rw \\\n-v ~/docker/isaac-sim/logs:/root/.nvidia-omniverse/logs:rw \\\n-v ~/docker/isaac-sim/data:/root/.local/share/ov/data:rw \\\n-v ~/docker/isaac-sim/documents:/root/Documents:rw \\\nnvcr.io/nvidia/isaac-lab:2.0.2"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-setting.html#isaacsim-webrtc을-활용한-visualization",
    "href": "post/study/2025-04-19-IsaacLab-setting.html#isaacsim-webrtc을-활용한-visualization",
    "title": "[IsaacLab] 01. 환경 설정",
    "section": "IsaacSim WebRTC을 활용한 Visualization",
    "text": "IsaacSim WebRTC을 활용한 Visualization\n위 3가지 방법을 통하여 설치한 컨테이너에서 서버를 실행하게 되면, client에서는 이에 붙어 interaction을 수행할 수 있습니다.\n\nhost에서 webrtc client를 실행합니다. ​  ​ host에서 webrtc client를 실행합니다. 서버에서 실행한 IP로 변경 후 연결 버튼을 누릅니다. 서버와 클라이언트가 같은 곳에서 실행 중이라면 localhost인 127.0.0.1을 사용합니다.\n실행 화면 ​  ​ 위와 같이 client가 서버에 붙는 것을 확인할 수 있습니다. 이 때, server에서는 main: thread_init: already added for thread라는 warning이 뜨게 됩니다.\n\n버그: 현재 외부 인터넷을 통해서는 webrtc로 접속을 못하는 문제가 있음. 향후 버전에서 수정될 예정이라고 함.\n\n\n\nBug report image"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-setting.html#reference",
    "href": "post/study/2025-04-19-IsaacLab-setting.html#reference",
    "title": "[IsaacLab] 01. 환경 설정",
    "section": "Reference",
    "text": "Reference\n\nIsaacLab Docker 설치 가이드\nIsaacSim WebRTC 설치 가이드"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part2.html",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part2.html",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part2",
    "section": "",
    "text": "본 문서에서는 direct 방식의 cartpole 환경과, manager 방식의 cartpole 환경을 비교해보고자 합니다.\n각 코드는 아래에서 확인이 가능합니다.\n\n/workspace/isaaclab/source/isaaclab_tasks/isaaclab_tasks/direct/cartpole/cartpole_env.py\n/workspace/isaaclab/source/isaaclab_tasks/isaaclab_tasks/manager_based/classic/cartpole/cartpole_env_cfg.py"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part2.html#introduction",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part2.html#introduction",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part2",
    "section": "",
    "text": "본 문서에서는 direct 방식의 cartpole 환경과, manager 방식의 cartpole 환경을 비교해보고자 합니다.\n각 코드는 아래에서 확인이 가능합니다.\n\n/workspace/isaaclab/source/isaaclab_tasks/isaaclab_tasks/direct/cartpole/cartpole_env.py\n/workspace/isaaclab/source/isaaclab_tasks/isaaclab_tasks/manager_based/classic/cartpole/cartpole_env_cfg.py"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part2.html#코드-설명",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part2.html#코드-설명",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part2",
    "section": "코드 설명",
    "text": "코드 설명\n\nDirect 환경\nDirect 환경은 전통적인 환경 구현 방식과 유사하며, 하나의 클래스가 모든 구성 요소를 직접 구현합니다. 이는 구현의 투명성을 제공하며, 복잡한 로직 구현에 유리합니다. 또한, PyTorch JIT 또는 Warp와 같은 최적화 프레임워크를 활용해 성능상의 이점을 제공합니다.\n\n\n코드 보기-cartpole_env.py\n\n# Copyright (c) 2022-2025, The Isaac Lab Project Developers.\n# All rights reserved.\n#\n# SPDX-License-Identifier: BSD-3-Clause\n\nfrom __future__ import annotations\n\nimport math\nimport torch\nfrom collections.abc import Sequence\n\nfrom isaaclab_assets.robots.cartpole import CARTPOLE_CFG\n\nimport isaaclab.sim as sim_utils\nfrom isaaclab.assets import Articulation, ArticulationCfg\nfrom isaaclab.envs import DirectRLEnv, DirectRLEnvCfg\nfrom isaaclab.scene import InteractiveSceneCfg\nfrom isaaclab.sim import SimulationCfg\nfrom isaaclab.sim.spawners.from_files import GroundPlaneCfg, spawn_ground_plane\nfrom isaaclab.utils import configclass\nfrom isaaclab.utils.math import sample_uniform\n\n\n@configclass\nclass CartpoleEnvCfg(DirectRLEnvCfg):\n    # 환경 설정\n    decimation = 2\n    episode_length_s = 5.0\n    action_scale = 100.0  # [N]\n    action_space = 1\n    observation_space = 4\n    state_space = 0\n\n    # 시뮬레이션 설정\n    sim: SimulationCfg = SimulationCfg(dt=1 / 120, render_interval=decimation)\n\n    # 로봇 설정\n    robot_cfg: ArticulationCfg = CARTPOLE_CFG.replace(prim_path=\"/World/envs/env_.*/Robot\")\n    cart_dof_name = \"slider_to_cart\"\n    pole_dof_name = \"cart_to_pole\"\n\n    # 장면 설정\n    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=4096, env_spacing=4.0, replicate_physics=True)\n\n    # 리셋 설정\n    max_cart_pos = 3.0  # 카트가 이 위치를 초과하면 리셋됩니다 [m]\n    initial_pole_angle_range = [-0.25, 0.25]  # 리셋 시 폴 각도가 샘플링되는 범위 [rad]\n\n    # 보상 스케일\n    rew_scale_alive = 1.0\n    rew_scale_terminated = -2.0\n    rew_scale_pole_pos = -1.0\n    rew_scale_cart_vel = -0.01\n    rew_scale_pole_vel = -0.005\n\n\nclass CartpoleEnv(DirectRLEnv):\n    cfg: CartpoleEnvCfg\n\n    def __init__(self, cfg: CartpoleEnvCfg, render_mode: str | None = None, **kwargs):\n        super().__init__(cfg, render_mode, **kwargs)\n\n        self._cart_dof_idx, _ = self.cartpole.find_joints(self.cfg.cart_dof_name)\n        self._pole_dof_idx, _ = self.cartpole.find_joints(self.cfg.pole_dof_name)\n        self.action_scale = self.cfg.action_scale\n\n        self.joint_pos = self.cartpole.data.joint_pos\n        self.joint_vel = self.cartpole.data.joint_vel\n\n    def _setup_scene(self):\n        self.cartpole = Articulation(self.cfg.robot_cfg)\n        # 지면 추가\n        spawn_ground_plane(prim_path=\"/World/ground\", cfg=GroundPlaneCfg())\n        # 복제 및 복제\n        self.scene.clone_environments(copy_from_source=False)\n        # 장면에 관절 추가\n        self.scene.articulations[\"cartpole\"] = self.cartpole\n        # 조명 추가\n        light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))\n        light_cfg.func(\"/World/Light\", light_cfg)\n\n    def _get_observations(self) -&gt; dict:\n        obs = torch.cat(\n            (\n                self.joint_pos[:, self._pole_dof_idx[0]].unsqueeze(dim=1),\n                self.joint_vel[:, self._pole_dof_idx[0]].unsqueeze(dim=1),\n                self.joint_pos[:, self._cart_dof_idx[0]].unsqueeze(dim=1),\n                self.joint_vel[:, self._cart_dof_idx[0]].unsqueeze(dim=1),\n            ),\n            dim=-1,\n        )\n        observations = {\"policy\": obs}\n        return observations\n\n    def _get_rewards(self) -&gt; torch.Tensor:\n        total_reward = compute_rewards(\n            self.cfg.rew_scale_alive,\n            self.cfg.rew_scale_terminated,\n            self.cfg.rew_scale_pole_pos,\n            self.cfg.rew_scale_cart_vel,\n            self.cfg.rew_scale_pole_vel,\n            self.joint_pos[:, self._pole_dof_idx[0]],\n            self.joint_vel[:, self._pole_dof_idx[0]],\n            self.joint_pos[:, self._cart_dof_idx[0]],\n            self.joint_vel[:, self._cart_dof_idx[0]],\n            self.reset_terminated,\n        )\n        return total_reward\n\n\n\n\n\n\n\n\n구성 요소\n설명\n\n\n\n\nCartpoleEnvCfg\nDirect 환경의 설정을 정의하는 클래스입니다. 시뮬레이션의 decimation, episode 길이, 행동 스케일 등을 설정합니다.\n\n\nCartpoleEnv\nDirect 환경을 구현하는 클래스입니다. DirectRLEnv를 상속받아 환경의 초기화, 관찰, 보상 계산 등을 수행합니다.\n\n\n_setup_scene\n시뮬레이션 장면을 설정하는 함수입니다. 로봇과 환경을 초기화합니다.\n\n\n_get_observations\n에이전트의 관찰 데이터를 수집하여 반환하는 함수입니다. 관절 위치와 속도를 포함합니다.\n\n\n_get_rewards\n에이전트의 보상을 계산하는 함수입니다. 보상 계산 로직은 생략되어 있습니다.\n\n\n\n\n\nManager-based 환경\nManager-based 환경은 작업을 개별적으로 관리되는 구성 요소들로 분해하여 모듈화된 구현을 촉진합니다. 이는 협업에 효과적이며, 다양한 구성 요소를 손쉽게 교체할 수 있어 프로토타이핑과 실험에 유리합니다.\n\n\n코드 보기-cartpole_env_cfg.py\n\nimport math\n\nimport isaaclab.sim as sim_utils\nfrom isaaclab.assets import ArticulationCfg, AssetBaseCfg\nfrom isaaclab.envs import ManagerBasedRLEnvCfg\nfrom isaaclab.managers import EventTermCfg as EventTerm\nfrom isaaclab.managers import ObservationGroupCfg as ObsGroup\nfrom isaaclab.managers import ObservationTermCfg as ObsTerm\nfrom isaaclab.managers import RewardTermCfg as RewTerm\nfrom isaaclab.managers import SceneEntityCfg\nfrom isaaclab.managers import TerminationTermCfg as DoneTerm\nfrom isaaclab.scene import InteractiveSceneCfg\nfrom isaaclab.utils import configclass\n\nimport isaaclab_tasks.manager_based.classic.cartpole.mdp as mdp\n\n##\n# Pre-defined configs\n##\nfrom isaaclab_assets.robots.cartpole import CARTPOLE_CFG  # isort:skip\n\n\n##\n# Scene definition\n##\n\n\n@configclass\nclass CartpoleSceneCfg(InteractiveSceneCfg):\n    \"\"\"Configuration for a cart-pole scene.\"\"\"\n\n    # ground plane\n    ground = AssetBaseCfg(\n        prim_path=\"/World/ground\",\n        spawn=sim_utils.GroundPlaneCfg(size=(100.0, 100.0)),\n    )\n\n    # cartpole\n    robot: ArticulationCfg = CARTPOLE_CFG.replace(prim_path=\"{ENV_REGEX_NS}/Robot\")\n\n    # lights\n    dome_light = AssetBaseCfg(\n        prim_path=\"/World/DomeLight\",\n        spawn=sim_utils.DomeLightCfg(color=(0.9, 0.9, 0.9), intensity=500.0),\n    )\n\n\n##\n# MDP settings\n##\n\n\n@configclass\nclass ActionsCfg:\n    \"\"\"Action specifications for the MDP.\"\"\"\n\n    joint_effort = mdp.JointEffortActionCfg(asset_name=\"robot\", joint_names=[\"slider_to_cart\"], scale=100.0)\n\n\n@configclass\nclass ObservationsCfg:\n    \"\"\"Observation specifications for the MDP.\"\"\"\n\n    @configclass\n    class PolicyCfg(ObsGroup):\n        \"\"\"Observations for policy group.\"\"\"\n\n        # observation terms (order preserved)\n        joint_pos_rel = ObsTerm(func=mdp.joint_pos_rel)\n        joint_vel_rel = ObsTerm(func=mdp.joint_vel_rel)\n\n        def __post_init__(self) -&gt; None:\n            self.enable_corruption = False\n            self.concatenate_terms = True\n\n    # observation groups\n    policy: PolicyCfg = PolicyCfg()\n\n\n@configclass\nclass EventCfg:\n    \"\"\"Configuration for events.\"\"\"\n\n    # reset\n    reset_cart_position = EventTerm(\n        func=mdp.reset_joints_by_offset,\n        mode=\"reset\",\n        params={\n            \"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"]),\n            \"position_range\": (-1.0, 1.0),\n            \"velocity_range\": (-0.5, 0.5),\n        },\n    )\n\n    reset_pole_position = EventTerm(\n        func=mdp.reset_joints_by_offset,\n        mode=\"reset\",\n        params={\n            \"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"]),\n            \"position_range\": (-0.25 * math.pi, 0.25 * math.pi),\n            \"velocity_range\": (-0.25 * math.pi, 0.25 * math.pi),\n        },\n    )\n\n\n@configclass\nclass RewardsCfg:\n    \"\"\"Reward terms for the MDP.\"\"\"\n\n    # (1) Constant running reward\n    alive = RewTerm(func=mdp.is_alive, weight=1.0)\n    # (2) Failure penalty\n    terminating = RewTerm(func=mdp.is_terminated, weight=-2.0)\n    # (3) Primary task: keep pole upright\n    pole_pos = RewTerm(\n        func=mdp.joint_pos_target_l2,\n        weight=-1.0,\n        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"]), \"target\": 0.0},\n    )\n    # (4) Shaping tasks: lower cart velocity\n    cart_vel = RewTerm(\n        func=mdp.joint_vel_l1,\n        weight=-0.01,\n        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"])},\n    )\n    # (5) Shaping tasks: lower pole angular velocity\n    pole_vel = RewTerm(\n        func=mdp.joint_vel_l1,\n        weight=-0.005,\n        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"])},\n    )\n\n\n@configclass\nclass TerminationsCfg:\n    \"\"\"Termination terms for the MDP.\"\"\"\n\n    # (1) Time out\n    time_out = DoneTerm(func=mdp.time_out, time_out=True)\n    # (2) Cart out of bounds\n    cart_out_of_bounds = DoneTerm(\n        func=mdp.joint_pos_out_of_manual_limit,\n        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"slider_to_cart\"]), \"bounds\": (-3.0, 3.0)},\n    )\n\n\n##\n# Environment configuration\n##\n\n\n@configclass\nclass CartpoleEnvCfg(ManagerBasedRLEnvCfg):\n    \"\"\"Configuration for the cartpole environment.\"\"\"\n\n    # Scene settings\n    scene: CartpoleSceneCfg = CartpoleSceneCfg(num_envs=4096, env_spacing=4.0)\n    # Basic settings\n    observations: ObservationsCfg = ObservationsCfg()\n    actions: ActionsCfg = ActionsCfg()\n    events: EventCfg = EventCfg()\n    # MDP settings\n    rewards: RewardsCfg = RewardsCfg()\n    terminations: TerminationsCfg = TerminationsCfg()\n\n    # Post initialization\n    def __post_init__(self) -&gt; None:\n        \"\"\"Post initialization.\"\"\"\n        # general settings\n        self.decimation = 2\n        self.episode_length_s = 5\n        # viewer settings\n        self.viewer.eye = (8.0, 0.0, 5.0)\n        # simulation settings\n        self.sim.dt = 1 / 120\n        self.sim.render_interval = self.decimation\n\n\n\n\n\n\n\n\n구성 요소\n설명\n\n\n\n\nCartpoleSceneCfg\n카트폴 장면을 설정하는 클래스입니다. 지면, 로봇, 조명 등의 설정을 포함합니다.\n\n\nActionsCfg\nMDP의 행동 사양을 정의하는 클래스입니다. 로봇의 조인트에 대한 힘을 설정합니다.\n\n\nObservationsCfg\nMDP의 관찰 사양을 정의하는 클래스입니다. 정책 그룹에 대한 관찰을 설정합니다.\n\n\nEventCfg\n이벤트 구성을 정의하는 클래스입니다. 카트와 폴의 위치를 초기화하는 이벤트를 설정합니다.\n\n\nRewardsCfg\nMDP의 보상 항목을 정의하는 클래스입니다. 다양한 보상 조건을 설정합니다.\n\n\nTerminationsCfg\nMDP의 종료 조건을 정의하는 클래스입니다. 시간 초과 및 카트의 경계 초과 조건을 설정합니다.\n\n\nCartpoleEnvCfg\n카트폴 환경의 구성을 정의하는 클래스입니다. 장면, 관찰, 행동, 이벤트, 보상, 종료 조건을 포함합니다."
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part2.html#최종-정리",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part2.html#최종-정리",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part2",
    "section": "최종 정리",
    "text": "최종 정리\nDirect vs Manager-based 비교\n\n\n\n\n\n\n\n\n특징\nDirect-based\nManager-based\n\n\n\n\n모듈화\n하나의 클래스가 모든 구성 요소를 직접 구현합니다.\n환경을 개별 구성 요소로 분해하여 모듈화된 구현을 촉진합니다.\n\n\n구성 요소 교체 용이성\n구성 요소가 하나의 클래스에 통합되어 있어 교체가 어렵습니다.\n다양한 구성 요소를 손쉽게 교체할 수 있어 프로토타이핑과 실험에 유리합니다.\n\n\n세밀한 제어\n환경의 로직에 대해 보다 세밀한 제어가 가능합니다.\n매니저를 통해 간접적으로 제어하므로 세밀한 제어가 제한적일 수 있습니다.\n\n\n협업\n협업 시 모든 구성 요소가 하나의 클래스에 포함되어 있어 분업이 어려울 수 있습니다.\n모듈화된 구조로 인해 협업에 효과적이며, 개별 개발자가 특정 측면에 집중할 수 있습니다.\n\n\n성능 최적화\nPyTorch JIT 또는 Warp와 같은 최적화 프레임워크를 활용해 성능상의 이점을 제공합니다.\n모듈화로 인해 성능 최적화가 제한적일 수 있습니다.\n\n\n투명성\n구현의 투명성을 제공합니다.\n매니저에 의해 로직이 추상화되어 있어 구현의 투명성이 낮을 수 있습니다.\n\n\n복잡한 로직 구현\n복잡한 로직 구현에 유리합니다.\n복잡한 로직을 구현하기에는 적합하지 않을 수 있습니다."
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part2.html#references",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part2.html#references",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part2",
    "section": "References",
    "text": "References\n\nTask Design Workflows\nIsaacSim\nIsaacLab"
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part1.html",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part1.html",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part1",
    "section": "",
    "text": "이 문서에서는 IsaacLab 학습 프레임워크 구조 이해를 위한 기본적인 내용을 설명합니다. 본 문서는 Task Design Workflows 문서를 참고하여 작성되었습니다."
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part1.html#introduction",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part1.html#introduction",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part1",
    "section": "Introduction",
    "text": "Introduction\n작업(Task)은 특정 에이전트(로봇)를 위한 관찰(observations)과 행동(actions)의 인터페이스를 가진 환경(environment)으로 정의됩니다. 환경은 에이전트에게 현재의 관찰 데이터를 제공하고, 에이전트의 행동을 실행시켜 시뮬레이션을 시간 순으로 진행합니다. 로봇이 어떤 작업을 수행하든, 또는 그 작업을 어떤 방식으로 학습하든 간에, 환경 내 로봇 시뮬레이션의 공통된 구성 요소들이 존재합니다.\n이는 특히 강화학습(Reinforcement Learning, RL)에서 더욱 그러합니다. 벡터화된 GPU 시뮬레이션에서 행동, 관찰, 보상 등을 관리하는 것은 생각만 해도 어려울 수 있습니다! 이러한 요구를 충족하기 위해 Isaac Lab은 Manager 기반 시스템 내에서 RL 환경을 구축할 수 있는 기능을 제공합니다. 이를 통해 다양한 세부 사항을 적절한 매니저 클래스에 위임할 수 있습니다. 하지만, 개발 중에는 환경에 대한 세밀한 제어가 필요한 경우도 있기 때문에, 시뮬레이션에 직접 접근(Direct interface) 하여 완전한 제어를 가능하게 하는 방식도 함께 제공합니다.\n\nManager 기반: 환경은 개별 구성 요소(또는 매니저)로 분해되며, 각 구성 요소는 관찰 계산, 행동 적용, 랜덤화 적용 등의 환경의 다른 측면을 담당합니다. 사용자는 각 구성 요소에 대한 설정 클래스를 정의하고, 환경은 매니저들을 조정하고 그들의 함수를 호출하는 역할을 합니다.\nDirect: 사용자는 단일 클래스를 정의하여 전체 환경을 직접 구현합니다. 별도의 매니저 없이 이 클래스는 관찰 계산, 행동 적용, 보상 계산을 직접 수행합니다.\n\n이 두 가지 워크플로우에는 각각 장단점이 있습니다.\nManager 기반 워크플로우는 모듈화가 잘 되어 있어, 환경의 다양한 구성 요소들을 손쉽게 교체할 수 있습니다. 이는 환경을 프로토타이핑하거나 다양한 설정을 실험할 때 유용합니다.\n반면, Direct 워크플로우는 더 효율적이며, 환경의 로직에 대해 보다 세밀한 제어가 가능합니다. 이는 성능 최적화가 필요하거나 복잡한 로직을 구현할 때 적합합니다."
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part1.html#manager-based-environments",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part1.html#manager-based-environments",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part1",
    "section": "Manager-Based Environments",
    "text": "Manager-Based Environments\n\n\n\nManager-based Task Workflow\n\n\n\nManager 기반 환경은 작업을 개별적으로 관리되는 구성 요소들로 분해하여 모듈화된 구현을 촉진합니다.\n각 작업의 구성 요소(예: 보상 계산, 관찰 계산 등)는 Manager에 대한 설정(configuration)으로 지정될 수 있습니다.\nManager들은 필요에 따라 특정 계산을 실행하는 설정 가능한 함수들을 정의합니다.\n서로 다른 Manager들을 조정하는 역할은 envs.ManagerBasedEnv 클래스를 상속한 환경 클래스가 담당합니다.\n설정 클래스들은 모두 envs.ManagerBasedEnvCfg를 상속해야 합니다.\n새로운 학습 환경을 개발할 때, 환경을 독립적인 구성 요소들로 분할하는 것이 유리합니다.\n이 방식은 협업에 효과적이며, 개별 개발자가 환경의 특정 측면에 집중할 수 있게 합니다.\n개발된 구성 요소들은 하나의 실행 가능한 작업으로 통합될 수 있습니다.\n예를 들어, 서로 다른 센서 구성을 가진 여러 로봇이 있을 경우, 각 로봇의 센서 데이터를 처리하는 관찰 매니저가 필요할 수 있습니다.\n팀원들이 보상 설계에 대해 다른 의견을 가질 경우, 각자 자신만의 보상 매니저를 개발하고 교체하거나 실험할 수 있습니다.\n\n이러한 매니저 기반 워크플로우의 모듈성은 복잡한 프로젝트에서 필수적입니다!\n강화학습의 경우, 대부분의 구성은 이미 제공되고 있습니다.\n일반적으로는, 환경을 envs.ManagerBasedRLEnv로부터 상속받고, 설정을 envs.ManagerBasedRLEnvCfg로부터 상속받아 작성하는 것만으로 충분합니다."
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part1.html#direct-environments",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part1.html#direct-environments",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part1",
    "section": "Direct Environments",
    "text": "Direct Environments\n\n\n\nDirect-based Task Workflow\n\n\n\nDirect 스타일 환경은 전통적인 환경 구현 방식과 유사합니다.\n하나의 클래스가 보상 함수, 관찰 함수, 재설정 로직 등 모든 구성 요소를 직접 구현합니다.\n매니저 클래스가 필요 없으며, envs.DirectRLEnv 또는 envs.DirectMARLEnv의 API를 통해 작업을 구현합니다.\n모든 Direct 작업 환경은 위 두 클래스 중 하나를 상속해야 합니다.\n설정 클래스도 필요하며, envs.DirectRLEnvCfg 또는 envs.DirectMARLEnvCfg를 상속해야 합니다.\nIsaacGymEnvs 및 OmniIsaacGymEnvs에서 이전하는 사용자들에게 익숙한 워크플로우입니다.\n환경 구현 로직이 매니저에 의해 추상화되지 않아 구현의 투명성을 제공합니다.\n복잡한 로직 구현이나 개별 구성 요소로 분해하기 어려운 경우에 유리합니다.\nPyTorch JIT 또는 Warp와 같은 최적화 프레임워크를 활용해 성능상의 이점을 제공합니다.\n대규모 학습 시 환경 내 개별 연산의 최적화가 필요한 경우 유용합니다."
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part1.html#정리-및-요약",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part1.html#정리-및-요약",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part1",
    "section": "정리 및 요약",
    "text": "정리 및 요약\n\n\n\n\n\n\n\n\n특징\nTask-based (Manager 기반)\nDirect-based\n\n\n\n\n🔄 모듈화\n⭕(환경을 각 구성 요소로 나눔)\n❌(하나의 클래스에 통합)\n\n\n🔧 구성 요소 교체 용이성\n⭕(교체 쉬움)\n❌(교체 어려움)\n\n\n🎛️ 세밀한 제어\n🔺(제한적)\n⭕(가능)\n\n\n🤝 협업\n⭕(협업 유리)\n❌(협업 불리)\n\n\n🚀 성능 최적화\n🔺(최적화 제한)\n⭕(JIT, Warp 등 최적화 유리)\n\n\n🔍 투명성\n🔺(투명성 낮음)\n⭕(투명성 제공)\n\n\n🧩 복잡한 로직 구현\n🔺(복잡한 로직 불리)\n⭕(복잡한 로직 유리)\n\n\n\n이 표는 각 워크플로우의 장단점을 비교하여, 사용자가 필요에 따라 적절한 방식을 선택할 수 있도록 돕습니다."
  },
  {
    "objectID": "post/study/2025-04-19-IsaacLab-task-design-part1.html#references",
    "href": "post/study/2025-04-19-IsaacLab-task-design-part1.html#references",
    "title": "[IsaacLab] 03. Task 설계 구조의 이해 part1",
    "section": "References",
    "text": "References\n\nTask Design Workflows\nIsaacSim\nIsaacLab"
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-intro.html",
    "href": "post/competition/icros2025/2025-05-06-icros2025-intro.html",
    "title": "[icros2025] 1. 4족보행 대회 소개",
    "section": "",
    "text": "포스터 사진\n\n\n이번 대회는 사족보행로봇의 자율주행 및 원격조종 부분의 성능을 겨루는 장으로, 참가자들은 주어진 규정에 따라 각자의 로봇을 설계하고 조종하게 됩니다. 저희 팀은 “원격 조종” 파트에 참가하여, 직접 조종하는 방식으로 미션을 수행할 예정입니다. 대회 주요 규정은 아래 표에 정리하였습니다."
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-intro.html#개요",
    "href": "post/competition/icros2025/2025-05-06-icros2025-intro.html#개요",
    "title": "[icros2025] 1. 4족보행 대회 소개",
    "section": "",
    "text": "포스터 사진\n\n\n이번 대회는 사족보행로봇의 자율주행 및 원격조종 부분의 성능을 겨루는 장으로, 참가자들은 주어진 규정에 따라 각자의 로봇을 설계하고 조종하게 됩니다. 저희 팀은 “원격 조종” 파트에 참가하여, 직접 조종하는 방식으로 미션을 수행할 예정입니다. 대회 주요 규정은 아래 표에 정리하였습니다."
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-intro.html#icros-2025-4족보행로봇-챌린지-원격-조종-부문-규정-요약",
    "href": "post/competition/icros2025/2025-05-06-icros2025-intro.html#icros-2025-4족보행로봇-챌린지-원격-조종-부문-규정-요약",
    "title": "[icros2025] 1. 4족보행 대회 소개",
    "section": "ICROS 2025 4족보행로봇 챌린지: 원격 조종 부문 규정 요약",
    "text": "ICROS 2025 4족보행로봇 챌린지: 원격 조종 부문 규정 요약\n\n1. 대회 개요 및 참가 조건\n\n\n\n\n\n\n\n항목\n내용\n\n\n\n\n대회명\nICROS 2025 4족보행로봇 챌린지\n\n\n주제\n4족 보행 로봇을 이용한 일상 생활 미션 수행\n\n\n참가 자격\n팀장 1명 + 팀원 최대 3명 (총 4명, 초과시 운영측 문의)\n\n\n등록\nICROS 2025 학회 등록 + 대회 홈페이지 구글 폼 등록\n\n\n로봇 소유\n각 팀은 고유한 로봇 1대 보유 (공유 불가)\n\n\n부문 선택\n자율 주행 / 원격 조종 중 택1 (저희 팀은 원격 조종)\n\n\n대회 장소\n전북대 컨벤션센터 지하 1층 102호\n\n\n대회 방식\n오프라인 현장 대회 (기기 검사 → 예선 → 본선)\n\n\n\n\n\n\n2. 로봇 및 장비 규정\n\n\n\n\n\n\n\n항목\n내용\n\n\n\n\n로봇 규격\n가로 70~85cm, 폭 30~40cm, 높이 35~45cm\n\n\n동작 전원\n온보드 배터리(15000mAh 이하, 안전검사 통과)\n\n\n센서\n온보드/고정 센서만 사용, 외부 데이터 전송 금지\n\n\n무선통신\n무선 공유기 사용 가능(SSID는 팀명), 운영 방해 시 제한 가능\n\n\n조종 콘솔\n운영측 공간 내, 최대 2명 조종자, 외부 정보 제공 금지\n\n\n외부장치\n외부 센서/컴퓨팅/제어 금지(응급상황 제외)\n\n\n\n\n\n\n3. 경기 진행 및 점수\n\n예선\n\n\n\n\n\n\n\n항목\n내용\n\n\n\n\n방식\n트랙 내 출발~도착, 순서대로 코스 통과, 연습 가능\n\n\n점수\n원격 조종 완주 60점, 자율주행 완주 120점, 코스별 점수, 미완주 시 위치 기준 점수예선 ( 6 &gt; 5 &gt; 4 &gt; 3 &gt; 2 &gt; 1 ) 원격 조종: 완주 성공 시 60점 부여됩니다. 자율 주행 시: 완주 성공 시 120점 부여됩니다.\n\n\n순위\n점수 → 시간 순, 동점시 시간 우선\n\n\n재시도\n팀당 부여 시간 내 재시도(재부팅) 가능\n\n\n개입\n트랙 내 참가자 개입 시 무효, 재시도\n\n\n\n\n\n본선\n\n\n\n\n\n\n\n항목\n내용\n\n\n\n\n방식\n예선전 순위의 역순으로 진행. 배달 미션(바구니+물품), 출발~도착, 장애물 통과\n\n\n점수\n원격 조종 완주 100점, 자율주행 완주 200점, 코스별 점수, 미완주 시 위치/물품 기준 점수본선 ( 4 &gt; 3 &gt; 2 &gt; 1 &gt; 1 &gt; 2 &gt; 3 &gt; 4 &gt; 5 &gt; 6) 원격 조종: 완주 성공 시 100점이 부여됩니다. 자율 주행 시: 완주 성공 시 200점이 부여됩니다. 배달 물품: 배달 물품이 바닥에 떨어지는 경우 개수에 따라 -1점 페널티가 적용 됩니다.\n\n\n페널티\n배달 물품 낙하 시 개당 -1점, 장애물 충돌 시 패널티\n\n\n순위\n점수 → 배달 물품 개수 → 시간 → 도착 위치 순\n\n\n재시도\n팀당 부여 시간 내 재시도(재부팅) 가능\n\n\n\n\n\n\n\n4. 트랙 및 장애물\n\n\n\nimage-20250506220354959\n\n\n\n\n\nimage-20250506220433108\n\n\n\n\n\n\n\n\n\n항목\n내용\n\n\n\n\n트랙\nA(수령)~B(도착) 구역, 장애물(연석, 볼라드, 킥보드 등), 바닥: 단단한 재질 + 섬유 트랙 + 나무\n\n\n장애물 규정\n로봇 너비 이상 간격, 폼/고무/나무 등 안전 재질, 충돌 시 손상 없어야 함\n\n\n\n\n\n\n5. 심판 및 운영\n\n\n\n\n\n\n\n항목\n내용\n\n\n\n\n심판 권한\n규정 위반/위험 시 경기 중단, 점수 반영, 불공정 행위 시 기록 무효화\n\n\n오픈소스\n우승팀 알고리즘 오픈소스 공개 권장\n\n\n\n\n\n\n6. 대회 일정 (2025년 기준)\n\n\n\n날짜\n내용\n\n\n\n\n4월 14일\n참가자 등록 시작\n\n\n5월 31일\n참가자 등록 종료\n\n\n6월 13일\n참가자 오리엔테이션\n\n\n6월 25일\n개회식 및 트랙 공개, 기기검사\n\n\n6월 26일\n예선\n\n\n6월 27일\n본선 및 폐회식"
  },
  {
    "objectID": "post/competition/icros2025/2025-05-06-icros2025-intro.html#참고-및-추가-안내",
    "href": "post/competition/icros2025/2025-05-06-icros2025-intro.html#참고-및-추가-안내",
    "title": "[icros2025] 1. 4족보행 대회 소개",
    "section": "참고 및 추가 안내",
    "text": "참고 및 추가 안내\n\n자세한 규정 전문은 팀원 모두가 숙지해야 하며, 대회 공식 규정 문서 및 QRCKOREA 공식 홈페이지에서 최신 정보를 확인하세요.\n원격 조종 부문은 콘솔 내 2명만 조종 가능, 외부 정보 제공 금지 등 현장 규정에 유의해야 합니다.\n배달 미션과 장애물 통과 등 실제 미션 상황을 충분히 연습하고, 페널티 항목(물품 낙하, 장애물 충돌 등)에 주의하세요."
  }
]