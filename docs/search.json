[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beyond Steps RL",
    "section": "",
    "text": "Beyond Steps RL symbolizes the ambition to go beyond conventional boundaries in reinforcement learning (RL). The name reflects the integration of cutting-edge RL techniques with robotic locomotion, especially focusing on quadrupedal robots. It emphasizes innovation, exploration, and the pursuit of advancements that push RL applications beyond mere movementâ€”toward solving real-world challenges with precision and adaptability.\n\n\nStudy Records\n\n\n\nNo.\nPaper\nPresenter\nDate\nFile\n\n\n\n\n1\nNot Only Rewards but Also Constraints\nJihong Kim\n24.09.01\npaper_review\n\n\n2\nAgile But Safe: Learning Collision-Free High-Speed Legged Locomotion\nJungYeon Lee\n24.09.01\npaper_review\n\n\n3\nSpinning Up in Deep RL\nChanwoo Park\n24.09.22\npaper_review\n\n\n4\nNot Only Rewards but Also Constraints II\nJehee Lee\n24.10.06\npaper_review\n\n\n5\nConstrained Policy Optimization\nJinwon Kim\n24.10.06\npaper_review\n\n\n6\nIPO: Interior-point Policy Optimization under Constraints\nJungYeon Lee\n24.10.06\npaper_review\n\n\n7\nTRPO/PPO\nChanwoo Park\n24.10.20\npaper_review\n\n\n8\nConstrained Policy Optimization II\nJihong Kim\n24.10.20\npaper_review\n\n\n9\nLearning-based legged locomotion; state of the art and future perspectives\nJinwon Kim\n24.11.17\npaper_review\n\n\n10\npympc-quadruped\nJihong Kim\n25.01.05\ncode_review\n\n\n11\nModel Predictive Control\nJungYeon Lee\n25.01.05\npaper_review\n\n\n\n\n\nMembers\n\n\n\n\n\n\n\n\n\n\n\nJungYeon Lee\nJihong Kim\nJinwon Kim\nChanwoo Park"
  },
  {
    "objectID": "goal.html",
    "href": "goal.html",
    "title": "Goal",
    "section": "",
    "text": "ëª©í‘œë¡œ ì œì¶œí•˜ê³ ì í•˜ëŠ” í•™íšŒë‚˜ ì €ë„ ì œì¶œê¸°ê°„ í™•ì¸\n\n\n\n\n\nCORL 2025\nhttps://www.corl.org/home\n\n\n\nICRA 2026\n\n\n\nIROS 2026"
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html",
    "href": "post/study/2025-03-01-how-to-read.html",
    "title": "How to read papers",
    "section": "",
    "text": "ë…¼ë¬¸ì„ ì½ëŠ”ë²• ì˜ìƒì€ ë…¼ë¬¸ ì½ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•ì— ëŒ€í•œ íŒì„ ì œê³µí•©ë‹ˆë‹¤. ë…¼ë¬¸ ì½ëŠ” ê²ƒì— ì •ë‹µì´ ì—†ê³  ì´ë¯¸ ê°ìì˜ ë°©ë²•ì´ ìˆê² ìœ¼ë‚˜, ì°¸ê³ í•˜ê¸°ì— ì¢‹ì€ ê²ƒ ê°™ì•„ AI ìš”ì•½ë³¸ìœ¼ë¡œ ê³µìœ í•©ë‹ˆë‹¤.\ní•µì‹¬ì€ ì²˜ìŒë¶€í„° ëª¨ë“  ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì½ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ìì‹ ì˜ ì´í•´ë„ì— ë§ì¶° ì „ëµì ìœ¼ë¡œ ì ‘ê·¼í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ëœë“œë§ˆí¬ ë…¼ë¬¸ ì„ ì°¾ì•„ Introductionê³¼ Related Worksë¥¼ í†µí•´ ë¶„ì•¼ì˜ í° ê·¸ë¦¼ì„ íŒŒì•…í•˜ê³ , ì´í›„ì—ëŠ” Methodì™€ Experiment ì¤‘ì‹¬ìœ¼ë¡œ ì½ì–´ ë‚˜ê°€ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë˜í•œ, ë…¼ë¬¸ ì—ì„œ ê°ì¶°ì§„ í•œê³„ì  ì„ íŒŒì•…í•˜ê³ , ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ë°œê²¬í•˜ëŠ” ë° ì§‘ì¤‘í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ì˜ìƒì€ ì—°êµ¬ ì£¼ì œë¥¼ ì„¤ì • í•˜ê³  ë…¼ë¬¸ì˜ ì™„ì„±ë„ ë¥¼ ë†’ì´ëŠ” ë° ì‹¤ì§ˆì ì¸ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#ë…¼ë¬¸-ì½ê¸°ì˜-ì‹œì‘",
    "href": "post/study/2025-03-01-how-to-read.html#ë…¼ë¬¸-ì½ê¸°ì˜-ì‹œì‘",
    "title": "How to read papers",
    "section": "1. ğŸë…¼ë¬¸ ì½ê¸°ì˜ ì‹œì‘",
    "text": "1. ğŸë…¼ë¬¸ ì½ê¸°ì˜ ì‹œì‘\n\nì¼ë°˜ì ìœ¼ë¡œ ë…¼ë¬¸ í•œ í¸ì„ ì“°ê¸° ìœ„í•´ ì•½ 100í¸ì˜ ê´€ë ¨ ë…¼ë¬¸ ì„ ì½ì–´ì•¼ í•˜ì§€ë§Œ, ì´ëŠ” í•™ìˆ ì  ë‚´ìš©ê³¼ ìˆ˜ì‹ ë•Œë¬¸ì— ì‰½ì§€ ì•Šì€ ê³¼ì œë‹¤.\nê¸°ì¡´ì˜ ë…¼ë¬¸ ì½ê¸° ë°©ë²•(ì´ˆë¡, ê²°ë¡  ìˆœìœ¼ë¡œ ì½ê¸°)ì€ ë¹ ë¥´ê²Œ ì½ì„ ìˆ˜ëŠ” ìˆì§€ë§Œ, ë‚´ìš© ì´í•´ì™€ ê¸°ì–µì—ëŠ” íš¨ê³¼ì ì´ì§€ ì•Šë‹¤ëŠ” ê²ƒì´ ë°í˜€ì¡Œë‹¤.\níš¨ìœ¨ì ì¸ ë…¼ë¬¸ ì½ê¸°ì˜ í•µì‹¬ì€ ì •í•´ì§„ ìˆœì„œê°€ ì•„ë‹Œ, ì½ëŠ” ì‚¬ëŒì˜ ì´í•´ ìˆ˜ì¤€ ì— ë”°ë¼ ì ‘ê·¼ ë°©ì‹ì„ ì¡°ì •í•˜ëŠ” ê²ƒì´ë‹¤.\nìƒˆë¡œìš´ ì—°êµ¬ ë¶„ì•¼ì— ì§„ì…í•  ë•ŒëŠ” ê²½í—˜ì´ ì „í˜€ ì—†ëŠ” ìƒíƒœì—ì„œ ì‹œì‘í•˜ë¯€ë¡œ, ì´ì— ë§ëŠ” íŠ¹ë³„í•œ ì ‘ê·¼ ë°©ì‹ì´ í•„ìš”í•˜ë‹¤."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#ëœë“œë§ˆí¬-ë…¼ë¬¸-ì°¾ê¸°-ì—°êµ¬ì˜-ì‹œì‘ì ",
    "href": "post/study/2025-03-01-how-to-read.html#ëœë“œë§ˆí¬-ë…¼ë¬¸-ì°¾ê¸°-ì—°êµ¬ì˜-ì‹œì‘ì ",
    "title": "How to read papers",
    "section": "2. ğŸ”ëœë“œë§ˆí¬ ë…¼ë¬¸ ì°¾ê¸°: ì—°êµ¬ì˜ ì‹œì‘ì ",
    "text": "2. ğŸ”ëœë“œë§ˆí¬ ë…¼ë¬¸ ì°¾ê¸°: ì—°êµ¬ì˜ ì‹œì‘ì \n\nì—°êµ¬ ë¶„ì•¼ì˜ ëœë“œë§ˆí¬  ë…¼ë¬¸ ì„ ì°¾ëŠ” ê²ƒì´ ë…¼ë¬¸ ì½ê¸°ì˜ ì²« ë‹¨ê³„ë‹¤.\ní‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•œ ë…¼ë¬¸ì˜ Related Works ì„¹ì…˜ì„ í†µí•´ ê´€ë ¨ ì—°êµ¬ë“¤ì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.\nì—¬ëŸ¬ ë…¼ë¬¸ ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì¸ìš©ë˜ëŠ”  ë…¼ë¬¸ ë“¤ì„ ëª¨ì•„ë‘ë©´ ì•½ 10ê°œ ì •ë„ì˜ í•µì‹¬ ë…¼ë¬¸ ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.\nêµ¬ê¸€ ìŠ¤ì¹¼ë¼ì—ì„œ ì¸ìš© íšŸìˆ˜ ë¥¼ í™•ì¸í•˜ì—¬ ë…¼ë¬¸ì˜ ì¤‘ìš”ì„±ê³¼ í’ˆì§ˆì„ íŒë‹¨í•  ìˆ˜ ìˆë‹¤.\nëœë“œë§ˆí¬ ë…¼ë¬¸ì„ ì°¾ì•˜ë‹¤ê³  í•´ì„œ ë°”ë¡œ ë°©ë²•ë¡ (Methodology)ì„ ì½ëŠ” ê²ƒì€ ì ì ˆí•˜ì§€ ì•Šë‹¤."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#ì—°êµ¬-ë¶„ì•¼ì˜-í°-ê·¸ë¦¼-íŒŒì•…í•˜ê¸°",
    "href": "post/study/2025-03-01-how-to-read.html#ì—°êµ¬-ë¶„ì•¼ì˜-í°-ê·¸ë¦¼-íŒŒì•…í•˜ê¸°",
    "title": "How to read papers",
    "section": "3. â›°ï¸ì—°êµ¬ ë¶„ì•¼ì˜ í° ê·¸ë¦¼ íŒŒì•…í•˜ê¸°",
    "text": "3. â›°ï¸ì—°êµ¬ ë¶„ì•¼ì˜ í° ê·¸ë¦¼ íŒŒì•…í•˜ê¸°\n\nì´ˆì‹¬ìëŠ” ê°œë³„ ë…¼ë¬¸ ì˜ ë¬¸ì œ í•´ê²° ë°©ì‹ë³´ë‹¤ ì—°êµ¬ ë¶„ì•¼ì˜ ì „ì²´ì ì¸ ë§¥ë½ ì„ ë¨¼ì € ì´í•´í•´ì•¼ í•œë‹¤.\nëœë“œë§ˆí¬ ë…¼ë¬¸ ì˜ Introductionê³¼ Related Works ë¥¼ ìì„¸íˆ ì½ì–´ ì—°êµ¬ ë¶„ì•¼ì˜ í° ë°©í–¥ì„±ì„ íŒŒì•…í•´ì•¼ í•œë‹¤.\në§ˆì¹˜ ì´ì•¼ê¸°ì±…ì„ ì½ë“¯ì´ ì—°êµ¬ ë¶„ì•¼ì˜ íë¦„ì„ íŒŒì•…í•˜ëŠ” ê³¼ì •ì€ ìƒê°ë³´ë‹¤ í¥ë¯¸ë¡­ë‹¤.\nì—¬ëŸ¬ ë…¼ë¬¸ ì„ ì½ë‹¤ ë³´ë©´ ë°˜ë³µë˜ëŠ” ë‚´ìš© ì´ ë‚˜íƒ€ë‚˜ëŠ”ë°, ì´ëŠ” í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë°˜ì ì¸ ì´í•´ë¥¼ ì–»ì—ˆë‹¤ëŠ” ì‹ í˜¸ë‹¤.\nì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ ì—°êµ¬ ë¶„ì•¼ì—ì„œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ”ì§€ ëŒ€ëµì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#ë…¼ë¬¸-ì½ê¸°ì˜-íš¨ìœ¨ì -ì ‘ê·¼-ë°©ë²•",
    "href": "post/study/2025-03-01-how-to-read.html#ë…¼ë¬¸-ì½ê¸°ì˜-íš¨ìœ¨ì -ì ‘ê·¼-ë°©ë²•",
    "title": "How to read papers",
    "section": "4. ğŸ§ë…¼ë¬¸ ì½ê¸°ì˜ íš¨ìœ¨ì  ì ‘ê·¼ ë°©ë²•",
    "text": "4. ğŸ§ë…¼ë¬¸ ì½ê¸°ì˜ íš¨ìœ¨ì  ì ‘ê·¼ ë°©ë²•\n\nì—°êµ¬ ë¶„ì•¼ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ íŒŒì•…í•œ í›„ì—ëŠ” Introductionê³¼ Related Worksë¥¼ ê±´ë„ˆë›°ê³  Methodologyì™€ Experiment ìœ„ì£¼ë¡œ ë…¼ë¬¸ ì„ ì½ëŠ”ë‹¤.\nì—¬ëŸ¬ ë…¼ë¬¸ ì˜ Introductionê³¼ Related WorksëŠ” ë¹„ìŠ·í•œ ë‚´ìš© ì„ ë‹¤ë£¨ë¯€ë¡œ, ì´ë¯¸ ì•Œê³  ìˆëŠ” ë‚´ìš©ì€ ìƒëµí•˜ê³  ë¬¸ì œ í•´ê²° ë°©ì‹ì— ì§‘ì¤‘ í•œë‹¤.\nì´ëŸ¬í•œ ì „ëµì  ì ‘ê·¼ìœ¼ë¡œ ì½ì–´ì•¼ í•  ì–‘ì´ ì ˆë°˜ ì´í•˜ë¡œ ì¤„ì–´ë“¤ì–´ íš¨ìœ¨ì„±ì´ í¬ê²Œ í–¥ìƒëœë‹¤.\nì²˜ìŒë¶€í„° ëê¹Œì§€ ëª¨ë“  ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì½ëŠ” ëŒ€ì‹ , ì¤‘ìš”í•œ ë…¼ë¬¸ë“¤ì„ ì„ ë³„í•˜ì—¬ Methodologyì™€ Experiment ìœ„ì£¼ ë¡œ ì½ëŠ”ë‹¤.\nì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ë…¼ë¬¸ì„ ì½ë‹¤ ë³´ë©´ ì—°êµ¬ ë¶„ì•¼ì˜ ì „ì²´ì ì¸ íë¦„ê³¼ ë¯¸í•´ê²° ë¬¸ì œ ë“¤ì„ íŒŒì•…í•  ìˆ˜ ìˆê²Œ ëœë‹¤."
  },
  {
    "objectID": "post/study/2025-03-01-how-to-read.html#ì¢‹ì€-ë…¼ë¬¸ì˜-ì¡°ê±´ê³¼-íš¨ê³¼ì ì¸-ë…¼ë¬¸-ì½ê¸°-ë°©ë²•",
    "href": "post/study/2025-03-01-how-to-read.html#ì¢‹ì€-ë…¼ë¬¸ì˜-ì¡°ê±´ê³¼-íš¨ê³¼ì ì¸-ë…¼ë¬¸-ì½ê¸°-ë°©ë²•",
    "title": "How to read papers",
    "section": "5. ğŸš©ì¢‹ì€ ë…¼ë¬¸ì˜ ì¡°ê±´ê³¼ íš¨ê³¼ì ì¸ ë…¼ë¬¸ ì½ê¸° ë°©ë²•",
    "text": "5. ğŸš©ì¢‹ì€ ë…¼ë¬¸ì˜ ì¡°ê±´ê³¼ íš¨ê³¼ì ì¸ ë…¼ë¬¸ ì½ê¸° ë°©ë²•\n\nì¢‹ì€ ë…¼ë¬¸ ì€ ë…¸ë²¨í‹°(novelty) ì™€ ì™„ê²°ì„± ì´ë¼ëŠ” ë‘ ê°€ì§€ ìš”ì†Œë¥¼ ì¶©ì¡±í•´ì•¼ í•œë‹¤.\në…¸ë²¨í‹° ëŠ” ë‹¤ë¥¸ ì‚¬ëŒì´ ì‹œë„í•˜ì§€ ì•Šì€ ìƒˆë¡­ê³  ê°€ì¹˜ ìˆëŠ” ì•„ì´ë””ì–´ë¥¼ ì˜ë¯¸í•˜ë©°, ì´ë¥¼ ìœ„í•´ì„œëŠ” ê¸°ì¡´ ì—°êµ¬ë¥¼ íŒŒì•…í•´ì•¼ í•œë‹¤.\në…¼ë¬¸ 100í¸ì„ ì½ëŠ” ì§„ì •í•œ ëª©ì ì€ ì—°êµ¬ ë¶„ì•¼ë¥¼ ì´í•´í•˜ê³  ì¢‹ì€ ì—°êµ¬ ì£¼ì œë¥¼ ì„¤ì • í•˜ê¸° ìœ„í•¨ì´ë‹¤.\në…¼ë¬¸ ì„ ì½ì„ ë•ŒëŠ” ë°©ë²•ë¡ (Methodology)ê³¼ ì‹¤í—˜(Experiment) ìœ„ì£¼ë¡œ ì½ë˜, ê° ë°©ë²•ì˜ í•œê³„ì  ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.\në…¼ë¬¸ì€ ë°€ë„ ë†’ì€ í…ìŠ¤íŠ¸ ì´ë¯€ë¡œ, ëª¨ë“  ë…¼ë¬¸ ì„ ì™„ì „íˆ ì†Œí™”í•˜ê¸°ë³´ë‹¤ëŠ” íš¨ìœ¨ì ìœ¼ë¡œ ì„ ë³„í•˜ê³  ì´í•´ í•˜ëŠ” ëŠ¥ë ¥ì´ ì¤‘ìš”í•˜ë‹¤."
  },
  {
    "objectID": "post/paper/2025-02-26-papers.html",
    "href": "post/paper/2025-02-26-papers.html",
    "title": "[5] 3mins papers",
    "section": "",
    "text": "ê°•í™”í•™ìŠµ(Reinforcement Learning, RL)ì„ í™œìš©í•˜ì—¬ ì‚¬ì¡±ë³´í–‰ ë¡œë´‡ì˜ ì í”„ ë™ì‘ì„ êµ¬í˜„í•˜ëŠ” ë™ì‹œì—, ì‹œë®¬ë ˆì´ì…˜-í˜„ì‹¤ ê°„ì˜ ì„±ëŠ¥ ì°¨ì´(sim-to-real gap)ë¥¼ ì¤„ì´ëŠ” ìƒˆë¡œìš´ ê¸°ë²•ì„ ì œì•ˆí•¨. ì´ë¥¼ ìœ„í•´ì„œ ì£¼íŒŒìˆ˜ ì˜ì—­ì—ì„œì˜ ì„í”¼ë˜ìŠ¤ ë§¤ì¹­ ê¸°ë²•ì„ í™œìš©í•˜ì˜€ìŒ\nê¸°ì¡´ì˜ ì—°êµ¬ëŠ” Domain gap, ë¬´ì‘ìœ„ì˜ Domain randomization, í•˜ë‚˜ì˜ ì •ì±…(ì í”„ ë“±)ë§Œì„ í•™ìŠµ, model-basedì˜ reference motionì„ ì°¸ì¡° ë“±ì´ í•„ìš”í–ˆìœ¼ë‚˜ ë³¸ ì—°êµ¬ì—ì„œëŠ” ì„í”¼ë˜ìŠ¤ ë§¤ì¹­ ê¸°ë²•ì„ í†µí•´ Domain gapì„ ì¤„ì´ê³  referenceì—†ì´ ë‹¤ì–‘í•œ ëª¨ì…˜ë“¤ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” í•™ìŠµ ë°©ë²•ì„ ì œì•ˆí•¨\nì§€ê¸ˆê¹Œì§€ì˜ ì—°êµ¬ì—ì„œëŠ” ì í”„ ëª¨ì…˜ì˜ ê²½ìš°, model-based controllerë‚˜ referenceë¥¼ í™œìš©í•™ìŠµí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ì—ˆìœ¼ë‚˜, ë³¸ ë…¼ë¬¸ì€ ìˆœìˆ˜ ê°•í™”í•™ìŠµë§Œìœ¼ë¡œ 55cmì˜ ê±°ë¦¬ë¥¼ ì í”„í•˜ëŠ” ëª¨ìŠµì„ ë³´ì„. ë§¤ìš° í¥ë¯¸ë¡œì›Œì„œ ì¢€ ë” ê¹Šì´ íŒŒë³´ê³ ì í•¨.\nPaper Link"
  },
  {
    "objectID": "post/paper/2025-02-26-papers.html#section-1",
    "href": "post/paper/2025-02-26-papers.html#section-1",
    "title": "[5] 3mins papers",
    "section": ">",
    "text": "&gt;\n\n\n\nPaper Link\nCode"
  },
  {
    "objectID": "post/paper/2025-02-26-papers.html#section-3",
    "href": "post/paper/2025-02-26-papers.html#section-3",
    "title": "[5] 3mins papers",
    "section": ">",
    "text": "&gt;\n\n\n\nPaper Link\nCode"
  },
  {
    "objectID": "post/paper/2025-02-26-papers.html#section-5",
    "href": "post/paper/2025-02-26-papers.html#section-5",
    "title": "[5] 3mins papers",
    "section": ">",
    "text": "&gt;\n\n\n\nPaper Link\nCode"
  },
  {
    "objectID": "post/paper/2025-02-22-papers.html",
    "href": "post/paper/2025-02-22-papers.html",
    "title": "[3] 3mins papers",
    "section": "",
    "text": "Hybrid Internal Model\n\nLearning Agile Legged Locomotion with Simulated Robot Response\n\n\nContrasive Learning(SwaV ê¸°ë°˜)ì„ í†µí•´ Latent vector ìƒì„±. Positive Pairì™€ Negative Pairë¥¼ ë¶„ë¥˜í•˜ì—¬ ë¹„ìŠ·í•œ í™˜ê²½ì—ì„œëŠ” Latent vectorë“¤ì´ ë¹„ìŠ·í•œ ê°’ì„ ê°–ë„ë¡, ë‹¤ë¥¸ í™˜ê²½ì—ì„œëŠ” ë‹¤ë¥¸ í‘œí˜„ì„ ê°–ë„ë¡ í•˜ëŠ” í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ Latent vector space ê³µê°„ì„ êµ¬ì„±í•˜ë„ë¡ í•¨.\nì§ì ‘ì ìœ¼ë¡œ í™˜ê²½ ì •ë³´ë¥¼ ì˜ˆì¸¡í•˜ì§€ ì•Šê³  ê°„ì ‘ì ìœ¼ë¡œ proprioceptive dataë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€í˜•ì— ëŒ€ì‘í•˜ëŠ” ë°©ë²•ì„\nì½”ë“œê°€ ê³µê°œë˜ì–´ ìˆê³  ì„±ëŠ¥ì´ ì¢‹ì•„ë³´ì´ë©°, ì§€í˜•ì— ë”°ë¼ Latentê°€ êµ¬ë¶„ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©° DreamWaQë³´ë‹¤ ëšœë ·í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” íŠ¹ì§•ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ\nPaper Link\nCode\n\n\n\n\nLearning Quadrupedal Locomotion over Challenging Terrain\n\nì •ë§ ìœ ëª…í•œ ë…¼ë¬¸ì„. ê·¹ë„ë¡œ ê°•ê±´í•œ ë³´í–‰ ì»¨íŠ¸ë¡¤ëŸ¬ ì„¤ê³„ë¥¼ ìœ„í•´ proprioceptive (ê³ ìœ ìˆ˜ìš©ì„± - ë‚´ê³„ ì •ë³´ = joint encoder, IMU, etc) ë§Œì„ ì‚¬ìš©í•´ ì‚¬ì¡±ë³´í–‰ ë¡œë´‡ì˜ ì§„í™, deformable terrain, ìê°ˆ, ë™ì ì¸ ì¥íŒ, ì‹ìƒ, ê±°ì„¼ ë¬¼ì‚´ì—ì„œë„ ë™ì‘í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŒ. ë” ë‹¨ìˆœí•œ ë„ë©”ì¸ì—ì„œ í•™ìŠµì„ ìˆ˜í–‰í•´ë„, ì‹¤ì œ ìì—° í™˜ê²½ì—ì„œì˜ ê°•ê±´í•¨ì„ ê·¹ëŒ€í™” í•  ìˆ˜ ìˆìŒ.\nêµ¬í˜„ì„ ìœ„í•´ êµì‚¬ ì •ì±…ì„ í•™ìŠµí•˜ì˜€ìŒ. ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì§€ë©´ì˜ ì •í™•í•œ ë†’ì´, ê²½ì‚¬ë„, ì ‘ì´‰ ìƒíƒœì™€ ê°™ì€ íŠ¹ê¶Œ (privileged) ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì •ì±…ì„ í•™ìŠµí•˜ê³ , ì‹¤ì œ ë¡œë´‡ì—ì„œëŠ” student policyë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ ìˆ˜ìš©ì„± ë°ì´í„°ë§Œì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” í•™ìƒ ì •ì±…ì„ í•™ìŠµí•¨.\nì§€ë‚œ ë…¼ë¬¸ì¸ Provable Partially Observable Reinforcement Learning with Privileged Informationì™€ ì—°ê²°í•´ ë³´ë©´, ê²°êµ­ ì´ë ‡ê²Œ í•™ìŠµì„ í•´ì„œ ë™ì‘ì„ í•˜ëŠ” ì´ìœ ëŠ”, â€™íŠ¹ê¶Œ ì •ë³´â€™ì˜ í™œìš©ì´ ê²°ì •ë¡ ì  í•„í„° ì¡°ê±´ (Deterministic Filter Condition) ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•Šë”ë¼ë„, ì¼ë¶€ ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•  ìˆ˜ ìˆë‹¤ëŠ” ëœ»ìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆìŒ. ì ì ˆí•œ constraint ë¥¼ ê±¸ì–´ì„œ lower boundë¥¼ ë§Œë“ ë‹¤ë©´, safe condition (constraint)ë¥¼ ë§Œì¡±í•  ìˆ˜ ìˆëŠ” ê°•í™”í•™ìŠµ ë³´í–‰ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ë§Œë“¤ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ ìƒê°ë©ë‹ˆë‹¤.\nPaper Link\n\n\n\n\nExtreme Parkour with Legged Robots\n\nì €ë¹„ìš©ì˜ ì„¼ì„œ(depth ì¹´ë©”ë¼ í•œê°œ)ì™€ ì €ë ´í•œ ì‚¬ì¡±ë³´í–‰ë¡œë´‡(actuationì´ ë¶€ì •í™•í•¨)ì„ í™œìš©í•˜ì—¬ ë§¤ìš° ë›°ì–´ë‚œ ìˆ˜ì¤€ì˜ íŒŒì¿ ë¥´ ë™ì‘ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ì†Œê°œí•˜ëŠ” ë…¼ë¬¸ì„.\nì´ëŠ” ê¸°ì¡´ì˜ íŒŒì¿ ë¥´ì™€ ê°™ì€ ë™ì‘ì„ ìˆ˜í–‰í•˜ëŠ” ë…¼ë¬¸ë“¤ë³´ë‹¤ í›¨ì”¬ ê°„ë‹¨í•˜ë©´ì„œë„ ì €ë¹„ìš©ìœ¼ë¡œ ì‹¤í˜„í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í–ˆë‹¤. ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robotsì˜ ê²½ìš° êµ¬í˜„ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” mapping module ë”°ë¡œ, planning moduleë”°ë¡œ, control module ë”°ë¡œ í•„ìš”í–ˆì—ˆì§€ë§Œ, ì´ ë°©ì‹ì€ í•˜ë‚˜ì˜ ë„¤íŠ¸ì›ŒíŠ¸ë§Œ í•™ìŠµì‹œì¼œì£¼ë©´ ëœë‹¤. ì´ë•Œ í•™ìŠµ ê¸°ë²•ì€ ìœ„ ì´ë¯¸ì§€ì™€ ê°™ì´ privileged informationì„ í†µí•´ ì™„ì„±ë„ ìˆëŠ” teacher ëª¨ë¸ì„ ë§Œë“¤ê³ , ì´ë¥¼ Dual Distillation í•™ìŠµ ê¸°ë²•ì„ í†µí•˜ì—¬ student ëª¨ë¸ì„ ë§Œë“ ë‹¤. (phase 1: ì „ë¬¸ê°€ ë°ì´í„°ë¥¼ í™œìš©í•œ ì§€ë„í•™ìŠµ / Phase 2: ì§€ë„ ë°ì´í„° ì—†ì´ ë¡œë´‡ì´ ìŠ¤ìŠ¤ë¡œ ë°©í–¥ì„ ì˜ˆì¸¡í•˜ë©° í•™ìŠµ)\nì˜ìƒì„ í™•ì¸í•œ í›„, í•™ìŠµì´ ë§¤ìš° ì˜ ì´ë£¨ì–´ì¡Œìœ¼ë©° sim-to-real ì „ì´ë„ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœ ë„¤íŠ¸ì›Œí¬ê°€ ì¶©ë¶„íˆ ì•ˆì „í•œì§€ì— ëŒ€í•´ì„œëŠ” ë‹¤ì†Œ ì˜ë¬¸ì´ ë“ ë‹¤. íŠ¹íˆ, ì¢Œìš° ë° í›„ë°©ì— ëŒ€í•œ ì •ë³´ ì—†ì´ ë†’ì€ ì†ë„ë¡œ ì „ì§„í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ëŠ” ì˜ˆìƒì¹˜ ëª»í•œ ìœ„í—˜ì„ ì´ˆë˜í•  ê°€ëŠ¥ì„±ì´ ìˆì–´ ë³´ì¸ë‹¤. ë˜í•œ, ì´ì „ ì—°êµ¬ì—ì„œëŠ” ìƒë‹¨ ì¥ì• ë¬¼ê¹Œì§€ ê³ ë ¤í•˜ì—¬ ë¡œë´‡ì´ ëª¸ì„ ìˆ™ì´ëŠ” ë™ì‘ì„ í•™ìŠµí•˜ëŠ” ë“± ë” ë‹¤ì–‘í•œ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤. ë”°ë¼ì„œ ë‹¨ìˆœíˆ ì €ë¹„ìš© ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ë°˜ë“œì‹œ ìµœì„ ì˜ ë°©ë²•ì´ë¼ê³ ëŠ” ìƒê°ë˜ì§€ ì•ŠëŠ”ë‹¤. ë¹„ìš© ì ˆê°ë„ ì¤‘ìš”í•˜ì§€ë§Œ, ë³´ë‹¤ í’ë¶€í•œ ì„¼ì„œ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì•ˆì „ì„±ê³¼ ì „ë°˜ì ì¸ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì ‘ê·¼ì´ ë” ë°”ëŒì§í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\nPaper Link\nVideo\n\n\n\n\nLearning Quadruped Locomotion Using Differentiable Simulation\n\níš¨ìœ¨ì ì¸ ì—­ì „íŒŒë¥¼ ìœ„í•´ ë‹¨ìˆœí•œ surrogate dynamics modelì—ì„œ ì–»ì€ smooth gradientsì™€ ì •í™•í•œ forward simulationì„ ìœ„í•´ non-differentiable ì‹œë®¬ë ˆì´í„°ì˜ high fidelityì„ ê²°í•©í•œ ìƒˆë¡œìš´ ì •ì±…í•™ìŠµ ë°©ì‹ ì œì•ˆ\nnon-differentiableì¸ IsaacGym ì‹œë®¬ë ˆì´í„°ëŠ” ë³µì¡í•œ contact ë™ì—­í•™ì„ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í™œìš©í•˜ì—¬ ë‹¨ìˆœí™”ëœ rigid-body dynamics modelì˜ ìƒíƒœ(state)ë¥¼ ì •ë ¬(align)í•¨ìœ¼ë¡œì¨, training pipelineì´ ì‹¤ì œ ë™ì—­í•™ì— ê¸°ë°˜ì„ ë‘ë„ë¡ ë³´ì¥\nDifferential simulatorë¥¼ ë”°ë¡œ ê°œë°œí•˜ëŠ” ì›€ì§ì„(brax)ë„ ìˆëŠ”ë° differential loss termì„ ì¶”ê°€í•´ì„œ non-differential simulatorë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ì´ ìƒˆë¡œì› ìœ¼ë©° PPOë³´ë‹¤ ì ì€ agentìˆ˜ë¡œë„ í•™ìŠµ rewardê°€ ë¹ ë¥´ê²Œ maximizationëœë‹¤ëŠ” ì ì´ ë†€ë¼ì› ìŒ\nPaper Link"
  },
  {
    "objectID": "post/paper/2025-02-28-papers.html",
    "href": "post/paper/2025-02-28-papers.html",
    "title": "[6] 3mins papers",
    "section": "",
    "text": "Combining Learning-based Locomotion Policy with Model-based Manipulation for Legged Mobile Manipulators\n\n3ì¤„ìš”ì•½\n\nMPCë¥¼ ì‚¬ìš©í•˜ëŠ” ë§¤ë‹ˆí“°ë ˆì´í„°ì™€ RLì„ ì‚¬ìš©í•˜ëŠ” 4ì¡±ë¡œë´‡ì˜ í†µí•©\në§¤ë‹ˆí“°ë ˆì´í„°ê°€ ì›€ì§ì´ë©´, 4ì¡±ë¡œë´‡ì€ ê·¸ ì™¸ë€ì„ ì˜ˆì¸¡í•´ì„œ ë¡œë´‡ì´ ê· í˜•ì„ ìœ ì§€í•˜ê³  ì•ˆì •ì ìœ¼ë¡œ ì´ë™í•  ìˆ˜ ìˆê²Œ í•¨.\nRLì€ ë§¤ë‹ˆí“°ë ˆì´í„°ì˜ ë™ì—­í•™ ëª¨ë¸ì„ ì§ì ‘ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì™¸ë€ì„ ê°€í•´ì¤€ë‹¤ìŒì— MSEë¥¼ í†µí•´ ì˜ˆì¸¡ëª¨ë¸ì„ ë§Œë“œëŠ”ê²ƒì„.\n\n\n\nTraining in Simulation\n\në¡œë´‡ì€ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ë¬´ì‘ìœ„ í˜ì˜ ì¡°í•©ì¸ wrench sequenceë¥¼ ì ìš©ë°›ìŒ. í•™ìŠµê³¼ì •ì—ì„œ ì´ ë Œì¹˜ ì‹œí€€ìŠ¤ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•¨.\ní•™ìŠµì¤‘ì—ëŠ” ë³„ë„ì˜ ë§¤ë‹ˆí“°ë ˆì´í„° ì œì–´ê¸°ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ. legged gymì—ì„œ push robotê³¼ ê°™ì€ í•¨ìˆ˜ë¥¼ ì§€ì†ì ìœ¼ë¡œ ê°€í•´ì£¼ëŠ”ê²ƒê³¼ ë™ì¼í•¨.\n\n\n\nResults\n\në§¤ë‹ˆí“°ë ˆì´í„°ì˜ ë‹¤ì–‘í•œ ë™ì‘ê³¼ ë¬´ê²Œ ë³€í™”ì— ëŒ€í•œ ì ì‘ì„±ì´ ìƒê¹€.\nì œì–´ ì‹œìŠ¤í…œì„ 4ì¡±ë¡œë´‡, ë§¤ë‹ˆí“°ë ˆì´í„° ì»¨íŠ¸ë¡¤ëŸ¬ ë‘ê°œì˜ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ê° ëª¨ë“ˆì˜ ë…ë¦½ì ì¸ ê°œë°œ ë° ìµœì í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨.\në¶„ëª… ì´ì „ì— ì´ê±°ë‘ ê°™ì€ ì»¨ì…‰ì¸ ì‹œë®¬ë ˆì´ì…˜ ë…¼ë¬¸ì´ ì•„ì¹´ì´ë¸Œì— ì˜¬ë¼ì™”ì—ˆëŠ”ë° ì§€ê¸ˆì€ ëª»ì°¾ê² ìŒ. í™•ì‹¤íˆ ì§ê´€ì ì¸ ì•„ì´ë””ì–´ì—¬ë„ ì‹¤ì œ í•˜ë“œì›¨ì–´ë¡œ ì‹¤í—˜ì„ í•´ì•¼ ë…¼ë¬¸ì´ ë˜ëŠ”ê²ƒê°™ë‹¤.\nPaper Link\n\n \n\n\n\nFoundation Models in Robotics: Applications, Challenges, and the Future\n\nê°•í™”í•™ìŠµ + MPC ì£¼ì œìª½ìœ¼ë¡œ ë³´ê³  ìˆì§€ë§Œ ì´ë²ˆì—ëŠ” ê´€ì‹¬ì„ ê°€ì§€ê³  ìˆëŠ” Foundation modelì— ëŒ€í•´ì„œ ê°€ì§€ê³  ì™€ë´¤ë‹¤. ë³¸ ë…¼ë¬¸ì€ Foundation modelì´ ë¡œë³´í‹±ìŠ¤ì—ì„œ ì–´ë–¤ì‹ìœ¼ë¡œ í™œìš©ë˜ì–´ ì™”ê³ , ì–´ë–»ê²Œ ì—°êµ¬ê°€ ë ì§€ì— ëŒ€í•´ì„œ ë…¼ì˜í•˜ëŠ” ë…¼ë¬¸ì´ë‹¤. Foundation modelì„ í†µí•´ ë¡œë´‡ì˜ ì¸ì§€, ì˜ì‚¬ê²°ì •, ì œì–´ì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ë¶„ì„í•œë‹¤.\níŠ¹íˆ, ê¸°ì´ˆ ëª¨ë¸ì„ í™œìš©í•œ ë¡œë´‡ ì •ì±… í•™ìŠµê³¼ ê°•í™”í•™ìŠµì˜ ì ‘ì ìœ¼ë¡œ Language-Assisted Reinforcement Learning (LLM ê¸°ë°˜ ë³´ìƒ í•™ìŠµ ë° íƒìƒ‰), Vision-Language Value Learning (VLMì„ í™œìš©í•œ Value Function í•™ìŠµ), Robotics Transformers (Transformer ê¸°ë°˜ í–‰ë™ ì •ì±… í•™ìŠµ) ë“±ì´ ì œì‹œë˜ì—ˆìœ¼ë©°, ì´ëŠ” ê°•í™”í•™ìŠµ ê¸°ë°˜ì˜ ì œì–´ê¸° ì„¤ê³„ì— ìœ ìš©í•  ìˆ˜ ìˆë‹¤.\në‹¤ë§Œ, ì‹¤ì‹œê°„ ì‘ë‹µ ì†ë„ ë¬¸ì œ, ë°ì´í„° ë¶€ì¡±, ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ë¶€ì¡± ë“±ì˜ í•œê³„ê°€ ì¡´ì¬í•˜ë©°, RLê³¼ MPCë¥¼ ê²°í•©í•œ ì œì–´ê¸° í•™ìŠµì„ ìœ„í•œ foundation modelì˜ ì§ì ‘ì ì¸ ì ìš© ì‚¬ë¡€ëŠ” ë¶€ì¡±í•˜ë‹¤. í–¥í›„ ì—°êµ¬ ë°©í–¥ìœ¼ë¡œ RLê³¼ ê²°í•© ê°€ëŠ¥í•œ ê²½ëŸ‰í™”ëœ foundation model ê°œë°œ ë° ë¡œë´‡ ì œì–´ íŠ¹í™”ëœ ë°ì´í„°ì…‹ êµ¬ì¶•ì´ í•„ìš”í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\nPaper Link\nYoutube"
  },
  {
    "objectID": "post/paper/2025-02-20-papers.html",
    "href": "post/paper/2025-02-20-papers.html",
    "title": "[2] 3mins papers",
    "section": "",
    "text": "Actor-Cross-Critic\n\nLeveraging Privileged Information for Partially Observable Reinforcement Learning\n\n\nVAE ë“±ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë“¤ì€ ê°„ì ‘ì ì¸ ì •ë³´ ì œê³µì„ í•˜ê¸° ë•Œë¬¸ì— Asymmetric Actor Critic êµ¬ì¡°ì˜ ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•¨\në‘ê°œì˜ Critic ë„¤íŠ¸ì›Œí¬ë“¤ì„ í•™ìŠµí•˜ê³  Advantageê°€ ë” í° ê°’ì„ ì„ íƒí•˜ì—¬ Actorì—ê²Œ ì œê³µí•˜ëŠ” ë°©ë²•. Oracle Criticê³¼ Executor Critic 2ê°œì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ê°€ì§€ê³  Overestimation ë¬¸ì œë¥¼ ë°©ì§€\nì´ëŸ¬í•œ ACCì˜ í•™ìŠµ ê³¼ì •ì´ ê¸°ì¡´ì˜ Actor Critic ë°©ë²•ë³´ë‹¤ ë” ì•ˆì •ì ì´ê³  ë¹ ë¥´ê²Œ ìˆ˜ë ´í•¨ì„ ì¦ëª…\nPaper Link\n\n\n\n\nANYmal Parkour\n\nLearning Agile Navigation forQuadrupedal Robots\n\n\nì™„ì „ í•™ìŠµ ê¸°ë°˜(fully learned) ë„¤ë¹„ê²Œì´ì…˜: í™˜ê²½ì˜ 3D ì¬êµ¬ì„± ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë³´í–‰ ê¸°ìˆ ì„ ë™ì ìœ¼ë¡œ ì„ íƒ.\ní•˜ì´ë¸Œë¦¬ë“œ ì •ì±… êµ¬ì¡°: PPO ê¸°ë°˜ìœ¼ë¡œ Gaussian ë¶„í¬(ì €ìˆ˜ì¤€ ëª…ë ¹) + ë²”ì£¼í˜• ë¶„í¬(ê¸°ìˆ  ì„ íƒ)ë¥¼ ê²°í•©.\nìƒˆë¡œìš´ ë³´í–‰ ê¸°ìˆ  í•™ìŠµ: ì í”„, ë“±ë°˜, ì›…í¬ë¦¬ê¸° ë“± ë‹¤ì–‘í•œ ì—­ë™ì  ë™ì‘ì„ í•™ìŠµí•˜ì—¬ ë†’ì€ ì¥ì• ë¬¼ë„ ê·¹ë³µ ê°€ëŠ¥.\nì‹ ê²½ë§ ê¸°ë°˜ 3D í™˜ê²½ ì¬êµ¬ì„±: ë‹¤ì¤‘ í•´ìƒë„ ë§µí•‘ì„ í†µí•´ ë¡œë´‡ ì£¼ë³€ì€ ê³ í•´ìƒë„ë¡œ, ë©€ë¦¬ ìˆëŠ” í™˜ê²½ì€ ì €í•´ìƒë„ë¡œ ì¸ì‹í•˜ì—¬ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ì„ í™•ë³´.\nì§€ê¸ˆê¹Œì§€ ì—°êµ¬í•´ì˜¨ ì—°êµ¬ ë‚´ìš©ë“¤ì„ ì´ë§ë¼í•˜ì—¬ perception ë¶€í„° agileí•œ controlê¹Œì§€ ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•œ ëŠë‚Œì„ ë°›ìŒ\nPaper Link\n\n\n\n\nStiffness Tuning\n\nVariable Stiffness for Robust Locomotion through Reinforcement Learning\n\n\nê°•í™”í•™ìŠµ ë³´í–‰ ì œì–´ì—ì„œ, joint stiffness íŠœë‹ ì—†ì´ë„ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ ë³€ìˆ˜ stiffness p gainì„ action spaceì— í†µí•©í•˜ëŠ” ìƒˆë¡œìš´ ì œì–´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì•ˆ.\nstiffnessì„ ê´€ì ˆë³„(PJS), ë‹¤ë¦¬ë³„(PLS), í•˜ì´ë¸Œë¦¬ë“œ(HJLS)ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì œì–´í•˜ëŠ” ë°©ì‹ì„ ì ìš©í•˜ì—¬ PLSëŠ” ì†ë„ ì¶”ì  ë° ì™¸ë ¥ ëŒ€ì‘ì—ì„œ ìš°ìˆ˜í•˜ë©°, HJLSëŠ” ì—ë„ˆì§€ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ\nPd íŠœë‹ ì¤‘ d ê²Œì¸ì€ p ê²Œì¸ì— ì˜ì¡´ ë³€ìˆ˜ë¡œ ë†“ì•˜ê³  ê¸°ì¡´ì˜ action space ë””ìì¸ì„ ìƒ‰ë‹¤ë¥´ê²Œ í–ˆë‹¤ëŠ” ì ì—ì„œ ì°¸ì‹ í•œ ê²ƒ ê°™ìŒ. ê°•í™”í•™ìŠµ ë³´í–‰ì œì–´ì—ì„œ ê´€ìŠµì ìœ¼ë¡œ í•˜ëŠ” ë¶€ë¶„ì„ ê±´ë“œë ¸ë‹¤ëŠ” ì¸¡ë©´ì—ì„œ ìƒˆë¡œì› ìŒ.\nPaper link\n\n\n\n\nPrivileged Information\n\nProvable Partially Observable Reinforcement Learning with Privileged Information\n\n\nê°•í™”í•™ìŠµì€ ë¬´ì¡°ê±´ ì ìœ¼ë¡œ Partially Observable MDP (POMDP) í™˜ê²½ì´ë©°, í•™ìŠµ ë‹¨ê³„ì—ì„œ ì‹œë®¬ë ˆì´ì…˜ì—ì„œë§Œ ì–»ì„ ìˆ˜ ìˆëŠ” ë…¸ì´ì¦ˆê°€ ì—†ëŠ” ê¹¨ë—í•œ ë°ì´í„°ì¸ íŠ¹ê¶Œ ì •ë³´ (Privileged Information) ì´ ì¡´ì¬í•¨. ì˜ˆë¥¼ ë“¤ì–´ ë…¸ì´ì¦ˆê°€ ì—†ëŠ” ì™„ì „í•œ ìƒíƒœ ì •ë³´ë¥¼ ì´ìš©í•˜ë©´ ì •ì±…ì„ í•™ìŠµí•˜ëŠ”ë° ë„ì›€ì´ ë˜ë©°, ì‹¤ì œ ì‹¤í–‰ì—ì„œëŠ” ì „ë¬¸ê°€ ì¦ë¥˜ (expert distillation) ê³¼ ë¹„ëŒ€ì¹­ ì•¡í„°-í¬ë¦¬í‹± (í›ˆë ¨ê³¼ ì‹¤í–‰ ë‹¨ê³„ì—ì„œ ë‹¤ë¥¸ ê´€ì°°ê°’ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ - í¬ë¦¬í‹±ì€ íŠ¹ê¶Œ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ í‰ê°€, ì•¡í„°ëŠ” ì œí•œëœ ê´€ì°°ê°’ë§Œì„ ì‚¬ìš©)ì„ ì´ìš©í•˜ì—¬ í˜„ì‹¤ ë°ì´í„°ì— ì ìš©í•¨.\nìµœê·¼ ì¦ë¥˜ ê¸°ë²•ì´ ì—¬ëŸ¬ í•™ìŠµì—ì„œ ë§¤ìš° ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŒ. ê·¸ëŸ¬ë‚˜ ìµœê·¼ ì ìš©ë˜ê³  ìˆëŠ” ì¦ë¥˜ê¸°ë²•ì€ í•­ìƒ ìµœì  ì •ì±…ì„ ë³´ì¥í•˜ì§€ ì•Šìœ¼ë©°, ê·¼ì‚¬ ìµœì  ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ì†ì‹¤ì´ ë°œìƒí•¨. ì €ìë“¤ì€ POMDPì—ì„œ ê´€ì°°ê°’ì„ í†µí•´ í˜„ì¬ì˜ ì§„ì§œ ìƒíƒœ (state)ë¥¼ ê²°ì •í•  ìˆ˜ ìˆëŠ” ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ì— ëŒ€í•œ ì—¬ë¶€ë¥¼ ê²°ì •ë¡ ì  í•„í„° ì¡°ê±´ (Deterministic Filter Condition)ìœ¼ë¡œ ì •ì˜í•¨. ì´ ì¡°ê±´, ì¦‰ í•„í„°ê°€ ê²°ì •ë¡ ì ì¼ ê²½ìš°ì— (ê´€ì°°ê°’ì´ ì£¼ì–´ì§€ë©´ í˜„ì¬ ìƒíƒœë¥¼ ì •í™•í•˜ê²Œ ì¶”ë¡ ì´ ê°€ëŠ¥í•  ë•Œ), ì „ë¬¸ê°€ ì¦ë¥˜ì˜ ì†ì‹¤ì„ ë°©ì§€í•  ìˆ˜ ìˆìŒ. ì˜ˆì‹œë¡œ, ë¡œë´‡ ì†ì„ ì´ìš©í•œ ë¬¼ì²´ ì¡°ì‘ì€ (ì¡°ê±´: ë¡œë´‡ì´ ì†ì— ìˆëŠ” ë¬¼ì²´ì˜ ìœ„ì¹˜ì™€ ë°©í–¥ì„ ì •í™•íˆ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤ë©´, ê²°ê³¼: ê´€ì°°ì„ í†µí•´ ë¬¼ì²´ì˜ ì‹¤ì œ ìƒíƒœë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŒ) ì´ ì¡°ê±´ì„ ë§Œì¡±í•¨. ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì—ì„œëŠ” (ì¡°ê±´: ë„ë¡œì˜ ë¯¸ë„ëŸ¬ì›€ ì •ë„ëŠ” ì°¨ëŸ‰ ì„¼ì„œë¡œ ì•Œ ìˆ˜ ì—†ìŒ. ê²°ê³¼: ì„¼ì„œê°’ì´ ë™ì¼í•´ë„ ë„ë¡œì˜ ë¯¸ë„ëŸ¬ì›€ ì •ë„ëŠ” ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ) ì´ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŒ.\nì´ ë…¼ë¬¸ì€ NIPS ë…¼ë¬¸ì´ë©°, ìˆ˜ì‹ì  ì¦ëª…ì´ ë“¤ì–´ê°€ëŠ” ë…¼ë¬¸ìœ¼ë¡œ 72 í˜ì´ì§€ì˜ ë¶„ëŸ‰ì„ ê°€ì§€ê³  ìˆë„¤ìš”. ë¦¬ë·°ì–´ë“¤ì´ ê³ ìƒì„ ë§ì´ í–ˆê² ìŠµë‹ˆë‹¤.. ì´ ë…¼ë¬¸ì˜ ì•„ì´ë””ì–´ë¥¼ ì°¨ìš©í•´ ë³´ìë©´, ì¦ë¥˜ ë° ë¹„ëŒ€ì¹­ ì•¡í„°-í¬ë¦¬í‹±ì€ ì ìš©í•  ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì‚¬ì¡±ë³´í–‰ ë¡œë´‡ì€ ì´ â€™ê²°ì •ë¡ ì  í•„í„° ì¡°ê±´â€™ì´ ì¶©ì¡±ë˜ì§€ ì•Šì„ ì¡°ê±´ì„ ë„ˆë¬´ ë§ì´ ê°€ì§€ê³  ìˆê¸°ì—, ì´ ë…¼ë¬¸ì˜ ì•„ì´ë””ì–´ë¥¼ ì ìš©í•´ ë³´ë ¤ë©´ ì ë‹¹í•œ assumptionì„ í†µí•´ì„œ lower boundë¥¼ ì°¾ì•„ë‚´ì–´ ì ìš©í•´ ë³´ëŠ” ê²ƒë„ ì¬ë¯¸ìˆê² ë‹¤ëŠ” ìƒê°ì´ ë“œë„¤ìš”\nPaper Link"
  },
  {
    "objectID": "post/paper/2025-02-18-papers.html",
    "href": "post/paper/2025-02-18-papers.html",
    "title": "[1] 3mins papers",
    "section": "",
    "text": "DreamFLEX\n\nLearning Fault-Aware Quadrupedal Locomotion Controller for Anomaly Situation in Rough Terrains\n\n\në¡œë´‡ì´ ì‹¤ì œ ì˜¤ë˜ êµ¬ë™í–ˆì„ ì‹œì— ë§ˆì£¼ì¹  ìˆ˜ ìˆëŠ” í•˜ë“œì›¨ì–´ ê²°í•¨ì— ëŒ€ì‘í•˜ëŠ” ë°©ë²•ë¡ , jointì˜ ê²°í•¨ì´ ìƒê¸°ë©´ ì´ë¥¼ ê°ì§€í•˜ì—¬ ê²°í•¨ì´ ìˆëŠ” ìƒí™©ì— ë§ì¶°ì„œ ë³´í–‰ íŒ¨í„´ì„ ì¬êµ¬ì„±\nDreamWaQì˜ ì»¨ì…‰ì„ ìœ ì§€í•˜ë©° CENetì„ FEMNetìœ¼ë¡œ ë°”ê¾¸ì–´ ì¡°ì¸íŠ¸ ê²°í•¨ì„ ê°ì§€í•˜ëŠ” ê²ƒê³¼ ì—°ê´€ìˆëŠ” term \\(f_t\\) ì¶”ê°€\nJoint Faultì˜ ìƒí™©ì€ locked jointì™€weakened motor` 2ê°€ì§€ ê²½ìš°ì— ëŒ€í•´ ìƒí™©ì„ ì •ì˜í•˜ì—¬ ê²°í•¨ì´ ìˆëŠ” ë°œì„ ì œì™¸í•˜ê³ ì„œë¼ë„ ë³´í–‰í•˜ëŠ” ê²ƒì´ ëª©ì \nPaper Link\n\n\n\n\nRL-augmented MPC\n\nLearning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC\n\n\nRLì„ ë³´ì™„í•˜ëŠ” MPC í”„ë ˆì„ì›Œí¬ ê°œë°œ ê³ ì† ì´ë™, ëª¨ë¸ ë¶ˆí™•ì‹¤ì„± ì ì‘, ì¥ì• ë¬¼ íšŒí”¼ë¥¼ í¬í•¨í•˜ëŠ” ì ì‘í˜• ë¸”ë¼ì¸ë“œ(blind) ë³´í–‰ì„ ì‹¤í˜„\nìŠ¤íƒ ìŠ¤ ë°œ í˜ ì œì–´(stance foot force control)ì™€ ìŠ¤ìœ™ ë°œ ë°˜ì‚¬(swing foot reflection)ë¥¼ ê²°í•© ê¸°ì¡´ MPCê°€ ì§ë©´í•œ ìŠ¤ìœ™ ë°œ ê¶¤ì ì˜ ë¶ˆí™•ì‹¤ì„± ë¬¸ì œë¥¼ í•´ê²° ëª¨ë¸ ë¶ˆí™•ì‹¤ì„±(model uncertainty)ì— ì ì‘í•˜ë©´ì„œ ë¡œë´‡ì˜ ê· í˜• ìœ ì§€ ëŠ¥ë ¥ì„ ê°œì„ \në¡œë´‡-ë…ë¦½ì (robot-agnostic) RL ëª¨ë“ˆ ë„ì… ë‹¤ì–‘í•œ ë¡œë´‡ í”Œë«í¼(Unitree A1, Go1, AlienGo)ì—ì„œ zero-shot transfer ê°€ëŠ¥ RL ì •ì±…ì´ íŠ¹ì • ë¡œë´‡ì— ìµœì í™”ë˜ì§€ ì•Šê³  ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œë„ ì¼ë°˜í™” ê°€ëŠ¥\nstance phaseì™€ swing phaseë¥¼ ë¶„ë¦¬í•˜ëŠ” ê²ƒì€ ë‹¹ì—°í•œ ì „ì œë¡œ ìƒê°í–ˆì—ˆëŠ”ë°, ì´ë¥¼ í˜¼í•©í•˜ë ¤ëŠ” ì‹œë„ëŠ” ì‹ ì„ í•˜ê²Œ ëŠê»´ì§\nPaper Link\n\n\n\n\nCAIMAN\n\nCausal Action Influence Detection for Sample Efficient Loco-manipulation\n\n\n4ì¡±ë³´í–‰ ë¡œë´‡ì´ í¬ê³  ë¬´ê±°ìš´ ë¬¼ì²´ë¥¼ ì¡°ì‘í•˜ë©° ì´ë™í•  ìˆ˜ ìˆë„ë¡ ë¹„-íŒŒì§€(non-prehensile) ë°©ì‹ìœ¼ë¡œ í¬ê³  ë¬´ê±°ìš´ ë¬¼ì²´ë¥¼ ì¡°ì‘í•˜ë©° ì´ë™(loco-manipulation)í•˜ëŠ” Taskë¥¼ í•™ìŠµ\ncausal action influence(ì¸ê³¼ì  í–‰ë™ ì˜í–¥, CAI)ì„ í™œìš©í•˜ì—¬ ë¡œë´‡ì´ í™˜ê²½ì„ ì œì–´í•˜ëŠ” ìƒíƒœë¥¼ íƒì§€í•˜ê³ , ì´ë¥¼ ë‚´ì¬ì  ë™ê¸°(intrinsically motivated objective)ë¡œ ì‚¼ì•„ hierarchical control strategyì„ í†µí•´ í•™ìŠµ\në¬¼ì²´ë¥¼ ë¯¸ëŠ” ë¬¼ë¦¬ì  ê²°ê³¼ì— ëŒ€í•œ ë™ì—­í•™ ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ CAI ê°’ì„ ëŒì–´ë‚¸ë‹¤ëŠ” ì ì´ íŠ¹ì´í•´ì„œ í¥ë¯¸ë¡œì› ìœ¼ë©°, ìµœê·¼ì— Loco-Manipulationì´ë¼ê³  í•˜ë©´ ë³´í†µ ë§¤ë‹ˆí“°ë ˆì´í„°ë¥¼ ê°„ë‹¨í•œê±°ë¼ë„ ë‹¤ëŠ”ë° ì—¬ê¸°ì—ì„œëŠ” ê·¸ëƒ¥ ë°€ê¸°ì˜€ìŒ\nPaper Link\n\n\n\n\nSafe RL/MPC Testbed\n\në¡œë´‡ì˜ ë³€í™” ê°€ëŠ¥í•œ í™˜ê²½ì—ì„œì˜ MPC, RLì˜ ì•ˆì •ì„± í‰ê°€ Testbad ì •ì˜\n\n\nSafeRL/MPCë¼ê³  í•˜ë©´, ì–´ë–¤ í™˜ê²½ì—ì„œ ì•ˆì •ì„±ì„ í‰ê°€í•˜ëŠ” ê²ƒì´ í•©ë¦¬ì ì¸ê°€\në¡œë´‡ì— ìš”êµ¬ë˜ëŠ” Taskê°€ ë‹¤ì–‘í•´ì§ì— ë”°ë¼ í‰ê°€í•  ìˆ˜ ìˆëŠ” ì ì ˆí•œ Testbed ë° ê¸°ì¤€ í•„ìš”\ní™˜ê²½ë³€í™”ë¡œ â€œë¡œë´‡ ì™¸ë¶€ì˜ í™˜ê²½ ë³€í™”â€, â€œë²”ìš©ì  ì„ë¬´ ìˆ˜í–‰ì„ ìœ„í•œ ë¡œë´‡ì˜ ë™ì‘ìœ¼ë¡œ ì¸í•´ ë°œìƒëœ í™˜ê²½ ë³€í™”â€, â€œë¡œë´‡ì˜ ì¡°ì • ê°€ëŠ¥í•œ HW, ë¶€ì°©ë¬¼ì˜ ìœ„ì¹˜, êµ¬ì„±ìœ¼ë¡œ ì¸í•´ ë°œìƒëœ í™˜ê²½ ë³€í™”â€ë¥¼ ìƒê°í•´ë³¼ ìˆ˜ ìˆìŒ"
  },
  {
    "objectID": "post/paper/2025-02-24-papers.html",
    "href": "post/paper/2025-02-24-papers.html",
    "title": "[4] 3mins papers",
    "section": "",
    "text": "Paper Link\nCode\n\n\n\nRLOC: Terrain-Aware Legged Locomotion using Reinforcement Learning and Optimal Control\n\n2020ë…„ ë…¼ë¬¸ì´ì§€ë§Œ, ê³„ì†í•´ì„œ Privileged Information ì»¨ì…‰ì„ ìœ ì§€í•˜ë©´ì„œ, ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤. - ì´ ë…¼ë¬¸ì€ ê°•í™”í•™ìŠµ(RL)ê³¼ ëª¨ë¸ ê¸°ë°˜ ì œì–´ë¥¼ í†µí•©í•˜ì—¬ ì‚¬ì¡±ë³´í–‰ ë¡œë´‡ì´ ë‹¤ì–‘í•œ ì§€í˜•ì—ì„œ ë™ì  ë³´í–‰ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ì˜¨ë³´ë“œ ê³ ìœ ìˆ˜ìš©ì„±(proprioceptive) ë° ì™¸ë¶€ìˆ˜ìš©ì„±(exteroceptive) í”¼ë“œë°±ì„ í™œìš©í•˜ì—¬ ì„¼ì„œ ì •ë³´ì™€ ì›í•˜ëŠ” ì†ë„ ëª…ë ¹ì„ ë°œê±¸ìŒ ê³„íšìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” RL ì •ì±…ì„ í•™ìŠµí•©ë‹ˆë‹¤. í•™ìŠµëœ ì •ì±…ì€ ëª¨ë¸ ê¸°ë°˜ ëª¨ì…˜ ì»¨íŠ¸ë¡¤ëŸ¬ì™€ ê²°í•©ë˜ì–´ ë³µì¡í•œ ì§€í˜•ì—ì„œë„ ì•ˆì •ì ì¸ ë³´í–‰ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ë˜í•œ, ì‹ ì²´ ì „ì²´ì˜ ì›€ì§ì„ ì¶”ì  ë° íšŒë³µ ì œì–´ë¥¼ ìœ„í•œ ë³´ì¡° RL ì •ì±…ì„ ë„ì…í•˜ì—¬ ë¬¼ë¦¬ì  íŒŒë¼ë¯¸í„°ì˜ ë³€í™”ì™€ ì™¸ë¶€ êµë€ì— ëŒ€ì‘í•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ANYmal B ë° ANYmal C ë¡œë´‡ í”Œë«í¼ì—ì„œ ì¬í•™ìŠµ ì—†ì´ë„ ì„±ê³µì ìœ¼ë¡œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n\nì´ ë…¼ë¬¸ì€ â€œProvable Partially Observable Reinforcement Learning with Privileged Informationâ€ ì—°êµ¬ì™€ ì—°ê²° ì§€ì–´ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì‹œë®¬ë ˆì´ì…˜ì—ì„œ íŠ¹ê¶Œ ì •ë³´(privileged information)ë¥¼ í™œìš©í•˜ì—¬ RL ì •ì±…ì„ í•™ìŠµí•˜ê³ , ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì œí•œëœ ì„¼ì„œ ì •ë³´ë§Œìœ¼ë¡œë„ ì•ˆì •ì ì¸ ë³´í–‰ì„ êµ¬í˜„í•˜ëŠ” ì ‘ê·¼ë²•ì€ ë‘ ì—°êµ¬ì˜ ê³µí†µëœ íŠ¹ì§•ì…ë‹ˆë‹¤. ì´ëŠ” íŠ¹ê¶Œ ì •ë³´ì˜ í™œìš©ì´ ê²°ì •ë¡ ì  í•„í„° ì¡°ê±´(deterministic filter condition)ì„ ë§Œì¡±í•˜ì§€ ì•Šë”ë¼ë„, ì ì ˆí•œ ì œì•½ ì¡°ê±´ì„ í†µí•´ ì•ˆì „í•œ ê°•í™”í•™ìŠµ ê¸°ë°˜ ë³´í–‰ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.\nì´ëŸ¬í•œ í†µì°°ì„ ë°”íƒ•ìœ¼ë¡œ, íŠ¹ê¶Œ ì •ë³´ë¥¼ í™œìš©í•œ êµì‚¬-í•™ìƒ í•™ìŠµ(teaching-student learning)ê³¼ ë¹„ëŒ€ì¹­ ì•¡í„°-í¬ë¦¬í‹±(asymmetric actor-critic) ë°©ë²•ì„ í†µí•´ ì‚¬ì¡±ë³´í–‰ ë¡œë´‡ì˜ ë³µì¡í•œ ì§€í˜•ì—ì„œì˜ ì•ˆì „í•œ ë³´í–‰ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì ì ˆí•œ ì œì•½ ì¡°ê±´ì„ ë„ì…í•˜ì—¬ ê°•í™”í•™ìŠµ ê¸°ë°˜ ë³´í–‰ ì»¨íŠ¸ë¡¤ëŸ¬ì˜ ì•ˆì •ì„±ê³¼ ì•ˆì „ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.\nPaper Link\nVideo\n\n\n\nSmall Home Robot\n\nLearning Quiet Walking for a Small Home Robot\n\n\nì§€ê¸ˆê¹Œì§€ì˜ ì—°êµ¬ëŠ” ê°•í™”í•™ìŠµì´ ì–¼ë§ˆë‚˜ ì œì–´ê¸°ë¥¼ ê°•ê±´í•˜ê²Œ ë§Œë“¤ì—ˆëŠëƒ, í˜¹ì€ ì–¼ë§ˆë‚˜ fancyí•œ ì›€ì§ì„ì„ ë§Œë“¤ì—ˆëŠëƒì— ì´ˆì ì„ ë‘ì–´ì™”ì—ˆìŒ. í•˜ì§€ë§Œ, ë³¸ ë…¼ë¬¸ì€ ì‹¤ì œ ê°€ì •ì— ë¡œë´‡ì´ íˆ¬ì…ì´ ë˜ì—ˆì„ ë•Œ ë¶ˆí¸í•¨ì„ ëŠë‚„ ê°€ëŠ¥ì„±ì´ ë†’ì€ â€œì†ŒìŒâ€ì— ì´ˆì ì„ ë§ì¶”ì–´ ì—°êµ¬ë¥¼ ì§„í–‰í–ˆë‹¤. ì´ì²˜ëŸ¼, ê¸°ìˆ ì ì¸ ì–´ë ¤ì›€ë³´ë‹¤ë„ ì‹¤ì œ ì‚¶ì— ìˆì–´ì„œ ë„ì›€ì´ ë ë§Œí•œ ê¸°ìˆ ì„ ì—°êµ¬í•˜ëŠ” ê²ƒ ë˜í•œ ì¢‹ì€ ë°©í–¥ìœ¼ë¡œ ë³´ì¸ë‹¤.\nê¸°ìˆ ì ì¸ ì–´ë ¤ì›€ì€ í¬ê²Œ ì—†ìœ¼ë©° curriculum learningì„ í†µí•´ 1st phaseì—ì„œëŠ” ì†ŒìŒì„ ì‹ ê²½ì“°ì§€ ì•ŠëŠ” ëª¨ì…˜ì„ í•™ìŠµí•˜ê³ , 2nd phaseì—ì„œ ì¡°ìš©í•œ ëª¨ì…˜ì„ í•™ìŠµí•˜ë„ë¡ ë§Œë“ ë‹¤. ë„¤íŠ¸ì›Œí¬ì˜ outputì€ â€œPD Gainì˜ Scaleâ€ê³¼ â€œ Target Joints Positionâ€ì„ ë‚´ë³´ë‚´ stiffí•œ ëª¨ì…˜ì„ ë§Œë“¤ì§€ damping ìˆëŠ” ëª¨ì…˜ì„ ë§Œë“¤ì§€ ì¡°ì •í•œë‹¤.\në³¸ ì‹¤í—˜(slope ê·¹ë³µì‹¤í—˜)ì— ë”°ë¥´ë©´ ì œì–´ê¸°ì˜ ê°•ê±´ë„ì™€ ì¡°ìš©í•¨ì´ ë°˜ë¹„ë¡€ ê´€ê³„ì„ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. ë¯¸ë˜ì—ëŠ” perceptive sensorë¥¼ í†µí•´ ì¢€ ë” ê³ ë„í™”ëœ ë³´í–‰ì „ëµì„ ì·¨í•˜ë„ë¡ ë§Œë“¤ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë˜í•œ, ë°œìêµ­ ì‚¬ìš´ë“œì™€ ê´€ë ¨ì´ìˆëŠ” ë°œ ì ‘ì´‰ ì†ë„ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°„ì ‘ rewardë¥¼ ì¤Œìœ¼ë¡œì¨ ë¬¸ì œë¥¼ í•´ê²°í–ˆëŠ”ë° ì´ëŠ” í† í¬ë¥¼ ì¤„ì´ë¯€ë¡œì¨ ë°°í„°ë¦¬ íš¨ìœ¨ì„±ì„ ì¦ê°€ì‹œí‚¤ëŠ” ë°©ì‹ ë“±ìœ¼ë¡œ í™œìš©ì´ ê°€ëŠ¥í•  ê²ƒì´ë‹¤.\nPaper Link\nYoutube\n\n\n\n\nMI-HGNN\n\nMorphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception\n\n\nMI-HGNNì€ ë¡œë´‡ì˜ ê´€ì ˆê³¼ ë§í¬ë¥¼ ê°ê° ë…¸ë“œì™€ ì—£ì§€ë¡œ ëª¨ë¸ë§í•˜ì—¬, ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ 8.4% ì„±ëŠ¥ í–¥ìƒê³¼ íƒì›”í•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±, ì¼ë°˜í™” ë° ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•¨\në¡œë´‡ì˜ í˜•íƒœí•™(morphology) ì •ë³´ë¥¼ ë°˜ì˜í•œ ì´ì¢… ê·¸ë˜í”„ ì‹ ê²½ë§(Heterogeneous Graph Neural Network) ì•„í‚¤í…ì²˜ì— ëª¨ë¸ ê¸°ë°˜ ì œì•½ì„ í†µí•©í•˜ì—¬, ì‹¤ì œì™€ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì—ì„œ ì ‘ì´‰ ì¸ì‹ ë¬¸ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°\nì´ì „ì— ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ë¡œ í–ˆì—ˆë˜ GNN ì•„ì´ë””ì–´ëŠ” ë‹¨ìˆœíˆ ë³´í–‰í•˜ëŠ” rl policyì— ê°„ì ‘ì ì¸ ì •ë³´ë¥¼ ë„£ì–´ì£¼ëŠ” ê±¸ë¡œ ì§„í–‰í–ˆì—ˆëŠ”ë° ì´ ë…¼ë¬¸ì—ì„œ GNNì€ ë³´ë‹¤ ì§ì ‘ì ìœ¼ë¡œ ground reaction forces (GRFs)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•™ìŠµì„ ì‹œì¼°ë‹¤ëŠ” ì ì´ ìƒˆë¡œì› ìœ¼ë©° ì œì–´í•˜ëŠ”ë° GRF ì •ë³´ê°€ í•„ìš”í•  ë•Œ í•œ ëª¨ë“ˆë¡œ ì´ìš©í•´ë³´ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆìŒ\nPaper Link"
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html",
    "href": "post/study/2025-02-17-mpc.html",
    "title": "Linear MPC",
    "section": "",
    "text": "This post is sourced from pympc-quadruped and translated into Korean."
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#dynamic-constraints",
    "href": "post/study/2025-02-17-mpc.html#dynamic-constraints",
    "title": "Linear MPC",
    "section": "1. Dynamic Constraints",
    "text": "1. Dynamic Constraints\n\na. Approximated Angular Velocity Dynamics\nThe robotâ€™s orientation is expressed as a vector of Z-Y-X Euler angles \\(\\Theta = [\\phi, \\theta, \\psi]^\\text{T}\\), where\n\n\\(\\psi\\) is the yaw,\n\\(\\theta\\) is the pitch, and\n\\(\\phi\\) is the roll.\n\nThese angles correspond to a sequence of rotations such that the transform from body to world coordinates can be expressed as \\[\n\\mathbf{R} = \\mathbf{R}_z(\\psi)\\mathbf{R}_y(\\theta)\\mathbf{R}_x(\\phi)\n\\]\nwhere \\(\\mathbf{R}_n(\\alpha)\\) represents a positive rotation of \\(\\alpha\\) about the \\(n\\)-axis. In details, we can write\n\\[\n\\mathbf{R}_z(\\psi) = \\begin{bmatrix}\n    \\cos\\psi & -\\sin \\psi & 0 \\\\\n    \\sin\\psi & \\cos \\psi  & 0 \\\\\n    0        & 0          & 1\n\\end{bmatrix},\n\\mathbf{R}_y(\\theta) = \\begin{bmatrix}\n    \\cos\\theta  & 0 & \\sin\\theta \\\\\n    0           & 1 & 0          \\\\\n    -\\sin\\theta & 0 & \\cos\\theta\n\\end{bmatrix},\n\\mathbf{R}_z(\\psi) = \\begin{bmatrix}\n    \\cos\\psi & -\\sin \\psi & 0 \\\\\n    \\sin\\psi & \\cos \\psi  & 0 \\\\\n    0        & 0          & 1\n\\end{bmatrix}\n\\]\nFrom [2], we have known that \\(\\dot{\\mathbf{R}} = [\\mathbf{\\omega}]\\mathbf{R}\\), where \\(\\mathbf{\\omega} \\in \\mathbb{R}^3\\) is the robotâ€™s angular velocity, \\([\\mathbf{\\omega}] \\in \\mathbb{R}^{3\\times3}\\) is defined as the skew-symmetric matrix with respect to \\(\\mathbf{\\omega}\\), and \\(\\mathbf{R}\\) is the rotation matrix which transforms from body to world coordinates. Then the angular velocity in world coordinates can be found with\n\\[\n\\begin{aligned}\n[\\mathbf{\\omega}] &= \\dot{\\mathbf{R}}\\mathbf{R}^{-1} = \\dot{\\mathbf{R}}\\mathbf{R}^\\text{T} \\\\\n&= \\left( \\frac{\\partial R}{\\partial \\psi} + \\frac{\\partial R}{\\partial \\theta} + \\frac{\\partial R}{\\partial \\phi} \\right)\\mathbf{R}^\\text{T} \\\\\n&= \\begin{bmatrix}\n    0 & \\dot{\\phi} \\sin\\theta - \\dot{\\psi} & \\dot{\\theta}\\cos\\psi + \\dot{\\phi}\\cos\\theta\\sin\\psi \\\\\n    -\\dot{\\phi} \\sin\\theta + \\dot{\\psi} & 0 & \\dot{\\theta}\\sin\\psi - \\dot{\\phi}\\cos\\psi\\cos\\theta \\\\\n    -\\dot{\\theta}\\cos\\psi - \\dot{\\phi}\\cos\\theta\\sin\\psi & -\\dot{\\theta}\\sin\\psi + \\dot{\\phi}\\cos\\psi\\cos\\theta & 0\n\\end{bmatrix}\n\\end{aligned}\n\\]\nThe above result is easy to get using the MATLAB script.\nsyms psi theta phi real\nsyms psidot thetadot phidot real\n\nRz = [cos(psi), -sin(psi), 0;\n      sin(psi), cos(psi),  0;\n      0,        0,         1];\n\nRy = [cos(theta),  0, sin(theta);\n      0,           1, 0;\n      -sin(theta), 0, cos(theta)];\n  \nRx = [1, 0,        0;\n      0, cos(phi), -sin(phi);\n      0, sin(phi), cos(phi)];\n  \nR = simplify(Rz*Ry*Rx);\n\nSomega = simplify((diff(R,phi)*phidot + diff(R,theta)*thetadot + diff(R,psi)*psidot) * R')\nNow we are ready to build connections between the angular velocity in world coordinates \\(\\mathbf{\\omega}\\) and the rate of change of Euler angles \\(\\dot{\\mathbf{\\Theta}} = [\\dot{\\phi}, \\dot{\\theta}, \\dot{\\psi}]^\\text{T}\\).\n\\[\n\\mathbf{\\omega} = \\begin{bmatrix}\n    -\\dot{\\theta}\\sin\\psi + \\dot{\\phi}\\cos\\psi\\cos\\theta \\\\\n    \\dot{\\theta}\\cos\\psi + \\dot{\\phi}\\cos\\theta\\sin\\psi \\\\\n    -\\dot{\\phi} \\sin\\theta + \\dot{\\psi}\n\\end{bmatrix}\n= \\begin{bmatrix}\n    \\cos\\theta\\cos\\psi & -\\sin\\psi & 0 \\\\\n    \\cos\\theta\\sin\\psi & \\cos\\psi  & 0 \\\\\n    -\\sin\\theta        & 0         & 1\n\\end{bmatrix}\\begin{bmatrix}\n    \\dot{\\phi} \\\\ \\dot{\\theta} \\\\ \\dot{\\psi}\n\\end{bmatrix} = \\mathbf{E}\\dot{\\mathbf{\\Theta}}\n\\]\nIf the robot is not pointed vertically, which means \\(\\cos\\theta \\neq 0\\), the matrix \\(\\mathbf{E}\\) is invertable. In such case, we can get\n\\[\n\\dot{\\mathbf{\\Theta}} = \\mathbf{E}^{-1}\\mathbf{\\omega} = \\begin{bmatrix}\n    \\frac{\\cos\\psi}{\\cos\\theta} & \\frac{\\sin\\psi}{\\cos\\theta} & 0 \\\\\n    -\\sin\\psi                   & \\cos\\psi                    & 0 \\\\\n    \\cos\\psi\\tan\\theta          & \\sin\\psi\\tan\\theta          & 1\n\\end{bmatrix}\\mathbf{\\omega}\n\\]\nFor small values of roll \\(\\phi\\) and pitch \\(\\theta\\), the above equation can be approximated as\n\\[\n\\dot{\\mathbf{\\Theta}} \\approx \\begin{bmatrix}\n    \\cos\\psi  & \\sin\\psi & 0 \\\\\n    -\\sin\\psi & \\cos\\psi & 0 \\\\\n    0         & 0        & 1\n\\end{bmatrix} \\mathbf{\\omega}\n\\]\nwhich is equivalent to\n\\[\n\\dot{\\mathbf{\\Theta}} \\approx \\mathbf{R}_z^\\text{T} \\mathbf{\\omega}\n\\tag{1}\n\\]\nNote that the order in which the Euler angle rotations are defined is important; with an alternate sequence of rotations, the approximation will be inaccurate for reasonable robot orientations.\n\n\nb. Simplified Single Rigid Body Model\nThe predictive controller models the robot as a single rigid body subject to forces at the contact patches. Although ignoring leg dynamics is a major simplification, the controller is still able to stabilize a high-DoF system and is robust to these multi-body effects.\nFor the Cheetah 3 robot, this simplification is reasonable: the mass of the legs is roughly 10% of the robot's total mass\nFor each ground reaction force \\(\\mathbf{f}_i \\in \\mathbb{R}^3\\), the vector from the CoM to the point where the force is applied is \\(\\mathbf{r}_i \\in \\mathbb{R}^3\\). The rigid body dynamics in world coordinates are given by\n\\[\n\\begin{aligned}\n\\ddot{\\mathbf{p}} &= \\frac{\\sum_{i=1}^n\\mathbf{f}_i}{m} - \\mathbf{g} \\\\\n\\frac{\\text{d}}{\\text{d}t}(\\mathbf{I\\omega}) &= \\sum_{i=1}^n \\mathbf{r}_i \\times \\mathbf{f}_i\\\\\n\\dot{\\mathbf{R}} &= [\\mathbf{\\omega}]\\mathbf{R}\n\\end{aligned} \\tag{2}\n\\]\nwhere \\(\\mathbf{p} \\in \\mathbb{R}^3\\) is the robotâ€™s position in world frame, \\(m \\in \\mathbb{R}\\) is the robotâ€™s mass, \\(\\mathbf{g} \\in \\mathbb{R}^3\\) is the acceleration of gravity, and \\(\\mathbf{I} \\in \\mathbb{R}^3\\) is the robotâ€™s inertia tensor.\n\n\në³€ìˆ˜ ì •ë¦¬\n\n\n\\(\\mathbf{f}\\): ì§€ë©´ ë°˜ë°œë ¥\n\\(\\mathbf{r}_i\\): ì§ˆëŸ‰ ì¤‘ì‹¬(CoM)ì—ì„œ í˜ì´ ì‘ìš©í•˜ëŠ” ì§€ì ê¹Œì§€ì˜ ë²¡í„°\n\\(\\mathbf{p}\\): world frameì—ì„œ robot position\n\n\n\nThe nonlinear dynamics in the second and third equation of (2) motivate the approximations to avoid the nonconvex optimization that would otherwise be required for model predictive control.\n(2)ì˜ ë‘ ë²ˆì§¸ì™€ ì„¸ ë²ˆì§¸ ë°©ì •ì‹ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë¹„ì„ í˜• ë™ì—­í•™ì€, ëª¨ë¸ ì˜ˆì¸¡ ì œì–´ì—ì„œ ìš”êµ¬ë˜ëŠ” nonconvex ìµœì í™”ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ê·¼ì‚¬í™”ë¥¼ ë„ì…í•´ì•¼ í•  í•„ìš”ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.\n\n\n2ë²ˆì§¸ ì‹\nThe second equation in (2) can be approximated with:\n\\[\n\\frac{\\text{d}}{\\text{d}t}(\\mathbf{I\\omega}) = \\mathbf{I\\dot{\\omega}} + \\omega \\times (\\mathbf{I\\omega}) \\approx \\mathbf{I\\dot{\\omega}} = \\sum_{i=1}^n \\mathbf{r}_i \\times \\mathbf{f}_i \\tag{3}\n\\]\nThis approximation has been made in other peopleâ€™s work. The \\(\\omega \\times (\\mathbf{I\\omega})\\) term is small for bodies with small angular velocities and does not contribute significantly to the dynamics of the robot. The inertia tensor in the world coordinate system can be found with\nì´ ê·¼ì‚¬ëŠ” ë‹¤ë¥¸ ì—°êµ¬ì—ì„œë„ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. \\(\\omega \\times (\\mathbf{I\\omega})\\) í•­ì€ ì‘ì€ ê°ì†ë„ë¥¼ ê°€ì§€ëŠ” ë¬¼ì²´ì—ì„œëŠ” ì‘ìœ¼ë©°, ë¡œë´‡ì˜ ë™ì—­í•™ì— í¬ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì›”ë“œ ì¢Œí‘œê³„ì—ì„œì˜ inertia tensorëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\n\\mathbf{I} = \\mathbf{R}\\mathbf{I}_{\\mathcal{B}}\\mathbf{R}^\\text{T}\n\\]\nwhere \\(\\mathbf{I}_\\mathcal{B}\\) is the inertia tensor in body coordinates. For small roll and pitch angles, This can be approximated by\n\\[\n\\mathbf{\\hat{I}} = \\mathbf{R}_z(\\psi)\\mathbf{I}_{\\mathcal{B}}\\mathbf{R}_z(\\psi)^\\text{T}\n\\tag{4}\n\\]\nwhere \\(\\mathbf{\\hat{I}}\\) is the approximated robotâ€™s inertia tensor in world frame. Combining equations (3)(4), we get\n\\[\n\\mathbf{\\dot{\\omega}} = \\mathbf{\\hat{I}}^{-1}\\sum_{i=1}^n \\mathbf{r}_i \\times \\mathbf{f}_i = \\mathbf{\\hat{I}}^{-1}\\sum_{i=1}^n [\\mathbf{r}_i] \\mathbf{f}_i\n\\tag{5}\n\\]\n\n\n3ë²ˆì§¸ ì‹\nFor the third equation of (2), we have made the approximation in section 1.a, which gives us \\[\n\\dot{\\mathbf{\\Theta}} \\approx \\mathbf{R}_z^\\text{T} \\mathbf{\\omega}\n\\]\n\n\nc.Â Continuous-Time State Space Model\nFrom the discussion above, we can write the simplified single rigid body model using equations (1)(2)\n\\[\n\\begin{aligned}\n\\dot{\\mathbf{\\Theta}} &= \\mathbf{R}_z^\\text{T} \\mathbf{\\omega} \\\\\n\\dot{\\mathbf{p}} &= \\dot{\\mathbf{p}} \\\\\n\\mathbf{\\dot{\\omega}} &= \\mathbf{\\hat{I}}^{-1}\\sum_{i=1}^n [\\mathbf{r}_i] \\mathbf{f}_i \\\\\n\\ddot{\\mathbf{p}} &= \\frac{\\sum_{i=1}^n\\mathbf{f}_i}{m} - \\mathbf{g}\n\\end{aligned}\n\\tag{6}\n\\]\nIn matrix form:\n\\[\n\\begin{bmatrix}\n    \\mathbf{\\dot{\\Theta}} \\\\ \\mathbf{\\dot{p}} \\\\ \\mathbf{\\dot{\\omega}} \\\\ \\mathbf{\\ddot{p}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{R}_z^\\text{T} & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{I}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{\\Theta} \\\\ \\mathbf{p} \\\\ \\mathbf{\\omega} \\\\ \\mathbf{\\dot{p}}\n\\end{bmatrix} +\n\\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1]\\\\\n    \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{f}_1 \\\\ \\mathbf{f}_2 \\\\ \\mathbf{f}_3 \\\\ \\mathbf{f}_4\n\\end{bmatrix} +\n\\begin{bmatrix}\n    \\mathbf{0}_{31} \\\\ \\mathbf{0}_{31} \\\\ \\mathbf{0}_{31} \\\\ -\\mathbf{g}\n\\end{bmatrix}\n\\]\nThis equation can be rewritten with an additional gravity state \\(g\\) (note that here \\(g\\) is a scalar) to put the dynamics into the convenient state-space form: \\[\n\\dot{\\mathbf{x}}(t) = \\mathbf{A_c}(\\psi)\\mathbf{x}(t) + \\mathbf{B_c}(\\mathbf{r}_1, \\cdots, \\mathbf{r}_4, \\psi)\\mathbf{u}(t)\n\\tag{7}\n\\]\nwhere \\(\\mathbf{A_c} \\in \\mathbb{R}^{13\\times13}\\) and \\(\\mathbf{B_c} \\in \\mathbb{R}^{13\\times12}\\).\nIn details, we have \\[\n\\begin{bmatrix}\n    \\mathbf{\\dot{\\Theta}} \\\\ \\mathbf{\\dot{p}} \\\\ \\mathbf{\\dot{\\omega}} \\\\ \\mathbf{\\ddot{p}} \\\\ -\\dot{g}\n\\end{bmatrix} = \\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{R}_z^\\text{T} & \\mathbf{0}_3 & 0\\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{I}_3 & 0\\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3 & 0\\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3          & \\mathbf{0}_3 & \\mathbf{e}_z\\\\\n    0            & 0            & 0                     & 0            & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{\\Theta} \\\\ \\mathbf{p} \\\\ \\mathbf{\\omega} \\\\ \\mathbf{\\dot{p}} \\\\ -g\n\\end{bmatrix} +\n\\begin{bmatrix}\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 & \\mathbf{0}_3 \\\\\n    \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1] & \\mathbf{\\hat{I}}^{-1}[\\mathbf{r}_1]\\\\\n    \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} & \\frac{\\mathbf{I}_3}{m} \\\\\n    0 & 0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{f}_1 \\\\ \\mathbf{f}_2 \\\\ \\mathbf{f}_3 \\\\ \\mathbf{f}_4\n\\end{bmatrix}\n\\tag{8}\n\\]\nThis form depends only on yaw and footstep locations. If these can be computed ahead of time, the dynamics become linear time-varying, which is suitable for convex model predictive control.\nì´ í˜•íƒœëŠ” ì˜¤ì§ yawì™€ ë°œê±¸ìŒ ìœ„ì¹˜ì—ë§Œ ì˜ì¡´í•©ë‹ˆë‹¤. ë§Œì•½ ì´ê²ƒë“¤ì´ ë¯¸ë¦¬ ê³„ì‚°ë  ìˆ˜ ìˆë‹¤ë©´, ë™ì—­í•™ì€ ì„ í˜• ì‹œê°„ ë³€í™” í˜•íƒœê°€ ë˜ì–´ convex model predictive controlì— ì í•©í•´ì§‘ë‹ˆë‹¤.\n\n\nd.Â Discretization\nSee https://en.wikipedia.org/wiki/Discretization for more details about the purposed method.\n\\[\n\\exp\\left(\\begin{bmatrix}\n          \\mathbf{A_c} & \\mathbf{B_c} \\\\\n          \\mathbf{0}   & \\mathbf{0}\n          \\end{bmatrix} \\text{dt}\\right) =\n          \\begin{bmatrix}\n          \\mathbf{A_d} & \\mathbf{B_d} \\\\\n          \\mathbf{0}   & \\mathbf{I}\n          \\end{bmatrix} \\tag{9}\n\\]\nThis allows us to express the dynamics in the discrete time form\n\\[\n\\mathbf{x}[k+1] = \\mathbf{A_d} \\mathbf{x}[k] + \\mathbf{B_d}[k]\\mathbf{u}[k]\n\\tag{10}\n\\]\nThe above approximation is only accurate if the robot is able to follow the reference trajectory. Large deviations from the reference trajectory, possibly caused by external or terrain disturbances, will result in \\(\\mathbf{B_d}[k]\\) being inaccurate. However, for the first time step, \\(\\mathbf{B_d}[k]\\) is calculated from the current robot state, and will always be correct. If, at any point, the robot is disturbed from following the reference trajectory, the next iteration of the MPC, which happens at most 40 ms after the disturbance, will recompute the reference trajectory based on the disturbed robot state, allowing it compensate for a disturbance.\nìœ„ ê·¼ì‚¬ëŠ” ë¡œë´‡ì´ reference trajectoryì„ ë”°ë¼ê°ˆ ìˆ˜ ìˆì„ ë•Œë§Œ ì •í™•í•©ë‹ˆë‹¤. ì™¸ë¶€ ìš”ì¸ì´ë‚˜ ì§€í˜•ì˜ ë°©í•´ë¡œ ì¸í•´ reference trajectoryì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ëŠ” ê²½ìš°, \\(\\mathbf{B_d}[k]\\)ê°€ ë¶€ì •í™•í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nê·¸ëŸ¬ë‚˜ first timestepì—ì„œëŠ” \\(\\mathbf{B_d}[k]\\)ê°€ í˜„ì¬ ë¡œë´‡ ìƒíƒœì—ì„œ ê³„ì‚°ë˜ë¯€ë¡œ í•­ìƒ ì •í™•í•©ë‹ˆë‹¤. ì–´ë–¤ ì‹œì ì—ì„œë“  ë¡œë´‡ì´ ì°¸ì¡° ê¶¤ì ì—ì„œ ë²—ì–´ë‚˜ ë°©í•´ë¥¼ ë°›ëŠ”ë‹¤ë©´, ë°©í•´ê°€ ë°œìƒí•œ í›„ ìµœëŒ€ 40ms ì´ë‚´ì— ì‹¤í–‰ë˜ëŠ” MPCì˜ ë‹¤ìŒ ë°˜ë³µì—ì„œ, ë°©í•´ë¥¼ ë°›ì€ ë¡œë´‡ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ reference trajectoryì„ ì¬ê³„ì‚°í•˜ì—¬ ë°©í•´ë¥¼ ë³´ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#force-constraints",
    "href": "post/study/2025-02-17-mpc.html#force-constraints",
    "title": "Linear MPC",
    "section": "2. Force Constraints",
    "text": "2. Force Constraints\n\na. Equality Constraints(ë“±ì‹ì¡°ê±´)\nThe equality constraint\n\\[\n\\mathbf{D}_k \\mathbf{u}_k = \\mathbf{0} \\tag{11}\n\\]\nis used to set all forces from feet off the ground to zero, enforcing the desired gait, where \\(\\mathbf{D}_k\\) is a matrix which selects forces corresponding with feet not in contact with the ground at timestep \\(k\\).\nì§€ë©´ì—ì„œ ë–¨ì–´ì§„ ë°œë“¤ì— ì‘ìš©í•˜ëŠ” ëª¨ë“  í˜ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì›í•˜ëŠ” gaitì„ ê°•ì œí•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ \\(\\mathbf{D}_k\\)ëŠ” ì‹œê°„ ë‹¨ê³„ \\(k\\)ì—ì„œ ì§€ë©´ì— ì ‘ì´‰í•˜ì§€ ì•Šì€ ë°œë“¤ì— í•´ë‹¹í•˜ëŠ” í˜ì„ ì„ íƒí•˜ëŠ” í–‰ë ¬ì…ë‹ˆë‹¤.\n\n\nb. Inequality Constraints(ë¶€ë“±ì‹ì¡°ê±´)\nThe inequality constraints limit the minimum and maximum \\(z\\)-force as well as a square pyramid approximation of the friction cone.\në¶€ë“±ì‹ ì œì•½ ì¡°ê±´ì€ ìµœì†Œ ë° ìµœëŒ€ \\(z\\)-í˜ì„ ì œí•œí•˜ë©°, friction coneì˜ ì‚¬ê°ë¿” ê·¼ì‚¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\nFor each foot, we have the following 10 inequality constraints (\\(i = 1,2,3,4\\)).\n\\[\n\\begin{aligned}\nf_{\\min} \\leq &f_{i,z} \\leq f_{\\max} \\\\\n-\\mu f_{i,z} \\leq  &f_{i,x} \\leq \\mu f_{i,z} \\\\\n-\\mu f_{i,z} \\leq  &f_{i,y} \\leq \\mu f_{i,z}\n\\end{aligned}\n\\]\nWe want to write these constraints in matrix form. Thus we need to look these equations in detail.\nFor example, the constraints \\(-\\mu f_{i,z} \\leq \\pm f_{i,x} \\leq \\mu f_{i,z}\\) actually are \\[\n\\begin{aligned}\n-\\mu f_{i,z} &\\leq f_{i,x} \\\\\n-\\mu f_{i,z} &\\leq -f_{i,x} \\\\\nf_{i,x} &\\leq \\mu f_{i,z} \\\\\n-f_{i,x} &\\leq \\mu f_{i,z}\n\\end{aligned}\n\\]\nWe can rewrite these equations as\n\\[\n\\begin{aligned}\nf_{i,x} + \\mu f_{i,z} &\\geq 0 \\\\\n-f_{i,x} + \\mu f_{i,z} &\\geq 0 \\\\\nf_{i,x} - \\mu f_{i,z} &\\leq 0 \\\\\n-f_{i,x} - \\mu f_{i,z} &\\leq 0\n\\end{aligned}\n\\]\nWe can see that the first two equations and the last two equations are the same. Thus we can use the first two equations to replace these four equations. We can also note that \\(f_{\\min} = 0\\) as always. With the observation discussed above, we can write the force constraints for one foot on the ground as\n\\[\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n\\leq\n\\begin{bmatrix}\n1 & 0 & \\mu \\\\\n-1 & 0 & \\mu \\\\\n0 & 1 & \\mu \\\\\n0 & -1 & \\mu \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{i,x} \\\\ f_{i,y} \\\\ f_{i,z}\n\\end{bmatrix}\n\\leq\n\\begin{bmatrix}\n\\infty \\\\ \\infty \\\\ \\infty \\\\ \\infty \\\\ f_{\\max}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#reference-trajectory-generation",
    "href": "post/study/2025-02-17-mpc.html#reference-trajectory-generation",
    "title": "Linear MPC",
    "section": "3. Reference Trajectory Generation",
    "text": "3. Reference Trajectory Generation\nThe desired robot behavior is used to construct the reference trajectory. In the application, our reference trajectories are simple and only contain non-zero \\(xy\\)-velocity, \\(xy\\)-position, \\(z\\)-postion, yaw, and yaw rate. All parameters are commanded directly by the robot operator except for yaw and \\(xy\\)-position, which are determined by integrating the appropriate velocities. The other states (roll, pitch, roll rate, pitch rate and \\(z\\)-velocity) are always set to 0. The reference trajectory is also used to determine the dynamics constraints and future foot placement locations.\nIn practice, the reference trajectory is short (between 0.5 and 0.3 seconds) and recalculated often (every 0.05 to 0.03 seconds) to ensure the simplified dynamics remain accurate if the robot is disturbed."
  },
  {
    "objectID": "post/study/2025-02-17-mpc.html#qp-formulation",
    "href": "post/study/2025-02-17-mpc.html#qp-formulation",
    "title": "Linear MPC",
    "section": "4. QP Formulation",
    "text": "4. QP Formulation\n\na. Batch Formulation for Dynamic Constraints\nKey idea: For the state space model, express \\(x_0, x_1, \\cdots, x_k\\) as function of \\(u_0\\).\n\\[\n\\begin{aligned}\nx_1 &= Ax_0 + Bu_0 \\\\\nx_2 &= Ax_1 + Bu_1 = A^2x_0 + ABu_0 + Bu_1 \\\\\n&\\vdots \\\\\nx_k & = A^kx_0 + A^{k-1}Bu_0 + A^{k-2}Bu_1 + \\cdots + Bu_{k-1}\n\\end{aligned}\n\\]\nWe can write these equations in matrix form\n\\[\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k\n\\end{bmatrix} =\n\\begin{bmatrix}\nA \\\\ A^2 \\\\ \\vdots \\\\A^k\n\\end{bmatrix}\nx_0\n+\n\\begin{bmatrix}\nB & 0 & \\cdots & 0 \\\\\nAB & B & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA^{k-1}B & A^{k-2}B & \\cdots & B\n\\end{bmatrix}\n\\begin{bmatrix}\nu_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{k-1}\n\\end{bmatrix}\n\\]\nwhere \\(k\\) is the horizon length. Let \\(x_t\\) denotes the system state at time step \\(t\\), i.e.Â at current state, then we can write\n\\[\n\\begin{bmatrix}\nx_{t+1} \\\\ x_{t+2} \\\\ \\vdots \\\\ x_{t+k}\n\\end{bmatrix} =\n\\begin{bmatrix}\nA \\\\ A^2 \\\\ \\vdots \\\\A^k\n\\end{bmatrix}\nx_t\n+\n\\begin{bmatrix}\nB & 0 & \\cdots & 0 \\\\\nAB & B & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA^{k-1}B & A^{k-2}B & \\cdots & B\n\\end{bmatrix}\n\\begin{bmatrix}\nu_t \\\\ u_{t+1} \\\\ \\vdots \\\\ u_{t+k-1}\n\\end{bmatrix}\n\\]\nWe can denote this equation as\n\\[\nX = S^x x_t + S^u U\n\\]\nFor a 2-norm cost function, we can write\n\\[\n\\begin{aligned}\nJ_k(x_t,\\mathbf{U}) &= J_f(x_k) + \\sum_{i=0}^{k-1}l(x_i,u_i) \\\\\n&= x_k^\\text{T} Q_f x_k + \\sum_{i=0}^{k-1}\\left(x_i^\\text{T}Q_ix_i + u_i^\\text{T}R_iu_i\\right) \\\\\n&= X^\\text{T}\\bar{Q}X + U^\\text{T}\\bar{R}U\n\\end{aligned}\n\\]\nwhere\n\\[\n\\mathbf{U} =\n\\begin{bmatrix}\n    X \\\\ U\n\\end{bmatrix} \\quad\n\\bar{Q} = \\begin{bmatrix}\n    Q_1 & & & \\\\\n     & \\ddots & & \\\\\n     & & Q_{k-1} & \\\\\n     & & & Q_f\n\\end{bmatrix} \\quad\n\\bar{R} = \\begin{bmatrix}\n    R & & \\\\\n     & \\ddots & \\\\\n     & & R \\\\\n\\end{bmatrix}\n\\]\nSubstituting the expression of state space model into the cost, we have\n\\[\n\\begin{aligned}\nJ_k(x_t,\\mathbf{U}) &= X^\\text{T}\\bar{Q}X + U^\\text{T}\\bar{R}U \\\\\n&= \\left(S^x x_t + S^u U\\right)^\\text{T}\\bar{Q}\\left(S^x x_t + S^u U\\right) + U^\\text{T}\\bar{R}U \\\\\n&= U^\\text{T}\\underbrace{\\left((S^u)^\\text{T}\\bar{Q}S^u+\\bar{R}\\right)}_H U + 2x_t^\\text{T}\\underbrace{(S^x)^\\text{T}\\bar{Q}S^u}_F U + x_t^\\text{T}\\underbrace{(S^x)^\\text{T}\\bar{Q}S^x}_Y x_t\\\\\n&= U^\\text{T}HU + 2x_tFU + x_t^\\text{T}Yx_t\n\\end{aligned}\n\\] Compare with the standard form of cost in QP formulation \\(\\frac{1}{2}U^\\text{T}HU + U^\\text{T}g\\), we can easily get \\[\n\\begin{aligned}\nH &= 2\\left((S^u)^\\text{T}\\bar{Q}S^u + \\bar{R}\\right) \\\\\ng &= 2(S^u)^\\text{T}\\bar{Q}S^xx_t\n\\end{aligned}\n\\]\n\n\nb. Create QP Cost\nRecall that the form of our MPC problem is\n\\[\n\\begin{aligned}\n\\min_{x, u} \\quad& \\sum_{i=0}^{k-1}(x_{i+1}-x_{i+1,\\text{ref}})^{\\text{T}}Q_i(x_{i+1}-x_{i+1,\\text{ref}}) + u_i^\\text{T}R_iu_i \\\\\n\\text{s.t.} \\quad& x_{i+1} = A_ix_i + B_iu_i \\\\\n& \\underline{c}_i \\leq C_iu_i \\leq \\overline{c}_i \\\\\n& D_iu_i = 0\n\\end{aligned}\n\\]\nwhere \\(i = 0, \\cdots, k-1\\).\nIn this project, we choose \\(Q_1 = \\cdots = Q_{k-1} = Q_f\\), i.e.\n\\[\n\\bar{Q} = \\begin{bmatrix}\n    Q & & & \\\\\n     & \\ddots & & \\\\\n     & & Q & \\\\\n     & & & Q\n\\end{bmatrix} \\quad\n\\bar{R} = \\begin{bmatrix}\n    R & & \\\\\n     & \\ddots & \\\\\n     & & R \\\\\n\\end{bmatrix}\n\\]\n_Qi = np.diag(np.array(parameters['Q'], dtype=float))\nQbar = np.kron(np.identity(horizon), _Qi)\n\n_r = parameters['R']\n_Ri = _r * np.identity(num_input, dtype=float)\nRbar = np.kron(np.identity(horizon), _Ri)\nLetâ€™s substitute the dynamic constraint into the cost function, then we can get\n\\[\n\\begin{aligned}\nJ &= \\sum_{i=0}^{k-1}(x_{i+1}-x_{i+1,\\text{ref}})^{\\text{T}}Q_i(x_{i+1}-x_{i+1,\\text{ref}}) + u_i^\\text{T}R_iu_i \\\\\n&= \\sum_{i=0}^{k-1}(A_ix_i+B_iu_i-x_{i+1,\\text{ref}})^{\\text{T}}Q_i(A_ix_i + B_iu_i-x_{i+1,\\text{ref}}) + u_i^\\text{T}R_iu_i \\\\\n&= (S^x x_t + S^u U - X_{\\text{ref}})^\\text{T}\\bar{Q}(S^x x_t + S^u U - X_\\text{ref}) + U^\\text{T}\\bar{R}U\n\\end{aligned}\n\\]\nwhere \\(X_\\text{ref} = [x_{1,\\text{ref}}, \\cdots, x_{k,\\text{ref}}]^\\text{T}\\). We can get the QP matrix with the same procedure.\n\\[\n\\begin{aligned}\nH &= 2\\left((S^u)^\\text{T}\\bar{Q}S^u + \\bar{R}\\right) \\\\\ng &= 2(S^u)^\\text{T}\\bar{Q}(S^xx_t-X_\\text{ref})\n\\end{aligned}\n\\]\n\n\nc.Â Create QP Constraint\n\n\nd.Â QP Solver\nqpsolvers: https://pypi.org/project/qpsolvers/\nAs mentioned in the project description of qpsolvers, we need to write our formulation in the form\n\\[\n\\begin{aligned}\n\\min_x &\\quad \\frac{1}{2}x^\\text{T}Px + q^\\text{T}x \\\\\n\\text{s.t.} & \\quad Gx \\leq h \\\\\n&\\quad Ax = b \\\\\n&\\quad \\text{lb} \\leq x \\leq \\text{ub}\n\\end{aligned}\n\\]\nThen we can get the solution using the code below\nfrom qpsolvers import solve_qp\nx = solve_qp(P, q, G, h, A, b, lb, ub)\nprint(\"QP solution: x = {}\".format(x))\nThe QP problem in the paper is written as\n\\[\n\\begin{aligned}\n\\min_\\mathbf{U} &\\quad \\frac{1}{2}\\mathbf{U}^\\text{T}\\mathbf{HU} + \\mathbf{U}^\\text{T}\\mathbf{g} \\\\\n\\text{s.t.} &\\quad \\underline{\\mathbf{c}} \\leq \\mathbf{CU} \\leq \\overline{\\mathbf{c}}\n\\end{aligned}\n\\]\n\n\nReference\n[1] Di Carlo J., Wensing P. M., Katz B., et al.Â Dynamic Locomotion in the MIT Cheetah 3 Through Convex Model-Predictive Control[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018: 1-9. [PDF]\n[2] Bledt G, Powell M J, Katz B, et al.Â Mit cheetah 3: Design and Control of a Robust, Dynamic Quadruped Robot[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018: 2245-2252. [PDF]"
  },
  {
    "objectID": "post/study/2025-03-01-review.html#height-sample-randomization",
    "href": "post/study/2025-03-01-review.html#height-sample-randomization",
    "title": "Learning robust perceptive locomotion for quadrupedal robots in the wild",
    "section": "Height sample randomization",
    "text": "Height sample randomization\n\nStudent Policy ë°©ë²•ë¡  ë” ìì„¸íˆ\n\nğŸ‘‰ í•µì‹¬ ì•„ì´ë””ì–´\n- ì‹¤ì œ í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì„¼ì„œ ì˜¤ë¥˜ ë° ë§µ í’ˆì§ˆ ì €í•˜ë¥¼ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜\n- Noise ì ìš© ë°©ì‹ì„ ì„¸ë¶„í™”í•˜ì—¬ ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ ì •ì±…ì´ ê°•ê±´í•˜ê²Œ ì‘ë™í•˜ë„ë¡ ìœ ë„\n- ì§€í˜• íŠ¹ì„± ë³€í™”ë„ ë°˜ì˜í•˜ì—¬ í˜„ì‹¤ì ì¸ ì§€í˜• ì ì‘ ëŠ¥ë ¥ ê°•í™”\n\nNoise ëª¨ë¸ ì ìš©\n\n\nNoise ëª¨ë¸ \\(n(\\tilde{o}^e_t | o^e_t, z)\\), \\(z \\in \\mathbb{R}^{8 \\times 4}\\) ì‚¬ìš©\në†’ì´ ìƒ˜í”Œì— ë‘ ê°€ì§€ ìœ í˜•ì˜ ì¸¡ì • Noise ì¶”ê°€\n\nScan Pointë¥¼ ìˆ˜í‰ìœ¼ë¡œ ì´ë™(Shift)\n\në†’ì´ ê°’ì— êµë€ ì¶”ê°€(Height Disturbance)\n\n\nNoise ê°’ì€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ ì—ì„œ ìƒ˜í”Œë§ë˜ë©°, Noise ë§¤ê°œë³€ìˆ˜ \\(z\\) ê°€ ë¶„ì‚°ì„ ê²°ì •\n\nNoise ë§¤ê°œë³€ìˆ˜ ë²¡í„° \\(z\\) ëŠ” ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì˜ ì¼ë¶€\n\ní›ˆë ¨ ê¸°ê°„ì´ ì§„í–‰ë¨ì— ë”°ë¼ Noise ê°•ë„ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€\n\n\n\n\n\n\nNoiseëŠ” 3ê°€ì§€ ë‹¤ë¥¸ ë²”ìœ„ ì—ì„œ ì ìš©\n\nPer-Scan-Point â†’ ê° ìŠ¤ìº” í¬ì¸íŠ¸ë§ˆë‹¤ ë§¤ ì‹œê°„ ë‹¨ê³„ë§ˆë‹¤ ì¬ìƒ˜í”Œë§\nPer-Foot â†’ ê° ë‹¤ë¦¬(ë°œ)ë³„ë¡œ ë§¤ ì‹œê°„ ë‹¨ê³„ë§ˆë‹¤ ì¬ìƒ˜í”Œë§\nPer-Episode â†’ ëª¨ë“  ìŠ¤ìº” í¬ì¸íŠ¸ì— ëŒ€í•´ ì¼ì •í•œ Noise ìœ ì§€\n\n\në§¤í•‘ ì˜¤ë¥˜ ì‹œë®¬ë ˆì´ì…˜\n\në§µ í’ˆì§ˆ ë³€í™” ë° ì˜¤ë¥˜ ì›ì¸ ì‹œë®¬ë ˆì´ì…˜\n3ê°€ì§€ ë§¤í•‘ ì¡°ê±´ ì •ì˜ (í›ˆë ¨ ì—í”¼ì†Œë“œì—ì„œ 60%, 30%, 10% í™•ë¥ ë¡œ ì„ íƒ)\n\nì •ìƒì ì¸ ìš´ì˜ ìƒíƒœ â†’ ì–‘í˜¸í•œ ë§µ í’ˆì§ˆ, í‘œì¤€ì ì¸ Noise ì ìš©\n\nì§€ë„ ì˜¤í”„ì…‹ ë°œìƒ â†’ ìì„¸ ì¶”ì • ë“œë¦¬í”„íŠ¸ ë˜ëŠ” ë³€í˜• ê°€ëŠ¥í•œ ì§€í˜•(ì˜ˆ: ì§„í™, ëˆˆ)ìœ¼ë¡œ ì¸í•´ í° ì˜¤í”„ì…‹ ì¶”ê°€\n\në§¤í•‘ ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜ â†’ íìƒ‰(occlusion) ë˜ëŠ” ì„¼ì„œ ì˜¤ë¥˜ë¡œ ì¸í•´ ê° ìŠ¤ìº” í¬ì¸íŠ¸ì— í° Noise ì¶”ê°€\n\n\n\n\n\nì§€í˜• ë³€í™” ì‹œë®¬ë ˆì´ì…˜\n\ní›ˆë ¨ ì§€í˜•ì„ ì…€(Cell) ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , íŠ¹ì • ì…€ì˜ height mapì— ì¶”ê°€ ì˜¤í”„ì…‹ ì ìš©\nì„œë¡œ ë‹¤ë¥¸ ì§€í˜• íŠ¹ì„± ê°„ì˜ ì „í™˜ ì‹œë®¬ë ˆì´ì…˜ (ì˜ˆ: ì‹ìƒ, ê¹Šì€ ëˆˆ ë“±)"
  },
  {
    "objectID": "post/study/2025-03-01-review.html#belief-state-encoder",
    "href": "post/study/2025-03-01-review.html#belief-state-encoder",
    "title": "Learning robust perceptive locomotion for quadrupedal robots in the wild",
    "section": "Belief state encoder",
    "text": "Belief state encoder\n\nStudent Policy ë°©ë²•ë¡  ë” ìì„¸íˆ\n\nğŸ‘‰ í•µì‹¬ ì•„ì´ë””ì–´\n- ê³ ìœ  ê°ê°(Proprioception)ê³¼ ì™¸ë¶€ ê°ê°(Exteroception) ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©\n    - ì™¸ë¶€ ê°ê°ì˜ ì‹ ë¢°ë„ë¥¼ í•™ìŠµí•˜ì—¬ ë™ì ìœ¼ë¡œ ì¡°ì ˆ \n    - ì™¸ë¶€ ê°ê°ì´ ë¶€ì •í™•í•  ë•ŒëŠ” ê³ ìœ  ê°ê°ì„ ë” ë§ì´ í™œìš©í•˜ë„ë¡ í•™ìŠµ\n- RNN(GRU) ê¸°ë°˜ìœ¼ë¡œ Belif stateë¥¼ ì¶”ì •í•˜ì—¬ POMDP ë¬¸ì œ í•´ê²° \n- ê²Œì´íŠ¸ ë””ì½”ë”ë¥¼ í™œìš©í•˜ì—¬ Belif stateì˜ ìœ íš¨ì„±ì„ ë³´ì¥\nGated Encoder\n\nê²Œì´íŠ¸ RNN ëª¨ë¸ê³¼ Multi-Modal Information Fusion ê¸°ë²•ì—ì„œ ì˜ê°\n\nadaptive gating ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš© â†’ ì™¸ë¶€ ê°ê° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€ ì¡°ì ˆ\n\n\n\n\nBelif state ê³„ì‚° ê³¼ì •\n\nì…ë ¥ ë°ì´í„°\n\nê³ ìœ  ê°ê°: \\(o^p_t\\)\n\në…¸ì´ì¦ˆê°€ í¬í•¨ëœ ì™¸ë¶€ ê°ê° íŠ¹ì§•: \\(l^e_t = g_e(\\tilde{o}^e_t)\\)\n\nì´ì „ ìˆ¨ê²¨ì§„ ìƒíƒœ: \\(h_t\\)\n\nRNNì„ í†µí•´ ì¤‘ê°„ Belif state \\(b^\\text{'}_t\\) ê³„ì‚°\n\\[\nb^\\text{'}_t, h_{t+1} = \\text{RNN}(o^p_t, l^e_t, h_t)\n\\]\n\nGRU(Gated Recurrent Unit) ê¸°ë°˜ RNN ì‚¬ìš©\n\nì‹œê°„ì  ì •ë³´(Sequential Information) ìœ ì§€\n\nattention ë²¡í„° \\(\\alpha\\) ê³„ì‚°: ì™¸ë¶€ ê°ê° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€ ê²°ì • \\[\n\\alpha = \\sigma(g_a(b^\\text{'}_t))\n\\]\n\n\\(g_a\\): ì™„ì „ ì—°ê²° ì‹ ê²½ë§(FCN)\n\\(\\sigma(\\cdot)\\): ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜\n\\(\\alpha\\) ê°’ì´ í´ìˆ˜ë¡ ì™¸ë¶€ ê°ê° ì •ë³´ë¥¼ ë” ë§ì´ ë°˜ì˜\n\nìµœì¢… Belif state \\(b_t\\) ê³„ì‚°\n\\[\nb_t = g_b(b^\\text{'}_t) + l^e_t \\cdot \\alpha\n\\]\n\n\\(g_b\\): ì™„ì „ ì—°ê²° ì‹ ê²½ë§(FCN)\nì™¸ë¶€ ê°ê° ì •ë³´ \\(l^e_t\\) ë¥¼ attention ë²¡í„° \\(\\alpha\\) ì— ì˜í•´ ê°€ì¤‘ ì¡°í•©\n\n\nGated Decoder\n\nê°™ì€ ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì„ ë””ì½”ë”ì—ì„œë„ ì‚¬ìš©\nPrivileged Information ë° ë†’ì´ ìƒ˜í”Œì„ ì¬êµ¬ì„±\nReconstruction Loss ê³„ì‚° â†’ Belif state \\(b_t\\) ê°€ í™˜ê²½ ì •ë³´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ë°˜ì˜í•˜ë„ë¡ ìœ ë„\n\n\n\n\nGRU(Gated Recurrent Unit) ì‚¬ìš© ì´ìœ \n\nGRUëŠ” LSTMë³´ë‹¤ ê³„ì‚°ëŸ‰ì´ ì ìœ¼ë©´ì„œë„ ì¥ê¸° ì˜ì¡´ì„±(Long-term dependencies)ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœ ì§€\nê²Œì´íŠ¸ êµ¬ì¡°ë¥¼ í†µí•´ ì¤‘ìš”í•œ ì •ë³´ë§Œ ì„ íƒì ìœ¼ë¡œ ì €ì¥ ë° ì—…ë°ì´íŠ¸ ê°€ëŠ¥"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Post",
    "section": "",
    "text": "Archive Study Materials\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow to read papers\n\n\n\n\n\n\netc\n\n\n\në…¼ë¬¸ ì½ëŠ” ë²•ì— ê´€í•œ ì˜ìƒ ìš”ì•½\n\n\n\n\n\nMar 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLearning robust perceptive locomotion for quadrupedal robots in the wild\n\n\n\n\n\n\nreview\n\n\n\në…¼ë¬¸ ì½ëŠ” ë²•ì— ê´€í•œ ì˜ìƒ ìš”ì•½\n\n\n\n\n\nMar 1, 2025\n\n\nJungyeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\n[6] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nCombining Legged and Manipulator/ Foundation Model / / /\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[5] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nImpedance Matching/ / / /\n\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[4] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\n / / Small Home Robot /MI-HGNN\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[3] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nHybrid Internal Model/Locomotion over Challenging Terrain/Extreme Parkour/Differentiable Simulation\n\n\n\n\n\nFeb 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[2] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nActor-Cross-Critic/ANYmal Parkour/Stiffness Tuning/Privileged Information\n\n\n\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 3mins papers\n\n\n\n\n\n\npaper\n\n\n3mins\n\n\n\nDreamFLEX/RL-augmented MPC/CAIMAN/Safe RL/MPC Testbed\n\n\n\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear MPC\n\n\n\n\n\n\nmpc\n\n\npaper\n\n\n\nBasics of Linear MPC Controller for Quadruped Walking Robots\n\n\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\nNo matching items"
  }
]